{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Datasets loaded\n",
      "\n",
      "Infight parts dataset shape: (21347, 42)\n",
      "Number of unique parts: 326\n",
      "Number of unique aircraft: 331\n",
      "Number of unique shipsets: 375\n",
      "Number of unique tails: 331\n",
      "Number of missing OwnerCompanyId values: 2298 (10.76%)\n",
      "\n",
      "Unique values in key columns\n",
      "PartNumber: 326 unique values (1.53%) out of 21347 non-null values\n",
      "Tail: 331 unique values (1.69%) out of 19530 non-null values\n",
      "ProgramShipsetNumber: 375 unique values (1.76%) out of 21347 non-null values\n",
      "OwnerCompanyId: 49 unique values (0.26%) out of 19049 non-null values\n",
      "Number of duplicate UniquePartInstallationIds in sample: 2020\n",
      "Processing chunk 1/1 for aggregation...\n",
      "Date range: 2012-03-26 00:00:00 to 2058-11-25 00:00:00\n",
      "Processing time chunk 1/47 (2012-03-26 00:00:00 to 2013-03-18 00:00:00)...\n",
      "Time chunk 1 completed with 3715 rows\n",
      "Processing time chunk 2/47 (2013-03-25 00:00:00 to 2014-03-17 00:00:00)...\n",
      "Time chunk 2 completed with 17353 rows\n",
      "Processing time chunk 3/47 (2014-03-24 00:00:00 to 2015-03-16 00:00:00)...\n",
      "Time chunk 3 completed with 49696 rows\n",
      "Processing time chunk 4/47 (2015-03-23 00:00:00 to 2016-03-14 00:00:00)...\n",
      "Time chunk 4 completed with 112263 rows\n",
      "Processing time chunk 5/47 (2016-03-21 00:00:00 to 2017-03-13 00:00:00)...\n",
      "Time chunk 5 completed with 186878 rows\n",
      "Processing time chunk 6/47 (2017-03-20 00:00:00 to 2018-03-12 00:00:00)...\n",
      "Time chunk 6 completed with 217861 rows\n",
      "Processing time chunk 7/47 (2018-03-19 00:00:00 to 2019-03-11 00:00:00)...\n",
      "Time chunk 7 completed with 253182 rows\n",
      "Processing time chunk 8/47 (2019-03-18 00:00:00 to 2020-03-09 00:00:00)...\n",
      "Time chunk 8 completed with 322120 rows\n",
      "Processing time chunk 9/47 (2020-03-16 00:00:00 to 2021-03-08 00:00:00)...\n",
      "Time chunk 9 completed with 380020 rows\n",
      "Processing time chunk 10/47 (2021-03-15 00:00:00 to 2022-03-07 00:00:00)...\n",
      "Time chunk 10 completed with 418110 rows\n",
      "Processing time chunk 11/47 (2022-03-14 00:00:00 to 2023-03-06 00:00:00)...\n",
      "Time chunk 11 completed with 456657 rows\n",
      "Processing time chunk 12/47 (2023-03-13 00:00:00 to 2024-03-04 00:00:00)...\n",
      "Time chunk 12 completed with 470649 rows\n",
      "Processing time chunk 13/47 (2024-03-11 00:00:00 to 2025-03-03 00:00:00)...\n",
      "Time chunk 13 completed with 464824 rows\n",
      "Processing time chunk 14/47 (2025-03-10 00:00:00 to 2026-03-02 00:00:00)...\n",
      "Time chunk 14 completed with 439781 rows\n",
      "Processing time chunk 15/47 (2026-03-09 00:00:00 to 2027-03-01 00:00:00)...\n",
      "Time chunk 15 completed with 395010 rows\n",
      "Processing time chunk 16/47 (2027-03-08 00:00:00 to 2028-02-28 00:00:00)...\n",
      "Time chunk 16 completed with 399876 rows\n",
      "Processing time chunk 17/47 (2028-03-06 00:00:00 to 2029-02-26 00:00:00)...\n",
      "Time chunk 17 completed with 399177 rows\n",
      "Processing time chunk 18/47 (2029-03-05 00:00:00 to 2030-02-25 00:00:00)...\n",
      "Time chunk 18 completed with 354840 rows\n",
      "Processing time chunk 19/47 (2030-03-04 00:00:00 to 2031-02-24 00:00:00)...\n",
      "Time chunk 19 completed with 295865 rows\n",
      "Processing time chunk 20/47 (2031-03-03 00:00:00 to 2032-02-23 00:00:00)...\n",
      "Time chunk 20 completed with 258381 rows\n",
      "Processing time chunk 21/47 (2032-03-01 00:00:00 to 2033-02-21 00:00:00)...\n",
      "Time chunk 21 completed with 218656 rows\n",
      "Processing time chunk 22/47 (2033-02-28 00:00:00 to 2034-02-20 00:00:00)...\n",
      "Time chunk 22 completed with 189670 rows\n",
      "Processing time chunk 23/47 (2034-02-27 00:00:00 to 2035-02-19 00:00:00)...\n",
      "Time chunk 23 completed with 164066 rows\n",
      "Processing time chunk 24/47 (2035-02-26 00:00:00 to 2036-02-18 00:00:00)...\n",
      "Time chunk 24 completed with 131004 rows\n",
      "Processing time chunk 25/47 (2036-02-25 00:00:00 to 2037-02-16 00:00:00)...\n",
      "Time chunk 25 completed with 99736 rows\n",
      "Processing time chunk 26/47 (2037-02-23 00:00:00 to 2038-02-15 00:00:00)...\n",
      "Time chunk 26 completed with 64007 rows\n",
      "Processing time chunk 27/47 (2038-02-22 00:00:00 to 2039-02-14 00:00:00)...\n",
      "Time chunk 27 completed with 31859 rows\n",
      "Processing time chunk 28/47 (2039-02-21 00:00:00 to 2040-02-13 00:00:00)...\n",
      "Time chunk 28 completed with 8018 rows\n",
      "Processing time chunk 29/47 (2040-02-20 00:00:00 to 2041-02-11 00:00:00)...\n",
      "Time chunk 29 completed with 6892 rows\n",
      "Processing time chunk 30/47 (2041-02-18 00:00:00 to 2042-02-10 00:00:00)...\n",
      "Time chunk 30 completed with 5852 rows\n",
      "Processing time chunk 31/47 (2042-02-17 00:00:00 to 2043-02-09 00:00:00)...\n",
      "Time chunk 31 completed with 5637 rows\n",
      "Processing time chunk 32/47 (2043-02-16 00:00:00 to 2044-02-08 00:00:00)...\n",
      "Time chunk 32 completed with 7027 rows\n",
      "Processing time chunk 33/47 (2044-02-15 00:00:00 to 2045-02-06 00:00:00)...\n",
      "Time chunk 33 completed with 8587 rows\n",
      "Processing time chunk 34/47 (2045-02-13 00:00:00 to 2046-02-05 00:00:00)...\n",
      "Time chunk 34 completed with 9191 rows\n",
      "Processing time chunk 35/47 (2046-02-12 00:00:00 to 2047-02-04 00:00:00)...\n",
      "Time chunk 35 completed with 8138 rows\n",
      "Processing time chunk 36/47 (2047-02-11 00:00:00 to 2048-02-03 00:00:00)...\n",
      "Time chunk 36 completed with 7239 rows\n",
      "Processing time chunk 37/47 (2048-02-10 00:00:00 to 2049-02-01 00:00:00)...\n",
      "Time chunk 37 completed with 6947 rows\n",
      "Processing time chunk 38/47 (2049-02-08 00:00:00 to 2050-01-31 00:00:00)...\n",
      "Time chunk 38 completed with 6760 rows\n",
      "Processing time chunk 39/47 (2050-02-07 00:00:00 to 2051-01-30 00:00:00)...\n",
      "Time chunk 39 completed with 6760 rows\n",
      "Processing time chunk 40/47 (2051-02-06 00:00:00 to 2052-01-29 00:00:00)...\n",
      "Time chunk 40 completed with 6760 rows\n",
      "Processing time chunk 41/47 (2052-02-05 00:00:00 to 2053-01-27 00:00:00)...\n",
      "Time chunk 41 completed with 6100 rows\n",
      "Processing time chunk 42/47 (2053-02-03 00:00:00 to 2054-01-26 00:00:00)...\n",
      "Time chunk 42 completed with 4555 rows\n",
      "Processing time chunk 43/47 (2054-02-02 00:00:00 to 2055-01-25 00:00:00)...\n",
      "Time chunk 43 completed with 2995 rows\n",
      "Processing time chunk 44/47 (2055-02-01 00:00:00 to 2056-01-24 00:00:00)...\n",
      "Time chunk 44 completed with 2080 rows\n",
      "Processing time chunk 45/47 (2056-01-31 00:00:00 to 2057-01-22 00:00:00)...\n",
      "Time chunk 45 completed with 2080 rows\n",
      "Processing time chunk 46/47 (2057-01-29 00:00:00 to 2058-01-21 00:00:00)...\n",
      "Time chunk 46 completed with 1700 rows\n",
      "Processing time chunk 47/47 (2058-01-28 00:00:00 to 2058-11-25 00:00:00)...\n",
      "Time chunk 47 completed with 620 rows\n",
      "Weekly sequences created!\n",
      "Number of weeks covered: 2436\n",
      "Number of unique parts: 326\n",
      "Date range: 2012-03-26 00:00:00 to 2058-11-25 00:00:00\n",
      "Saving data in 7 chunks...\n",
      "Data saved and temporary files cleaned up!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "data_dir = os.path.join('private', 'data', 'transformed')\n",
    "\n",
    "expected_files = [\n",
    "    'merged_inflight_parts.parquet',\n",
    "    'combined_rma.parquet',\n",
    "    'merged_flight_data.parquet',\n",
    "    'mtbf.parquet'\n",
    "]\n",
    "\n",
    "datasets = {}\n",
    "pd.reset_option('display.float_format')\n",
    "\n",
    "# Only load the files we need\n",
    "file_path = os.path.join(data_dir, 'merged_inflight_parts.parquet')\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"File {file_path} not found\")\n",
    "\n",
    "merged_inflight_parts_df = pd.read_parquet(file_path)\n",
    "\n",
    "# Convert to same dtypes\n",
    "for col in merged_inflight_parts_df.select_dtypes(include=['float64']).columns:\n",
    "    merged_inflight_parts_df[col] = pd.to_numeric(merged_inflight_parts_df[col], downcast='float')\n",
    "for col in merged_inflight_parts_df.select_dtypes(include=['int64']).columns:\n",
    "    merged_inflight_parts_df[col] = pd.to_numeric(merged_inflight_parts_df[col], downcast='integer')\n",
    "\n",
    "# Convert string columns to category\n",
    "categorical_candidate_cols = merged_inflight_parts_df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_candidate_cols:\n",
    "    nunique = merged_inflight_parts_df[col].nunique()\n",
    "    if nunique < len(merged_inflight_parts_df) * 0.5:  # If fewer than 50% of values are unique\n",
    "        merged_inflight_parts_df[col] = merged_inflight_parts_df[col].astype('category')\n",
    "\n",
    "print(\"Datasets loaded\")\n",
    "\n",
    "# Default decommision date is EIS + 10 years\n",
    "merged_inflight_parts_df['DecommissionDate'] = pd.NaT\n",
    "mask = merged_inflight_parts_df['DecommissionDate'].isna()\n",
    "merged_inflight_parts_df.loc[mask, 'DecommissionDate'] = merged_inflight_parts_df.loc[mask, 'EntryIntoServiceDate'] + pd.Timedelta(days=365.25 * 10)\n",
    "\n",
    "print(f\"\\nInfight parts dataset shape: {merged_inflight_parts_df.shape}\")\n",
    "print(f\"Number of unique parts: {merged_inflight_parts_df['PartNumber'].nunique()}\")\n",
    "print(f\"Number of unique aircraft: {merged_inflight_parts_df['Tail'].nunique()}\")\n",
    "\n",
    "# Concat the ProgramCode and Shipset Name Fields to create a unique identifier for each shipset\n",
    "merged_inflight_parts_df['ProgramShipsetNumber'] = merged_inflight_parts_df['ProgramCode'] + '-' + merged_inflight_parts_df['ShipsetName']\n",
    "\n",
    "# Count unique shipsets and tails\n",
    "print(f\"Number of unique shipsets: {merged_inflight_parts_df['ProgramShipsetNumber'].nunique()}\")\n",
    "print(f\"Number of unique tails: {merged_inflight_parts_df['Tail'].nunique()}\")\n",
    "\n",
    "# Count number of missing OwnerCompanyId values\n",
    "missing_owner_company = merged_inflight_parts_df['OwnerCompanyId'].isna().sum()\n",
    "print(f\"Number of missing OwnerCompanyId values: {missing_owner_company} ({(missing_owner_company / merged_inflight_parts_df.shape[0]) * 100:.2f}%)\")\n",
    "\n",
    "# Calculate percentage of unique values in key columns\n",
    "key_columns = ['PartNumber', 'Tail', 'ProgramShipsetNumber', 'OwnerCompanyId']\n",
    "print(\"\\nUnique values in key columns\")\n",
    "for col in key_columns:\n",
    "    total_values = merged_inflight_parts_df[col].count()\n",
    "    unique_values = merged_inflight_parts_df[col].nunique()\n",
    "    if total_values > 0:\n",
    "        unique_percent = (unique_values / total_values) * 100\n",
    "    else:\n",
    "        unique_percent = 0\n",
    "    print(f\"{col}: {unique_values} unique values ({unique_percent:.2f}%) out of {total_values} non-null values\")\n",
    "\n",
    "# Remove redundant columns and columns with no unique values\n",
    "redundant_columns = ['partnumber', 'productname', 'ShipsetName', 'ShipsetNumber', \n",
    "                     'EquipmentEntryId', 'DeliveryEquipmentEntryId',\n",
    "                     'InstallLocationType', 'EntryType', 'ShipsetType', \n",
    "                     'definitionlevel', 'useequipmentfamily']\n",
    "drop_cols = [col for col in redundant_columns if col in merged_inflight_parts_df.columns]\n",
    "merged_inflight_parts_df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# New identifier column\n",
    "merged_inflight_parts_df['UniquePartInstallationId'] = (\n",
    "    merged_inflight_parts_df['ProgramShipsetNumber'] + '-' + \n",
    "    merged_inflight_parts_df['PartNumber'] + '-' + \n",
    "    merged_inflight_parts_df['InstallLocation'].fillna('Unknown')\n",
    ")\n",
    "\n",
    "sample_size = min(100000, len(merged_inflight_parts_df))\n",
    "sample_df = merged_inflight_parts_df.sample(sample_size)\n",
    "unique_counts = sample_df.groupby('UniquePartInstallationId').size().reset_index(name='count')\n",
    "duplicates = unique_counts[unique_counts['count'] > 1]\n",
    "print(f\"Number of duplicate UniquePartInstallationIds in sample: {len(duplicates)}\")\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    agg_dict = {\n",
    "        'operatingmode': lambda x: ';'.join(sorted(x.dropna().astype(str).unique())),\n",
    "        'maximuminputpower': 'max',\n",
    "    }\n",
    "    \n",
    "    all_other_columns = [col for col in merged_inflight_parts_df.columns \n",
    "                         if col not in agg_dict.keys() and col != 'UniquePartInstallationId']\n",
    "    \n",
    "    for col in all_other_columns:\n",
    "        agg_dict[col] = 'first'\n",
    "    \n",
    "    # Group and aggregate in chunks\n",
    "    chunk_size = 100000\n",
    "    num_chunks = (len(merged_inflight_parts_df) + chunk_size - 1) // chunk_size\n",
    "    aggregated_chunks = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, len(merged_inflight_parts_df))\n",
    "        print(f\"Processing chunk {i+1}/{num_chunks} for aggregation...\")\n",
    "        \n",
    "        chunk = merged_inflight_parts_df.iloc[start_idx:end_idx]\n",
    "        unique_ids = chunk['UniquePartInstallationId']\n",
    "        agg_chunk = chunk.groupby('UniquePartInstallationId').agg(agg_dict)        \n",
    "        agg_chunk = agg_chunk.reset_index()\n",
    "        aggregated_chunks.append(agg_chunk)\n",
    "        \n",
    "        del chunk\n",
    "        del unique_ids\n",
    "        gc.collect()\n",
    "    \n",
    "    # Combine chunks\n",
    "    merged_inflight_parts_df = pd.concat(aggregated_chunks, ignore_index=True)\n",
    "    del aggregated_chunks\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"No duplicates found in sample, skipping aggregation.\")\n",
    "\n",
    "## Turn into weekly time series\n",
    "# Get range of dates\n",
    "min_date = merged_inflight_parts_df['EntryIntoServiceDate'].min()\n",
    "max_date = merged_inflight_parts_df['DecommissionDate'].max()\n",
    "\n",
    "# Create weekly date range\n",
    "date_range = pd.date_range(min_date, max_date, freq='W-MON')\n",
    "print(f\"Date range: {date_range[0]} to {date_range[-1]}\")\n",
    "\n",
    "# Sort by install date for efficiency in the loop\n",
    "merged_inflight_parts_df = merged_inflight_parts_df.sort_values('EntryIntoServiceDate')\n",
    "\n",
    "# Dictionaries for running totals\n",
    "running_fleet_totals = {}\n",
    "running_tail_totals = {}\n",
    "running_location_totals = {}\n",
    "running_company_totals = {}\n",
    "\n",
    "# Create a list of required columns to keep\n",
    "required_columns = ['UniquePartInstallationId', 'PartNumber', 'Tail', 'InstallLocation', \n",
    "                    'OwnerCompanyId', 'DeliveryQuantity', 'EntryIntoServiceDate', \n",
    "                    'DecommissionDate', 'ProgramShipsetNumber']\n",
    "\n",
    "# Divide date range into chunks to process\n",
    "time_chunks = [date_range[i:i+52] for i in range(0, len(date_range), 52)]\n",
    "all_weekly_data = []\n",
    "\n",
    "for chunk_idx, time_chunk in enumerate(time_chunks):\n",
    "    print(f\"Processing time chunk {chunk_idx+1}/{len(time_chunks)} ({time_chunk[0]} to {time_chunk[-1]})...\")\n",
    "    \n",
    "    chunk_weekly_data = []\n",
    "    \n",
    "    for week_start in time_chunk:\n",
    "        week_end = week_start + timedelta(days=6)\n",
    "        \n",
    "        # Identify events for this week using boolean\n",
    "        install_mask = (\n",
    "            (merged_inflight_parts_df['EntryIntoServiceDate'] > week_start - timedelta(days=7)) &\n",
    "            (merged_inflight_parts_df['EntryIntoServiceDate'] <= week_end)\n",
    "        )\n",
    "        \n",
    "        decommision_mask = (\n",
    "            (merged_inflight_parts_df['DecommissionDate'] > week_start - timedelta(days=7)) &\n",
    "            (merged_inflight_parts_df['DecommissionDate'] <= week_end)\n",
    "        )\n",
    "        \n",
    "        # Process install events\n",
    "        for _, part in merged_inflight_parts_df[install_mask][required_columns].iterrows():\n",
    "            part_num = part['PartNumber']\n",
    "            quantity = part['DeliveryQuantity']\n",
    "            tail = part['Tail']\n",
    "            location = part['InstallLocation']\n",
    "            company = part['OwnerCompanyId']\n",
    "            \n",
    "            # Update running totals\n",
    "            running_fleet_totals[part_num] = running_fleet_totals.get(part_num, 0) + quantity\n",
    "            running_tail_totals[(part_num, tail)] = running_tail_totals.get((part_num, tail), 0) + quantity\n",
    "            running_location_totals[(part_num, location)] = running_location_totals.get((part_num, location), 0) + quantity\n",
    "            running_company_totals[(part_num, company)] = running_company_totals.get((part_num, company), 0) + quantity\n",
    "        \n",
    "        # Process decommision events\n",
    "        for _, part in merged_inflight_parts_df[decommision_mask][required_columns].iterrows():\n",
    "            part_num = part['PartNumber']\n",
    "            quantity = part['DeliveryQuantity']\n",
    "            tail = part['Tail']\n",
    "            location = part['InstallLocation']\n",
    "            company = part['OwnerCompanyId']\n",
    "            \n",
    "            # Update running totals\n",
    "            running_fleet_totals[part_num] = running_fleet_totals.get(part_num, 0) - quantity\n",
    "            running_tail_totals[(part_num, tail)] = running_tail_totals.get((part_num, tail), 0) - quantity\n",
    "            running_location_totals[(part_num, location)] = running_location_totals.get((part_num, location), 0) - quantity\n",
    "            running_company_totals[(part_num, company)] = running_company_totals.get((part_num, company), 0) - quantity\n",
    "        \n",
    "        # Find active parts for this week\n",
    "        active_mask = (\n",
    "            (merged_inflight_parts_df['EntryIntoServiceDate'] <= week_end) & \n",
    "            (merged_inflight_parts_df['DecommissionDate'] >= week_start)\n",
    "        )\n",
    "        \n",
    "        current_week_parts = merged_inflight_parts_df[active_mask][required_columns].copy()\n",
    "        # Add week start\n",
    "        current_week_parts['WeekStart'] = week_start\n",
    "        \n",
    "        # Add running totals\n",
    "        if min_date <= week_start <= max_date:\n",
    "            # Add running totals using vectorized operations where possible\n",
    "            current_week_parts['RunningFleetTotal'] = current_week_parts['PartNumber'].map(\n",
    "                lambda pn: max(0, running_fleet_totals.get(pn, 0))\n",
    "            )\n",
    "            \n",
    "            current_week_parts['RunningTailTotal'] = current_week_parts.apply(\n",
    "                lambda row: max(0, running_tail_totals.get((row['PartNumber'], row['Tail']), 0)), axis=1\n",
    "            )\n",
    "            current_week_parts['RunningLocationTotal'] = current_week_parts.apply(\n",
    "                lambda row: max(0, running_location_totals.get((row['PartNumber'], row['InstallLocation']), 0)), axis=1\n",
    "            )\n",
    "            current_week_parts['RunningCompanyTotal'] = current_week_parts.apply(\n",
    "                lambda row: max(0, running_company_totals.get((row['PartNumber'], row['OwnerCompanyId']), 0)), axis=1\n",
    "            )\n",
    "        else:\n",
    "            # Set running totals to 0 outside the date range\n",
    "            current_week_parts['RunningFleetTotal'] = 0\n",
    "            current_week_parts['RunningTailTotal'] = 0\n",
    "            current_week_parts['RunningLocationTotal'] = 0\n",
    "            current_week_parts['RunningCompanyTotal'] = 0\n",
    "        \n",
    "        chunk_weekly_data.append(current_week_parts)\n",
    "    \n",
    "    # Combine weeks in this chunk\n",
    "    if chunk_weekly_data:\n",
    "        time_chunk_df = pd.concat(chunk_weekly_data, ignore_index=True)\n",
    "        all_weekly_data.append(time_chunk_df)\n",
    "        \n",
    "        del chunk_weekly_data\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Time chunk {chunk_idx+1} completed with {len(time_chunk_df)} rows\")\n",
    "        del time_chunk_df\n",
    "        gc.collect()\n",
    "\n",
    "# Combine chunks\n",
    "weekly_sequence_df = pd.concat(all_weekly_data, ignore_index=True)\n",
    "\n",
    "del all_weekly_data\n",
    "del merged_inflight_parts_df\n",
    "gc.collect()\n",
    "\n",
    "print(\"Weekly sequences created!\")\n",
    "print(f\"Number of weeks covered: {weekly_sequence_df['WeekStart'].nunique()}\")\n",
    "print(f\"Number of unique parts: {weekly_sequence_df['PartNumber'].nunique()}\")\n",
    "print(f\"Date range: {weekly_sequence_df['WeekStart'].min()} to {weekly_sequence_df['WeekStart'].max()}\")\n",
    "\n",
    "# Save to parquet\n",
    "processed_data_dir = os.path.join('private', 'data', 'processed')\n",
    "if not os.path.exists(processed_data_dir):\n",
    "    os.makedirs(processed_data_dir)\n",
    "\n",
    "# Save in chunks\n",
    "rows_per_chunk = 1000000\n",
    "num_save_chunks = (len(weekly_sequence_df) + rows_per_chunk - 1) // rows_per_chunk\n",
    "\n",
    "if num_save_chunks <= 1:\n",
    "    output_path = os.path.join(processed_data_dir, 'weekly_sequence_1_parts.parquet')\n",
    "    weekly_sequence_df.to_parquet(output_path)\n",
    "    print(\"Data saved!\")\n",
    "else:\n",
    "    # Save in multiple chunks and then recombine\n",
    "    print(f\"Saving data in {num_save_chunks} chunks...\")\n",
    "    temp_file_paths = []\n",
    "    \n",
    "    for i in range(num_save_chunks):\n",
    "        start_idx = i * rows_per_chunk\n",
    "        end_idx = min((i + 1) * rows_per_chunk, len(weekly_sequence_df))\n",
    "        \n",
    "        chunk = weekly_sequence_df.iloc[start_idx:end_idx]\n",
    "        temp_path = os.path.join(processed_data_dir, f'weekly_sequence_1_parts_temp_{i}.parquet')\n",
    "        chunk.to_parquet(temp_path)\n",
    "        temp_file_paths.append(temp_path)\n",
    "        \n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    combined_df = pd.read_parquet(temp_file_paths[0])\n",
    "    \n",
    "    for i in range(1, len(temp_file_paths)):\n",
    "        next_chunk = pd.read_parquet(temp_file_paths[i])\n",
    "        combined_df = pd.concat([combined_df, next_chunk], ignore_index=True)\n",
    "        del next_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Save the combined data\n",
    "    output_path = os.path.join(processed_data_dir, 'weekly_sequence_1_parts.parquet')\n",
    "    combined_df.to_parquet(output_path)\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    for temp_path in temp_file_paths:\n",
    "        os.remove(temp_path)\n",
    "    \n",
    "    print(\"Data saved and temporary files cleaned up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weekly sequence data...\n",
      "Weekly sequence loaded\n",
      "Memory usage of weekly_sequence_df: 1806.81 MB\n",
      "Loading RMA data...\n",
      "Available columns in RMA file: ['source', 'rma_number', 'part_number', 'serial_number', 'customer', 'status', 'received_date', 'ship_date', 'part_description', 'fault_code', 'lru_name', 'workshop_location', 'flight_hours', 'return_reason', 'warranty_end_date', 'return_reason_category', 'repair_duration']\n",
      "Missing received_date values: 105712 (36.45%)\n",
      "Cleaning RMA data...\n",
      "Date range: 2012-05-07 00:00:00+00:00 to 2025-04-01 12:00:00+00:00\n",
      "Processing RMA data for weekly sequences...\n",
      "RMA date range: 2012-05-07 00:00:00 to 2025-03-31 00:00:00\n",
      "Number of unique part numbers in RMA data: 358\n",
      "Processing 358 part numbers in 1 chunks...\n",
      "Processing part number chunk 1/1 (0 to 358)...\n",
      "Completed part number chunk 1/1\n",
      "Combining all chunks...\n",
      "RMA weekly data created with 76916 rows\n",
      "Memory usage of rma_weekly_df: 7.28 MB\n",
      "Merging RMA data into weekly sequence...\n",
      "Processing week chunk 1/47 (0 to 52)...\n",
      "Completed week chunk 1/47\n",
      "Processing week chunk 2/47 (52 to 104)...\n",
      "Completed week chunk 2/47\n",
      "Processing week chunk 3/47 (104 to 156)...\n",
      "Completed week chunk 3/47\n",
      "Processing week chunk 4/47 (156 to 208)...\n",
      "Completed week chunk 4/47\n",
      "Processing week chunk 5/47 (208 to 260)...\n",
      "Completed week chunk 5/47\n",
      "Processing week chunk 6/47 (260 to 312)...\n",
      "Completed week chunk 6/47\n",
      "Processing week chunk 7/47 (312 to 364)...\n",
      "Completed week chunk 7/47\n",
      "Processing week chunk 8/47 (364 to 416)...\n",
      "Completed week chunk 8/47\n",
      "Processing week chunk 9/47 (416 to 468)...\n",
      "Completed week chunk 9/47\n",
      "Processing week chunk 10/47 (468 to 520)...\n",
      "Completed week chunk 10/47\n",
      "Processing week chunk 11/47 (520 to 572)...\n",
      "Completed week chunk 11/47\n",
      "Processing week chunk 12/47 (572 to 624)...\n",
      "Completed week chunk 12/47\n",
      "Processing week chunk 13/47 (624 to 676)...\n",
      "Completed week chunk 13/47\n",
      "Processing week chunk 14/47 (676 to 728)...\n",
      "Completed week chunk 14/47\n",
      "Processing week chunk 15/47 (728 to 780)...\n",
      "Completed week chunk 15/47\n",
      "Processing week chunk 16/47 (780 to 832)...\n",
      "Completed week chunk 16/47\n",
      "Processing week chunk 17/47 (832 to 884)...\n",
      "Completed week chunk 17/47\n",
      "Processing week chunk 18/47 (884 to 936)...\n",
      "Completed week chunk 18/47\n",
      "Processing week chunk 19/47 (936 to 988)...\n",
      "Completed week chunk 19/47\n",
      "Processing week chunk 20/47 (988 to 1040)...\n",
      "Completed week chunk 20/47\n",
      "Processing week chunk 21/47 (1040 to 1092)...\n",
      "Completed week chunk 21/47\n",
      "Processing week chunk 22/47 (1092 to 1144)...\n",
      "Completed week chunk 22/47\n",
      "Processing week chunk 23/47 (1144 to 1196)...\n",
      "Completed week chunk 23/47\n",
      "Processing week chunk 24/47 (1196 to 1248)...\n",
      "Completed week chunk 24/47\n",
      "Processing week chunk 25/47 (1248 to 1300)...\n",
      "Completed week chunk 25/47\n",
      "Processing week chunk 26/47 (1300 to 1352)...\n",
      "Completed week chunk 26/47\n",
      "Processing week chunk 27/47 (1352 to 1404)...\n",
      "Completed week chunk 27/47\n",
      "Processing week chunk 28/47 (1404 to 1456)...\n",
      "Completed week chunk 28/47\n",
      "Processing week chunk 29/47 (1456 to 1508)...\n",
      "Completed week chunk 29/47\n",
      "Processing week chunk 30/47 (1508 to 1560)...\n",
      "Completed week chunk 30/47\n",
      "Processing week chunk 31/47 (1560 to 1612)...\n",
      "Completed week chunk 31/47\n",
      "Processing week chunk 32/47 (1612 to 1664)...\n",
      "Completed week chunk 32/47\n",
      "Processing week chunk 33/47 (1664 to 1716)...\n",
      "Completed week chunk 33/47\n",
      "Processing week chunk 34/47 (1716 to 1768)...\n",
      "Completed week chunk 34/47\n",
      "Processing week chunk 35/47 (1768 to 1820)...\n",
      "Completed week chunk 35/47\n",
      "Processing week chunk 36/47 (1820 to 1872)...\n",
      "Completed week chunk 36/47\n",
      "Processing week chunk 37/47 (1872 to 1924)...\n",
      "Completed week chunk 37/47\n",
      "Processing week chunk 38/47 (1924 to 1976)...\n",
      "Completed week chunk 38/47\n",
      "Processing week chunk 39/47 (1976 to 2028)...\n",
      "Completed week chunk 39/47\n",
      "Processing week chunk 40/47 (2028 to 2080)...\n",
      "Completed week chunk 40/47\n",
      "Processing week chunk 41/47 (2080 to 2132)...\n",
      "Completed week chunk 41/47\n",
      "Processing week chunk 42/47 (2132 to 2184)...\n",
      "Completed week chunk 42/47\n",
      "Processing week chunk 43/47 (2184 to 2236)...\n",
      "Completed week chunk 43/47\n",
      "Processing week chunk 44/47 (2236 to 2288)...\n",
      "Completed week chunk 44/47\n",
      "Processing week chunk 45/47 (2288 to 2340)...\n",
      "Completed week chunk 45/47\n",
      "Processing week chunk 46/47 (2340 to 2392)...\n",
      "Completed week chunk 46/47\n",
      "Processing week chunk 47/47 (2392 to 2436)...\n",
      "Completed week chunk 47/47\n",
      "Combining all merged chunks...\n",
      "Final merged data created with 6909194 rows\n",
      "Memory usage of final data: 1964.95 MB\n",
      "Saving data in 7 chunks...\n",
      "Saved chunk 1/7\n",
      "Saved chunk 2/7\n",
      "Saved chunk 3/7\n",
      "Saved chunk 4/7\n",
      "Saved chunk 5/7\n",
      "Saved chunk 6/7\n",
      "Saved chunk 7/7\n",
      "All chunks saved!\n",
      "Temporary files saved at: private/data/processed/weekly_sequence_2_rma_temp_0.parquet, private/data/processed/weekly_sequence_2_rma_temp_1.parquet, private/data/processed/weekly_sequence_2_rma_temp_2.parquet, private/data/processed/weekly_sequence_2_rma_temp_3.parquet, private/data/processed/weekly_sequence_2_rma_temp_4.parquet, private/data/processed/weekly_sequence_2_rma_temp_5.parquet, private/data/processed/weekly_sequence_2_rma_temp_6.parquet\n",
      "Combining saved chunks...\n",
      "Data saved and temporary files cleaned up!\n",
      "All processing completed!\n",
      "Final data saved at: private/data/processed/weekly_sequence_2_rma.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "\n",
    "# Define directories\n",
    "processed_data_dir = os.path.join('private', 'data', 'processed')\n",
    "data_dir = os.path.join('private', 'data', 'transformed')\n",
    "\n",
    "print(\"Loading weekly sequence data...\")\n",
    "# Load the weekly sequence data\n",
    "input_path = os.path.join(processed_data_dir, 'weekly_sequence_1_parts.parquet')\n",
    "essential_columns = ['PartNumber', 'WeekStart', 'UniquePartInstallationId', 'RunningFleetTotal', \n",
    "                    'RunningTailTotal', 'RunningLocationTotal', 'RunningCompanyTotal', 'Tail']\n",
    "weekly_sequence_df = pd.read_parquet(input_path, columns=essential_columns)\n",
    "\n",
    "print(\"Weekly sequence loaded\")\n",
    "print(f\"Memory usage of weekly_sequence_df: {weekly_sequence_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"Loading RMA data...\")\n",
    "input_path = os.path.join(data_dir, 'combined_rma.parquet')\n",
    "\n",
    "# First all available columns to see what's actually in the file\n",
    "try:\n",
    "    temp_df = pd.read_parquet(input_path, columns=None)\n",
    "    available_columns = temp_df.columns.tolist()\n",
    "    print(f\"Available columns in RMA file: {available_columns}\")\n",
    "    del temp_df\n",
    "    gc.collect()\n",
    "    \n",
    "    rma_columns = ['rma_number', 'part_number', 'received_date', 'ship_date', 'repair_duration']\n",
    "    \n",
    "    # Add optional columns if they exist\n",
    "    if 'fault_category' in available_columns:\n",
    "        rma_columns.append('fault_category')\n",
    "    elif 'fault_code' in available_columns:\n",
    "        rma_columns.append('fault_code')  # Alternative column\n",
    "        \n",
    "    if 'repair_reason_category' in available_columns:\n",
    "        rma_columns.append('repair_reason_category')\n",
    "    elif 'return_reason_category' in available_columns:\n",
    "        rma_columns.append('return_reason_category')  # Alternative column\n",
    "        \n",
    "    if 'received_year' in available_columns:\n",
    "        rma_columns.append('received_year')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error reading all columns: {str(e)}\")\n",
    "    rma_columns = ['rma_number', 'part_number', 'received_date', 'ship_date', 'repair_duration']\n",
    "\n",
    "combined_rma_df = pd.read_parquet(input_path, columns=rma_columns)\n",
    "\n",
    "# Fix data types issues\n",
    "for col in combined_rma_df.select_dtypes(include=['float64']).columns:\n",
    "    combined_rma_df[col] = pd.to_numeric(combined_rma_df[col], downcast='float')\n",
    "for col in combined_rma_df.select_dtypes(include=['int64']).columns:\n",
    "    combined_rma_df[col] = pd.to_numeric(combined_rma_df[col], downcast='integer')\n",
    "\n",
    "# Use categories for string columns\n",
    "for col in combined_rma_df.select_dtypes(include=['object']).columns:\n",
    "    if combined_rma_df[col].nunique() < len(combined_rma_df) * 0.5:\n",
    "        combined_rma_df[col] = combined_rma_df[col].astype('category')\n",
    "\n",
    "# Check for missing values in critical columns\n",
    "missing_values = combined_rma_df['received_date'].isna().sum()\n",
    "print(f\"Missing received_date values: {missing_values} ({missing_values/len(combined_rma_df)*100:.2f}%)\")\n",
    "\n",
    "# Clean the RMA data\n",
    "print(\"Cleaning RMA data...\")\n",
    "combined_rma_df = combined_rma_df.dropna(subset=['received_date'])\n",
    "\n",
    "# Check for duplicates\n",
    "if 'received_year' not in combined_rma_df.columns:\n",
    "    combined_rma_df['received_year'] = combined_rma_df['received_date'].dt.year\n",
    "\n",
    "# Remove any date before 2012\n",
    "combined_rma_df = combined_rma_df[combined_rma_df['received_date'] >= '2012-01-01']\n",
    "if 'ship_date' in combined_rma_df.columns:\n",
    "    # Handle null ship dates\n",
    "    combined_rma_df = combined_rma_df[\n",
    "        combined_rma_df['ship_date'].isna() | \n",
    "        (combined_rma_df['ship_date'] >= '2012-01-01')\n",
    "    ]\n",
    "\n",
    "print(f\"Date range: {combined_rma_df['received_date'].min()} to {combined_rma_df['received_date'].max()}\")\n",
    "\n",
    "# Fix timezones\n",
    "for date_col in ['received_date', 'ship_date']:\n",
    "    if date_col in combined_rma_df.columns:\n",
    "        if hasattr(combined_rma_df[date_col].dtype, 'tz') and combined_rma_df[date_col].dtype.tz is not None:\n",
    "            combined_rma_df[date_col] = combined_rma_df[date_col].dt.tz_localize(None)\n",
    "\n",
    "# Rename part_number to match weekly_sequence_df\n",
    "combined_rma_df = combined_rma_df.rename(columns={'part_number': 'PartNumber'})\n",
    "\n",
    "# Process RMA data for weekly sequences\n",
    "print(\"Processing RMA data for weekly sequences...\")\n",
    "min_date = combined_rma_df['received_date'].min()\n",
    "max_date = combined_rma_df['received_date'].max()\n",
    "date_range = pd.date_range(min_date, max_date, freq='W-MON')\n",
    "print(f\"RMA date range: {date_range[0]} to {date_range[-1]}\")\n",
    "\n",
    "# Get unique part numbers in RMA data\n",
    "unique_part_numbers = combined_rma_df['PartNumber'].unique()\n",
    "print(f\"Number of unique part numbers in RMA data: {len(unique_part_numbers)}\")\n",
    "\n",
    "# Process in chunks\n",
    "chunk_size = 1000  # Process 1000 part numbers at a time\n",
    "num_chunks = (len(unique_part_numbers) + chunk_size - 1) // chunk_size\n",
    "all_rma_weekly_data = []\n",
    "\n",
    "print(f\"Processing {len(unique_part_numbers)} part numbers in {num_chunks} chunks...\")\n",
    "\n",
    "for chunk_idx in range(num_chunks):\n",
    "    start_idx = chunk_idx * chunk_size\n",
    "    end_idx = min((chunk_idx + 1) * chunk_size, len(unique_part_numbers))\n",
    "    chunk_part_numbers = unique_part_numbers[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Processing part number chunk {chunk_idx+1}/{num_chunks} ({start_idx} to {end_idx})...\")\n",
    "    \n",
    "    # Filter RMA data to only this chunk's part numbers\n",
    "    chunk_rma_df = combined_rma_df[combined_rma_df['PartNumber'].isin(chunk_part_numbers)]\n",
    "    \n",
    "    # Initialize tracking for this chunk\n",
    "    chunk_rma_weekly_data = []\n",
    "    chunk_running_rma_totals = {}\n",
    "    \n",
    "    # Process each week for these part numbers\n",
    "    for week_start in date_range:\n",
    "        week_end = week_start + timedelta(days=6)\n",
    "        \n",
    "        received_mask = (\n",
    "            (chunk_rma_df['received_date'] >= week_start) &\n",
    "            (chunk_rma_df['received_date'] <= week_end)\n",
    "        )\n",
    "        \n",
    "        shipped_mask = (\n",
    "            (chunk_rma_df['ship_date'].notna()) &\n",
    "            (chunk_rma_df['ship_date'] >= week_start) &\n",
    "            (chunk_rma_df['ship_date'] <= week_end)\n",
    "        )\n",
    "        \n",
    "        # Get counts for part numbers in this chunk for this week\n",
    "        received_parts = chunk_rma_df[received_mask]['PartNumber'].value_counts().to_dict()\n",
    "        shipped_parts = chunk_rma_df[shipped_mask]['PartNumber'].value_counts().to_dict()\n",
    "        \n",
    "        # Update data for each part number in this chunk\n",
    "        for part_num in chunk_part_numbers:\n",
    "            received_count = received_parts.get(part_num, 0)\n",
    "            shipped_count = shipped_parts.get(part_num, 0)\n",
    "            \n",
    "            # Only add to the results if there's activity\n",
    "            if received_count > 0 or shipped_count > 0 or chunk_running_rma_totals.get(part_num, 0) > 0:\n",
    "                in_repair = chunk_running_rma_totals.get(part_num, 0) + received_count - shipped_count\n",
    "                \n",
    "                chunk_rma_weekly_data.append({\n",
    "                    'PartNumber': part_num,\n",
    "                    'WeekStart': week_start,\n",
    "                    'ReceivedCount': received_count,\n",
    "                    'ShippedCount': shipped_count,\n",
    "                    'InRepair': in_repair\n",
    "                })\n",
    "                \n",
    "                # Update running total\n",
    "                chunk_running_rma_totals[part_num] = in_repair\n",
    "    \n",
    "    # Convert chunk results to DataFrame\n",
    "    if chunk_rma_weekly_data:\n",
    "        chunk_df = pd.DataFrame(chunk_rma_weekly_data)\n",
    "        all_rma_weekly_data.append(chunk_df)\n",
    "        \n",
    "        del chunk_rma_weekly_data\n",
    "        del chunk_running_rma_totals\n",
    "        del chunk_rma_df\n",
    "        gc.collect()\n",
    "    \n",
    "    # Report progress\n",
    "    print(f\"Completed part number chunk {chunk_idx+1}/{num_chunks}\")\n",
    "\n",
    "# Combine all chunks\n",
    "print(\"Combining all chunks...\")\n",
    "if all_rma_weekly_data:\n",
    "    rma_weekly_df = pd.concat(all_rma_weekly_data, ignore_index=True)\n",
    "    \n",
    "    del all_rma_weekly_data\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"RMA weekly data created with {len(rma_weekly_df)} rows\")\n",
    "    print(f\"Memory usage of rma_weekly_df: {rma_weekly_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"No RMA data to process.\")\n",
    "    rma_weekly_df = pd.DataFrame(columns=['PartNumber', 'WeekStart', 'ReceivedCount', 'ShippedCount', 'InRepair'])\n",
    "\n",
    "# Merge in chunks\n",
    "print(\"Merging RMA data into weekly sequence...\")\n",
    "\n",
    "# Get unique weeks in the weekly sequence for chunking\n",
    "unique_weeks = weekly_sequence_df['WeekStart'].unique()\n",
    "unique_weeks = np.sort(unique_weeks)\n",
    "\n",
    "# Process in chunks of weeks\n",
    "week_chunk_size = 52  # Process one year at a time\n",
    "num_week_chunks = (len(unique_weeks) + week_chunk_size - 1) // week_chunk_size\n",
    "all_merged_chunks = []\n",
    "\n",
    "for week_chunk_idx in range(num_week_chunks):\n",
    "    start_idx = week_chunk_idx * week_chunk_size\n",
    "    end_idx = min((week_chunk_idx + 1) * week_chunk_size, len(unique_weeks))\n",
    "    chunk_weeks = unique_weeks[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Processing week chunk {week_chunk_idx+1}/{num_week_chunks} ({start_idx} to {end_idx})...\")\n",
    "    \n",
    "    # Filter weekly sequence to only this chunk's weeks\n",
    "    sequence_chunk = weekly_sequence_df[weekly_sequence_df['WeekStart'].isin(chunk_weeks)].copy()\n",
    "    rma_chunk = rma_weekly_df[rma_weekly_df['WeekStart'].isin(chunk_weeks)]\n",
    "    \n",
    "    # Merge this chunk\n",
    "    if not rma_chunk.empty:\n",
    "        merged_chunk = sequence_chunk.merge(\n",
    "            rma_chunk,\n",
    "            how='left',\n",
    "            on=['WeekStart', 'PartNumber']\n",
    "        )\n",
    "        \n",
    "        # Fill missing values with 0\n",
    "        for col in ['ReceivedCount', 'ShippedCount', 'InRepair']:\n",
    "            if col in merged_chunk.columns:\n",
    "                # Only fill 0s for dates between min_date and max_date\n",
    "                date_mask = (merged_chunk['WeekStart'] >= min_date) & (merged_chunk['WeekStart'] <= max_date)\n",
    "                merged_chunk.loc[date_mask, col] = merged_chunk.loc[date_mask, col].fillna(0)\n",
    "    else:\n",
    "        merged_chunk = sequence_chunk.copy()\n",
    "        for col in ['ReceivedCount', 'ShippedCount', 'InRepair']:\n",
    "            merged_chunk[col] = np.nan\n",
    "            # Only fill 0s for dates between min_date and max_date\n",
    "            date_mask = (merged_chunk['WeekStart'] >= min_date) & (merged_chunk['WeekStart'] <= max_date)\n",
    "            merged_chunk.loc[date_mask, col] = 0\n",
    "    \n",
    "    all_merged_chunks.append(merged_chunk)\n",
    "    \n",
    "    del sequence_chunk\n",
    "    del rma_chunk\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Completed week chunk {week_chunk_idx+1}/{num_week_chunks}\")\n",
    "\n",
    "# Combine all merged chunks\n",
    "print(\"Combining all merged chunks...\")\n",
    "rma_sequence_df = pd.concat(all_merged_chunks, ignore_index=True)\n",
    "\n",
    "del all_merged_chunks\n",
    "del weekly_sequence_df\n",
    "del rma_weekly_df\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Final merged data created with {len(rma_sequence_df)} rows\")\n",
    "print(f\"Memory usage of final data: {rma_sequence_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Save in chunks\n",
    "rows_per_save_chunk = 1000000\n",
    "num_save_chunks = (len(rma_sequence_df) + rows_per_save_chunk - 1) // rows_per_save_chunk\n",
    "\n",
    "if num_save_chunks <= 1:\n",
    "    print(\"Saving merged data...\")\n",
    "    output_path = os.path.join(processed_data_dir, 'weekly_sequence_2_rma.parquet')\n",
    "    rma_sequence_df.to_parquet(output_path)\n",
    "    print(\"Data saved!\")\n",
    "else:\n",
    "    # Save in multiple chunks and then recombine\n",
    "    print(f\"Saving data in {num_save_chunks} chunks...\")\n",
    "    temp_file_paths = []\n",
    "    \n",
    "    for i in range(num_save_chunks):\n",
    "        start_idx = i * rows_per_save_chunk\n",
    "        end_idx = min((i + 1) * rows_per_save_chunk, len(rma_sequence_df))\n",
    "        \n",
    "        chunk = rma_sequence_df.iloc[start_idx:end_idx]\n",
    "        temp_path = os.path.join(processed_data_dir, f'weekly_sequence_2_rma_temp_{i}.parquet')\n",
    "        chunk.to_parquet(temp_path)\n",
    "        temp_file_paths.append(temp_path)\n",
    "        \n",
    "        del chunk\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Saved chunk {i+1}/{num_save_chunks}\")\n",
    "    \n",
    "    print(\"All chunks saved!\")\n",
    "    print(f\"Temporary files saved at: {', '.join(temp_file_paths)}\")\n",
    "\n",
    "# Combine the files\n",
    "print(\"Combining saved chunks...\")\n",
    "combined_df = pd.read_parquet(temp_file_paths[0])\n",
    "for i in range(1, len(temp_file_paths)):\n",
    "    next_chunk = pd.read_parquet(temp_file_paths[i])\n",
    "    combined_df = pd.concat([combined_df, next_chunk], ignore_index=True)\n",
    "    del next_chunk\n",
    "    gc.collect()\n",
    "# Save the combined data\n",
    "output_path = os.path.join(processed_data_dir, 'weekly_sequence_2_rma.parquet')\n",
    "combined_df.to_parquet(output_path)\n",
    "\n",
    "# Clean up temporary files  \n",
    "for temp_path in temp_file_paths:\n",
    "    os.remove(temp_path)\n",
    "print(\"Data saved and temporary files cleaned up!\")\n",
    "print(\"All processing completed!\")\n",
    "print(f\"Final data saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weekly sequence data...\n",
      "Weekly sequence loaded\n",
      "Loading flight data...\n",
      "Memory usage of merged_flight_data_df: 22.57 MB\n",
      "  TailNumber FlightStartTime FlightEndTime FlightDuration  RawResets  \\\n",
      "0     PR-MHF             NaT           NaT            NaT       <NA>   \n",
      "1    RPC9911             NaT           NaT            NaT       <NA>   \n",
      "2    RPC9911             NaT           NaT            NaT       <NA>   \n",
      "3    RPC9911             NaT           NaT            NaT       <NA>   \n",
      "4     PT-MXE             NaT           NaT            NaT       <NA>   \n",
      "\n",
      "   TotalPassengers  BusinessClass  EconomyClass  SeatResets Airline  \\\n",
      "0              NaN            NaN           NaN         NaN     LAN   \n",
      "1              NaN            NaN           NaN         NaN     PAL   \n",
      "2              NaN            NaN           NaN         NaN     PAL   \n",
      "3              NaN            NaN           NaN         NaN     PAL   \n",
      "4              NaN            NaN           NaN         NaN     LAN   \n",
      "\n",
      "      AircraftType  \n",
      "0  Airbus A320-200  \n",
      "1  Airbus A321-200  \n",
      "2  Airbus A321-200  \n",
      "3  Airbus A321-200  \n",
      "4  Airbus A321-200  \n",
      "Cleaning flight data...\n",
      "Date range: 2019-03-01 00:02:03 to 2025-04-13 22:11:29\n",
      "After filtering: 493476 rows\n",
      "Unique tails in flight data: 786\n",
      "Unique tails in sequence data: 331\n",
      "Overlapping unique tails: 137\n",
      "Sample overlapping tails: ['RPC9905', 'B18101', 'DAIKO', 'JA717A', 'B58203']\n",
      "Filtered flight data from 493476 to 88379 rows\n",
      "Processing 137 tails in 3 chunks...\n",
      "Processing tail chunk 1/3 (0 to 50)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96216/1805448141.py:120: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  chunk_weekly = chunk_data.groupby(['StandardTail', 'WeekStart']).agg({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed tail chunk 1/3\n",
      "Processing tail chunk 2/3 (50 to 100)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96216/1805448141.py:120: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  chunk_weekly = chunk_data.groupby(['StandardTail', 'WeekStart']).agg({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed tail chunk 2/3\n",
      "Processing tail chunk 3/3 (100 to 137)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96216/1805448141.py:120: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  chunk_weekly = chunk_data.groupby(['StandardTail', 'WeekStart']).agg({\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed tail chunk 3/3\n",
      "Combining all chunks...\n",
      "Weekly flight data created:\n",
      "  StandardTail  WeekStart  RawResets  TotalPassengers  BusinessClass  \\\n",
      "0        3BNBM 2019-06-04       <NA>              NaN            NaN   \n",
      "1        3BNBM 2019-07-02       <NA>              NaN            NaN   \n",
      "2        3BNBM 2024-04-09       <NA>              NaN            NaN   \n",
      "3        3BNBM 2024-04-16       <NA>              NaN            NaN   \n",
      "4        3BNBM 2024-04-23       <NA>              NaN            NaN   \n",
      "\n",
      "   EconomyClass  FlightDuration  SeatResets Airline AircraftType  \n",
      "0           NaN             NaN         NaN    <NA>         <NA>  \n",
      "1           NaN             NaN         NaN    <NA>         <NA>  \n",
      "2           NaN             NaN         NaN    <NA>         <NA>  \n",
      "3           NaN             NaN         NaN    <NA>         <NA>  \n",
      "4           NaN             NaN         NaN    <NA>         <NA>  \n",
      "Filtered from 6909194 to 2843069 rows with overlapping tails\n",
      "Processing 2843069 rows in 569 chunks...\n",
      "Processing chunk 1/569 (0 to 5000)...\n",
      "Processing chunk 2/569 (5000 to 10000)...\n",
      "Processing chunk 3/569 (10000 to 15000)...\n",
      "Processing chunk 4/569 (15000 to 20000)...\n",
      "Processing chunk 5/569 (20000 to 25000)...\n",
      "Completed 5/569 chunks (0.9%)\n",
      "Processing chunk 6/569 (25000 to 30000)...\n",
      "Processing chunk 7/569 (30000 to 35000)...\n",
      "Processing chunk 8/569 (35000 to 40000)...\n",
      "Processing chunk 9/569 (40000 to 45000)...\n",
      "Processing chunk 10/569 (45000 to 50000)...\n",
      "Completed 10/569 chunks (1.8%)\n",
      "Processing chunk 11/569 (50000 to 55000)...\n",
      "Processing chunk 12/569 (55000 to 60000)...\n",
      "Processing chunk 13/569 (60000 to 65000)...\n",
      "Processing chunk 14/569 (65000 to 70000)...\n",
      "Processing chunk 15/569 (70000 to 75000)...\n",
      "Completed 15/569 chunks (2.6%)\n",
      "Processing chunk 16/569 (75000 to 80000)...\n",
      "Processing chunk 17/569 (80000 to 85000)...\n",
      "Processing chunk 18/569 (85000 to 90000)...\n",
      "Processing chunk 19/569 (90000 to 95000)...\n",
      "Processing chunk 20/569 (95000 to 100000)...\n",
      "Completed 20/569 chunks (3.5%)\n",
      "Processing chunk 21/569 (100000 to 105000)...\n",
      "Processing chunk 22/569 (105000 to 110000)...\n",
      "Processing chunk 23/569 (110000 to 115000)...\n",
      "Processing chunk 24/569 (115000 to 120000)...\n",
      "Processing chunk 25/569 (120000 to 125000)...\n",
      "Completed 25/569 chunks (4.4%)\n",
      "Processing chunk 26/569 (125000 to 130000)...\n",
      "Processing chunk 27/569 (130000 to 135000)...\n",
      "Processing chunk 28/569 (135000 to 140000)...\n",
      "Processing chunk 29/569 (140000 to 145000)...\n",
      "Processing chunk 30/569 (145000 to 150000)...\n",
      "Completed 30/569 chunks (5.3%)\n",
      "Processing chunk 31/569 (150000 to 155000)...\n",
      "Processing chunk 32/569 (155000 to 160000)...\n",
      "Processing chunk 33/569 (160000 to 165000)...\n",
      "Processing chunk 34/569 (165000 to 170000)...\n",
      "Processing chunk 35/569 (170000 to 175000)...\n",
      "Completed 35/569 chunks (6.2%)\n",
      "Processing chunk 36/569 (175000 to 180000)...\n",
      "Processing chunk 37/569 (180000 to 185000)...\n",
      "Processing chunk 38/569 (185000 to 190000)...\n",
      "Processing chunk 39/569 (190000 to 195000)...\n",
      "Processing chunk 40/569 (195000 to 200000)...\n",
      "Completed 40/569 chunks (7.0%)\n",
      "Processing chunk 41/569 (200000 to 205000)...\n",
      "Processing chunk 42/569 (205000 to 210000)...\n",
      "Processing chunk 43/569 (210000 to 215000)...\n",
      "Processing chunk 44/569 (215000 to 220000)...\n",
      "Processing chunk 45/569 (220000 to 225000)...\n",
      "Completed 45/569 chunks (7.9%)\n",
      "Processing chunk 46/569 (225000 to 230000)...\n",
      "Processing chunk 47/569 (230000 to 235000)...\n",
      "Processing chunk 48/569 (235000 to 240000)...\n",
      "Processing chunk 49/569 (240000 to 245000)...\n",
      "Processing chunk 50/569 (245000 to 250000)...\n",
      "Completed 50/569 chunks (8.8%)\n",
      "Processing chunk 51/569 (250000 to 255000)...\n",
      "Processing chunk 52/569 (255000 to 260000)...\n",
      "Processing chunk 53/569 (260000 to 265000)...\n",
      "Processing chunk 54/569 (265000 to 270000)...\n",
      "Processing chunk 55/569 (270000 to 275000)...\n",
      "Completed 55/569 chunks (9.7%)\n",
      "Processing chunk 56/569 (275000 to 280000)...\n",
      "Processing chunk 57/569 (280000 to 285000)...\n",
      "Processing chunk 58/569 (285000 to 290000)...\n",
      "Processing chunk 59/569 (290000 to 295000)...\n",
      "Processing chunk 60/569 (295000 to 300000)...\n",
      "Completed 60/569 chunks (10.5%)\n",
      "Processing chunk 61/569 (300000 to 305000)...\n",
      "Processing chunk 62/569 (305000 to 310000)...\n",
      "Processing chunk 63/569 (310000 to 315000)...\n",
      "Processing chunk 64/569 (315000 to 320000)...\n",
      "Processing chunk 65/569 (320000 to 325000)...\n",
      "Completed 65/569 chunks (11.4%)\n",
      "Processing chunk 66/569 (325000 to 330000)...\n",
      "Processing chunk 67/569 (330000 to 335000)...\n",
      "Processing chunk 68/569 (335000 to 340000)...\n",
      "Processing chunk 69/569 (340000 to 345000)...\n",
      "Processing chunk 70/569 (345000 to 350000)...\n",
      "Completed 70/569 chunks (12.3%)\n",
      "Processing chunk 71/569 (350000 to 355000)...\n",
      "Processing chunk 72/569 (355000 to 360000)...\n",
      "Processing chunk 73/569 (360000 to 365000)...\n",
      "Processing chunk 74/569 (365000 to 370000)...\n",
      "Processing chunk 75/569 (370000 to 375000)...\n",
      "Completed 75/569 chunks (13.2%)\n",
      "Processing chunk 76/569 (375000 to 380000)...\n",
      "Processing chunk 77/569 (380000 to 385000)...\n",
      "Processing chunk 78/569 (385000 to 390000)...\n",
      "Processing chunk 79/569 (390000 to 395000)...\n",
      "Processing chunk 80/569 (395000 to 400000)...\n",
      "Completed 80/569 chunks (14.1%)\n",
      "Processing chunk 81/569 (400000 to 405000)...\n",
      "Processing chunk 82/569 (405000 to 410000)...\n",
      "Processing chunk 83/569 (410000 to 415000)...\n",
      "Processing chunk 84/569 (415000 to 420000)...\n",
      "Processing chunk 85/569 (420000 to 425000)...\n",
      "Completed 85/569 chunks (14.9%)\n",
      "Processing chunk 86/569 (425000 to 430000)...\n",
      "Processing chunk 87/569 (430000 to 435000)...\n",
      "Processing chunk 88/569 (435000 to 440000)...\n",
      "Processing chunk 89/569 (440000 to 445000)...\n",
      "Processing chunk 90/569 (445000 to 450000)...\n",
      "Completed 90/569 chunks (15.8%)\n",
      "Processing chunk 91/569 (450000 to 455000)...\n",
      "Processing chunk 92/569 (455000 to 460000)...\n",
      "Processing chunk 93/569 (460000 to 465000)...\n",
      "Processing chunk 94/569 (465000 to 470000)...\n",
      "Processing chunk 95/569 (470000 to 475000)...\n",
      "Completed 95/569 chunks (16.7%)\n",
      "Processing chunk 96/569 (475000 to 480000)...\n",
      "Processing chunk 97/569 (480000 to 485000)...\n",
      "Processing chunk 98/569 (485000 to 490000)...\n",
      "Processing chunk 99/569 (490000 to 495000)...\n",
      "Processing chunk 100/569 (495000 to 500000)...\n",
      "Completed 100/569 chunks (17.6%)\n",
      "Processing chunk 101/569 (500000 to 505000)...\n",
      "Processing chunk 102/569 (505000 to 510000)...\n",
      "Processing chunk 103/569 (510000 to 515000)...\n",
      "Processing chunk 104/569 (515000 to 520000)...\n",
      "Processing chunk 105/569 (520000 to 525000)...\n",
      "Completed 105/569 chunks (18.5%)\n",
      "Processing chunk 106/569 (525000 to 530000)...\n",
      "Processing chunk 107/569 (530000 to 535000)...\n",
      "Processing chunk 108/569 (535000 to 540000)...\n",
      "Processing chunk 109/569 (540000 to 545000)...\n",
      "Processing chunk 110/569 (545000 to 550000)...\n",
      "Completed 110/569 chunks (19.3%)\n",
      "Processing chunk 111/569 (550000 to 555000)...\n",
      "Processing chunk 112/569 (555000 to 560000)...\n",
      "Processing chunk 113/569 (560000 to 565000)...\n",
      "Processing chunk 114/569 (565000 to 570000)...\n",
      "Processing chunk 115/569 (570000 to 575000)...\n",
      "Completed 115/569 chunks (20.2%)\n",
      "Processing chunk 116/569 (575000 to 580000)...\n",
      "Processing chunk 117/569 (580000 to 585000)...\n",
      "Processing chunk 118/569 (585000 to 590000)...\n",
      "Processing chunk 119/569 (590000 to 595000)...\n",
      "Processing chunk 120/569 (595000 to 600000)...\n",
      "Completed 120/569 chunks (21.1%)\n",
      "Processing chunk 121/569 (600000 to 605000)...\n",
      "Processing chunk 122/569 (605000 to 610000)...\n",
      "Processing chunk 123/569 (610000 to 615000)...\n",
      "Processing chunk 124/569 (615000 to 620000)...\n",
      "Processing chunk 125/569 (620000 to 625000)...\n",
      "Completed 125/569 chunks (22.0%)\n",
      "Processing chunk 126/569 (625000 to 630000)...\n",
      "Processing chunk 127/569 (630000 to 635000)...\n",
      "Processing chunk 128/569 (635000 to 640000)...\n",
      "Processing chunk 129/569 (640000 to 645000)...\n",
      "Processing chunk 130/569 (645000 to 650000)...\n",
      "Completed 130/569 chunks (22.8%)\n",
      "Processing chunk 131/569 (650000 to 655000)...\n",
      "Processing chunk 132/569 (655000 to 660000)...\n",
      "Processing chunk 133/569 (660000 to 665000)...\n",
      "Processing chunk 134/569 (665000 to 670000)...\n",
      "Processing chunk 135/569 (670000 to 675000)...\n",
      "Completed 135/569 chunks (23.7%)\n",
      "Processing chunk 136/569 (675000 to 680000)...\n",
      "Processing chunk 137/569 (680000 to 685000)...\n",
      "Processing chunk 138/569 (685000 to 690000)...\n",
      "Processing chunk 139/569 (690000 to 695000)...\n",
      "Processing chunk 140/569 (695000 to 700000)...\n",
      "Completed 140/569 chunks (24.6%)\n",
      "Processing chunk 141/569 (700000 to 705000)...\n",
      "Processing chunk 142/569 (705000 to 710000)...\n",
      "Processing chunk 143/569 (710000 to 715000)...\n",
      "Processing chunk 144/569 (715000 to 720000)...\n",
      "Processing chunk 145/569 (720000 to 725000)...\n",
      "Completed 145/569 chunks (25.5%)\n",
      "Processing chunk 146/569 (725000 to 730000)...\n",
      "Processing chunk 147/569 (730000 to 735000)...\n",
      "Processing chunk 148/569 (735000 to 740000)...\n",
      "Processing chunk 149/569 (740000 to 745000)...\n",
      "Processing chunk 150/569 (745000 to 750000)...\n",
      "Completed 150/569 chunks (26.4%)\n",
      "Processing chunk 151/569 (750000 to 755000)...\n",
      "Processing chunk 152/569 (755000 to 760000)...\n",
      "Processing chunk 153/569 (760000 to 765000)...\n",
      "Processing chunk 154/569 (765000 to 770000)...\n",
      "Processing chunk 155/569 (770000 to 775000)...\n",
      "Completed 155/569 chunks (27.2%)\n",
      "Processing chunk 156/569 (775000 to 780000)...\n",
      "Processing chunk 157/569 (780000 to 785000)...\n",
      "Processing chunk 158/569 (785000 to 790000)...\n",
      "Processing chunk 159/569 (790000 to 795000)...\n",
      "Processing chunk 160/569 (795000 to 800000)...\n",
      "Completed 160/569 chunks (28.1%)\n",
      "Processing chunk 161/569 (800000 to 805000)...\n",
      "Processing chunk 162/569 (805000 to 810000)...\n",
      "Processing chunk 163/569 (810000 to 815000)...\n",
      "Processing chunk 164/569 (815000 to 820000)...\n",
      "Processing chunk 165/569 (820000 to 825000)...\n",
      "Completed 165/569 chunks (29.0%)\n",
      "Processing chunk 166/569 (825000 to 830000)...\n",
      "Processing chunk 167/569 (830000 to 835000)...\n",
      "Processing chunk 168/569 (835000 to 840000)...\n",
      "Processing chunk 169/569 (840000 to 845000)...\n",
      "Processing chunk 170/569 (845000 to 850000)...\n",
      "Completed 170/569 chunks (29.9%)\n",
      "Processing chunk 171/569 (850000 to 855000)...\n",
      "Processing chunk 172/569 (855000 to 860000)...\n",
      "Processing chunk 173/569 (860000 to 865000)...\n",
      "Processing chunk 174/569 (865000 to 870000)...\n",
      "Processing chunk 175/569 (870000 to 875000)...\n",
      "Completed 175/569 chunks (30.8%)\n",
      "Processing chunk 176/569 (875000 to 880000)...\n",
      "Processing chunk 177/569 (880000 to 885000)...\n",
      "Processing chunk 178/569 (885000 to 890000)...\n",
      "Processing chunk 179/569 (890000 to 895000)...\n",
      "Processing chunk 180/569 (895000 to 900000)...\n",
      "Completed 180/569 chunks (31.6%)\n",
      "Processing chunk 181/569 (900000 to 905000)...\n",
      "Processing chunk 182/569 (905000 to 910000)...\n",
      "Processing chunk 183/569 (910000 to 915000)...\n",
      "Processing chunk 184/569 (915000 to 920000)...\n",
      "Processing chunk 185/569 (920000 to 925000)...\n",
      "Completed 185/569 chunks (32.5%)\n",
      "Processing chunk 186/569 (925000 to 930000)...\n",
      "Processing chunk 187/569 (930000 to 935000)...\n",
      "Processing chunk 188/569 (935000 to 940000)...\n",
      "Processing chunk 189/569 (940000 to 945000)...\n",
      "Processing chunk 190/569 (945000 to 950000)...\n",
      "Completed 190/569 chunks (33.4%)\n",
      "Processing chunk 191/569 (950000 to 955000)...\n",
      "Processing chunk 192/569 (955000 to 960000)...\n",
      "Processing chunk 193/569 (960000 to 965000)...\n",
      "Processing chunk 194/569 (965000 to 970000)...\n",
      "Processing chunk 195/569 (970000 to 975000)...\n",
      "Completed 195/569 chunks (34.3%)\n",
      "Processing chunk 196/569 (975000 to 980000)...\n",
      "Processing chunk 197/569 (980000 to 985000)...\n",
      "Processing chunk 198/569 (985000 to 990000)...\n",
      "Processing chunk 199/569 (990000 to 995000)...\n",
      "Processing chunk 200/569 (995000 to 1000000)...\n",
      "Completed 200/569 chunks (35.1%)\n",
      "Processing chunk 201/569 (1000000 to 1005000)...\n",
      "Processing chunk 202/569 (1005000 to 1010000)...\n",
      "Processing chunk 203/569 (1010000 to 1015000)...\n",
      "Processing chunk 204/569 (1015000 to 1020000)...\n",
      "Processing chunk 205/569 (1020000 to 1025000)...\n",
      "Completed 205/569 chunks (36.0%)\n",
      "Processing chunk 206/569 (1025000 to 1030000)...\n",
      "Processing chunk 207/569 (1030000 to 1035000)...\n",
      "Processing chunk 208/569 (1035000 to 1040000)...\n",
      "Processing chunk 209/569 (1040000 to 1045000)...\n",
      "Processing chunk 210/569 (1045000 to 1050000)...\n",
      "Completed 210/569 chunks (36.9%)\n",
      "Processing chunk 211/569 (1050000 to 1055000)...\n",
      "Processing chunk 212/569 (1055000 to 1060000)...\n",
      "Processing chunk 213/569 (1060000 to 1065000)...\n",
      "Processing chunk 214/569 (1065000 to 1070000)...\n",
      "Processing chunk 215/569 (1070000 to 1075000)...\n",
      "Completed 215/569 chunks (37.8%)\n",
      "Processing chunk 216/569 (1075000 to 1080000)...\n",
      "Processing chunk 217/569 (1080000 to 1085000)...\n",
      "Processing chunk 218/569 (1085000 to 1090000)...\n",
      "Processing chunk 219/569 (1090000 to 1095000)...\n",
      "Processing chunk 220/569 (1095000 to 1100000)...\n",
      "Completed 220/569 chunks (38.7%)\n",
      "Processing chunk 221/569 (1100000 to 1105000)...\n",
      "Processing chunk 222/569 (1105000 to 1110000)...\n",
      "Processing chunk 223/569 (1110000 to 1115000)...\n",
      "Processing chunk 224/569 (1115000 to 1120000)...\n",
      "Processing chunk 225/569 (1120000 to 1125000)...\n",
      "Completed 225/569 chunks (39.5%)\n",
      "Processing chunk 226/569 (1125000 to 1130000)...\n",
      "Processing chunk 227/569 (1130000 to 1135000)...\n",
      "Processing chunk 228/569 (1135000 to 1140000)...\n",
      "Processing chunk 229/569 (1140000 to 1145000)...\n",
      "Processing chunk 230/569 (1145000 to 1150000)...\n",
      "Completed 230/569 chunks (40.4%)\n",
      "Processing chunk 231/569 (1150000 to 1155000)...\n",
      "Processing chunk 232/569 (1155000 to 1160000)...\n",
      "Processing chunk 233/569 (1160000 to 1165000)...\n",
      "Processing chunk 234/569 (1165000 to 1170000)...\n",
      "Processing chunk 235/569 (1170000 to 1175000)...\n",
      "Completed 235/569 chunks (41.3%)\n",
      "Processing chunk 236/569 (1175000 to 1180000)...\n",
      "Processing chunk 237/569 (1180000 to 1185000)...\n",
      "Processing chunk 238/569 (1185000 to 1190000)...\n",
      "Processing chunk 239/569 (1190000 to 1195000)...\n",
      "Processing chunk 240/569 (1195000 to 1200000)...\n",
      "Completed 240/569 chunks (42.2%)\n",
      "Processing chunk 241/569 (1200000 to 1205000)...\n",
      "Processing chunk 242/569 (1205000 to 1210000)...\n",
      "Processing chunk 243/569 (1210000 to 1215000)...\n",
      "Processing chunk 244/569 (1215000 to 1220000)...\n",
      "Processing chunk 245/569 (1220000 to 1225000)...\n",
      "Completed 245/569 chunks (43.1%)\n",
      "Processing chunk 246/569 (1225000 to 1230000)...\n",
      "Processing chunk 247/569 (1230000 to 1235000)...\n",
      "Processing chunk 248/569 (1235000 to 1240000)...\n",
      "Processing chunk 249/569 (1240000 to 1245000)...\n",
      "Processing chunk 250/569 (1245000 to 1250000)...\n",
      "Completed 250/569 chunks (43.9%)\n",
      "Processing chunk 251/569 (1250000 to 1255000)...\n",
      "Processing chunk 252/569 (1255000 to 1260000)...\n",
      "Processing chunk 253/569 (1260000 to 1265000)...\n",
      "Processing chunk 254/569 (1265000 to 1270000)...\n",
      "Processing chunk 255/569 (1270000 to 1275000)...\n",
      "Completed 255/569 chunks (44.8%)\n",
      "Processing chunk 256/569 (1275000 to 1280000)...\n",
      "Processing chunk 257/569 (1280000 to 1285000)...\n",
      "Processing chunk 258/569 (1285000 to 1290000)...\n",
      "Processing chunk 259/569 (1290000 to 1295000)...\n",
      "Processing chunk 260/569 (1295000 to 1300000)...\n",
      "Completed 260/569 chunks (45.7%)\n",
      "Processing chunk 261/569 (1300000 to 1305000)...\n",
      "Processing chunk 262/569 (1305000 to 1310000)...\n",
      "Processing chunk 263/569 (1310000 to 1315000)...\n",
      "Processing chunk 264/569 (1315000 to 1320000)...\n",
      "Processing chunk 265/569 (1320000 to 1325000)...\n",
      "Completed 265/569 chunks (46.6%)\n",
      "Processing chunk 266/569 (1325000 to 1330000)...\n",
      "Processing chunk 267/569 (1330000 to 1335000)...\n",
      "Processing chunk 268/569 (1335000 to 1340000)...\n",
      "Processing chunk 269/569 (1340000 to 1345000)...\n",
      "Processing chunk 270/569 (1345000 to 1350000)...\n",
      "Completed 270/569 chunks (47.5%)\n",
      "Processing chunk 271/569 (1350000 to 1355000)...\n",
      "Processing chunk 272/569 (1355000 to 1360000)...\n",
      "Processing chunk 273/569 (1360000 to 1365000)...\n",
      "Processing chunk 274/569 (1365000 to 1370000)...\n",
      "Processing chunk 275/569 (1370000 to 1375000)...\n",
      "Completed 275/569 chunks (48.3%)\n",
      "Processing chunk 276/569 (1375000 to 1380000)...\n",
      "Processing chunk 277/569 (1380000 to 1385000)...\n",
      "Processing chunk 278/569 (1385000 to 1390000)...\n",
      "Processing chunk 279/569 (1390000 to 1395000)...\n",
      "Processing chunk 280/569 (1395000 to 1400000)...\n",
      "Completed 280/569 chunks (49.2%)\n",
      "Processing chunk 281/569 (1400000 to 1405000)...\n",
      "Processing chunk 282/569 (1405000 to 1410000)...\n",
      "Processing chunk 283/569 (1410000 to 1415000)...\n",
      "Processing chunk 284/569 (1415000 to 1420000)...\n",
      "Processing chunk 285/569 (1420000 to 1425000)...\n",
      "Completed 285/569 chunks (50.1%)\n",
      "Processing chunk 286/569 (1425000 to 1430000)...\n",
      "Processing chunk 287/569 (1430000 to 1435000)...\n",
      "Processing chunk 288/569 (1435000 to 1440000)...\n",
      "Processing chunk 289/569 (1440000 to 1445000)...\n",
      "Processing chunk 290/569 (1445000 to 1450000)...\n",
      "Completed 290/569 chunks (51.0%)\n",
      "Processing chunk 291/569 (1450000 to 1455000)...\n",
      "Processing chunk 292/569 (1455000 to 1460000)...\n",
      "Processing chunk 293/569 (1460000 to 1465000)...\n",
      "Processing chunk 294/569 (1465000 to 1470000)...\n",
      "Processing chunk 295/569 (1470000 to 1475000)...\n",
      "Completed 295/569 chunks (51.8%)\n",
      "Processing chunk 296/569 (1475000 to 1480000)...\n",
      "Processing chunk 297/569 (1480000 to 1485000)...\n",
      "Processing chunk 298/569 (1485000 to 1490000)...\n",
      "Processing chunk 299/569 (1490000 to 1495000)...\n",
      "Processing chunk 300/569 (1495000 to 1500000)...\n",
      "Completed 300/569 chunks (52.7%)\n",
      "Processing chunk 301/569 (1500000 to 1505000)...\n",
      "Processing chunk 302/569 (1505000 to 1510000)...\n",
      "Processing chunk 303/569 (1510000 to 1515000)...\n",
      "Processing chunk 304/569 (1515000 to 1520000)...\n",
      "Processing chunk 305/569 (1520000 to 1525000)...\n",
      "Completed 305/569 chunks (53.6%)\n",
      "Processing chunk 306/569 (1525000 to 1530000)...\n",
      "Processing chunk 307/569 (1530000 to 1535000)...\n",
      "Processing chunk 308/569 (1535000 to 1540000)...\n",
      "Processing chunk 309/569 (1540000 to 1545000)...\n",
      "Processing chunk 310/569 (1545000 to 1550000)...\n",
      "Completed 310/569 chunks (54.5%)\n",
      "Processing chunk 311/569 (1550000 to 1555000)...\n",
      "Processing chunk 312/569 (1555000 to 1560000)...\n",
      "Processing chunk 313/569 (1560000 to 1565000)...\n",
      "Processing chunk 314/569 (1565000 to 1570000)...\n",
      "Processing chunk 315/569 (1570000 to 1575000)...\n",
      "Completed 315/569 chunks (55.4%)\n",
      "Processing chunk 316/569 (1575000 to 1580000)...\n",
      "Processing chunk 317/569 (1580000 to 1585000)...\n",
      "Processing chunk 318/569 (1585000 to 1590000)...\n",
      "Processing chunk 319/569 (1590000 to 1595000)...\n",
      "Processing chunk 320/569 (1595000 to 1600000)...\n",
      "Completed 320/569 chunks (56.2%)\n",
      "Processing chunk 321/569 (1600000 to 1605000)...\n",
      "Processing chunk 322/569 (1605000 to 1610000)...\n",
      "Processing chunk 323/569 (1610000 to 1615000)...\n",
      "Processing chunk 324/569 (1615000 to 1620000)...\n",
      "Processing chunk 325/569 (1620000 to 1625000)...\n",
      "Completed 325/569 chunks (57.1%)\n",
      "Processing chunk 326/569 (1625000 to 1630000)...\n",
      "Processing chunk 327/569 (1630000 to 1635000)...\n",
      "Processing chunk 328/569 (1635000 to 1640000)...\n",
      "Processing chunk 329/569 (1640000 to 1645000)...\n",
      "Processing chunk 330/569 (1645000 to 1650000)...\n",
      "Completed 330/569 chunks (58.0%)\n",
      "Processing chunk 331/569 (1650000 to 1655000)...\n",
      "Processing chunk 332/569 (1655000 to 1660000)...\n",
      "Processing chunk 333/569 (1660000 to 1665000)...\n",
      "Processing chunk 334/569 (1665000 to 1670000)...\n",
      "Processing chunk 335/569 (1670000 to 1675000)...\n",
      "Completed 335/569 chunks (58.9%)\n",
      "Processing chunk 336/569 (1675000 to 1680000)...\n",
      "Processing chunk 337/569 (1680000 to 1685000)...\n",
      "Processing chunk 338/569 (1685000 to 1690000)...\n",
      "Processing chunk 339/569 (1690000 to 1695000)...\n",
      "Processing chunk 340/569 (1695000 to 1700000)...\n",
      "Completed 340/569 chunks (59.8%)\n",
      "Processing chunk 341/569 (1700000 to 1705000)...\n",
      "Processing chunk 342/569 (1705000 to 1710000)...\n",
      "Processing chunk 343/569 (1710000 to 1715000)...\n",
      "Processing chunk 344/569 (1715000 to 1720000)...\n",
      "Processing chunk 345/569 (1720000 to 1725000)...\n",
      "Completed 345/569 chunks (60.6%)\n",
      "Processing chunk 346/569 (1725000 to 1730000)...\n",
      "Processing chunk 347/569 (1730000 to 1735000)...\n",
      "Processing chunk 348/569 (1735000 to 1740000)...\n",
      "Processing chunk 349/569 (1740000 to 1745000)...\n",
      "Processing chunk 350/569 (1745000 to 1750000)...\n",
      "Completed 350/569 chunks (61.5%)\n",
      "Processing chunk 351/569 (1750000 to 1755000)...\n",
      "Processing chunk 352/569 (1755000 to 1760000)...\n",
      "Processing chunk 353/569 (1760000 to 1765000)...\n",
      "Processing chunk 354/569 (1765000 to 1770000)...\n",
      "Processing chunk 355/569 (1770000 to 1775000)...\n",
      "Completed 355/569 chunks (62.4%)\n",
      "Processing chunk 356/569 (1775000 to 1780000)...\n",
      "Processing chunk 357/569 (1780000 to 1785000)...\n",
      "Processing chunk 358/569 (1785000 to 1790000)...\n",
      "Processing chunk 359/569 (1790000 to 1795000)...\n",
      "Processing chunk 360/569 (1795000 to 1800000)...\n",
      "Completed 360/569 chunks (63.3%)\n",
      "Processing chunk 361/569 (1800000 to 1805000)...\n",
      "Processing chunk 362/569 (1805000 to 1810000)...\n",
      "Processing chunk 363/569 (1810000 to 1815000)...\n",
      "Processing chunk 364/569 (1815000 to 1820000)...\n",
      "Processing chunk 365/569 (1820000 to 1825000)...\n",
      "Completed 365/569 chunks (64.1%)\n",
      "Processing chunk 366/569 (1825000 to 1830000)...\n",
      "Processing chunk 367/569 (1830000 to 1835000)...\n",
      "Processing chunk 368/569 (1835000 to 1840000)...\n",
      "Processing chunk 369/569 (1840000 to 1845000)...\n",
      "Processing chunk 370/569 (1845000 to 1850000)...\n",
      "Completed 370/569 chunks (65.0%)\n",
      "Processing chunk 371/569 (1850000 to 1855000)...\n",
      "Processing chunk 372/569 (1855000 to 1860000)...\n",
      "Processing chunk 373/569 (1860000 to 1865000)...\n",
      "Processing chunk 374/569 (1865000 to 1870000)...\n",
      "Processing chunk 375/569 (1870000 to 1875000)...\n",
      "Completed 375/569 chunks (65.9%)\n",
      "Processing chunk 376/569 (1875000 to 1880000)...\n",
      "Processing chunk 377/569 (1880000 to 1885000)...\n",
      "Processing chunk 378/569 (1885000 to 1890000)...\n",
      "Processing chunk 379/569 (1890000 to 1895000)...\n",
      "Processing chunk 380/569 (1895000 to 1900000)...\n",
      "Completed 380/569 chunks (66.8%)\n",
      "Processing chunk 381/569 (1900000 to 1905000)...\n",
      "Processing chunk 382/569 (1905000 to 1910000)...\n",
      "Processing chunk 383/569 (1910000 to 1915000)...\n",
      "Processing chunk 384/569 (1915000 to 1920000)...\n",
      "Processing chunk 385/569 (1920000 to 1925000)...\n",
      "Completed 385/569 chunks (67.7%)\n",
      "Processing chunk 386/569 (1925000 to 1930000)...\n",
      "Processing chunk 387/569 (1930000 to 1935000)...\n",
      "Processing chunk 388/569 (1935000 to 1940000)...\n",
      "Processing chunk 389/569 (1940000 to 1945000)...\n",
      "Processing chunk 390/569 (1945000 to 1950000)...\n",
      "Completed 390/569 chunks (68.5%)\n",
      "Processing chunk 391/569 (1950000 to 1955000)...\n",
      "Processing chunk 392/569 (1955000 to 1960000)...\n",
      "Processing chunk 393/569 (1960000 to 1965000)...\n",
      "Processing chunk 394/569 (1965000 to 1970000)...\n",
      "Processing chunk 395/569 (1970000 to 1975000)...\n",
      "Completed 395/569 chunks (69.4%)\n",
      "Processing chunk 396/569 (1975000 to 1980000)...\n",
      "Processing chunk 397/569 (1980000 to 1985000)...\n",
      "Processing chunk 398/569 (1985000 to 1990000)...\n",
      "Processing chunk 399/569 (1990000 to 1995000)...\n",
      "Processing chunk 400/569 (1995000 to 2000000)...\n",
      "Completed 400/569 chunks (70.3%)\n",
      "Processing chunk 401/569 (2000000 to 2005000)...\n",
      "Processing chunk 402/569 (2005000 to 2010000)...\n",
      "Processing chunk 403/569 (2010000 to 2015000)...\n",
      "Processing chunk 404/569 (2015000 to 2020000)...\n",
      "Processing chunk 405/569 (2020000 to 2025000)...\n",
      "Completed 405/569 chunks (71.2%)\n",
      "Processing chunk 406/569 (2025000 to 2030000)...\n",
      "Processing chunk 407/569 (2030000 to 2035000)...\n",
      "Processing chunk 408/569 (2035000 to 2040000)...\n",
      "Processing chunk 409/569 (2040000 to 2045000)...\n",
      "Processing chunk 410/569 (2045000 to 2050000)...\n",
      "Completed 410/569 chunks (72.1%)\n",
      "Processing chunk 411/569 (2050000 to 2055000)...\n",
      "Processing chunk 412/569 (2055000 to 2060000)...\n",
      "Processing chunk 413/569 (2060000 to 2065000)...\n",
      "Processing chunk 414/569 (2065000 to 2070000)...\n",
      "Processing chunk 415/569 (2070000 to 2075000)...\n",
      "Completed 415/569 chunks (72.9%)\n",
      "Processing chunk 416/569 (2075000 to 2080000)...\n",
      "Processing chunk 417/569 (2080000 to 2085000)...\n",
      "Processing chunk 418/569 (2085000 to 2090000)...\n",
      "Processing chunk 419/569 (2090000 to 2095000)...\n",
      "Processing chunk 420/569 (2095000 to 2100000)...\n",
      "Completed 420/569 chunks (73.8%)\n",
      "Processing chunk 421/569 (2100000 to 2105000)...\n",
      "Processing chunk 422/569 (2105000 to 2110000)...\n",
      "Processing chunk 423/569 (2110000 to 2115000)...\n",
      "Processing chunk 424/569 (2115000 to 2120000)...\n",
      "Processing chunk 425/569 (2120000 to 2125000)...\n",
      "Completed 425/569 chunks (74.7%)\n",
      "Processing chunk 426/569 (2125000 to 2130000)...\n",
      "Processing chunk 427/569 (2130000 to 2135000)...\n",
      "Processing chunk 428/569 (2135000 to 2140000)...\n",
      "Processing chunk 429/569 (2140000 to 2145000)...\n",
      "Processing chunk 430/569 (2145000 to 2150000)...\n",
      "Completed 430/569 chunks (75.6%)\n",
      "Processing chunk 431/569 (2150000 to 2155000)...\n",
      "Processing chunk 432/569 (2155000 to 2160000)...\n",
      "Processing chunk 433/569 (2160000 to 2165000)...\n",
      "Processing chunk 434/569 (2165000 to 2170000)...\n",
      "Processing chunk 435/569 (2170000 to 2175000)...\n",
      "Completed 435/569 chunks (76.4%)\n",
      "Processing chunk 436/569 (2175000 to 2180000)...\n",
      "Processing chunk 437/569 (2180000 to 2185000)...\n",
      "Processing chunk 438/569 (2185000 to 2190000)...\n",
      "Processing chunk 439/569 (2190000 to 2195000)...\n",
      "Processing chunk 440/569 (2195000 to 2200000)...\n",
      "Completed 440/569 chunks (77.3%)\n",
      "Processing chunk 441/569 (2200000 to 2205000)...\n",
      "Processing chunk 442/569 (2205000 to 2210000)...\n",
      "Processing chunk 443/569 (2210000 to 2215000)...\n",
      "Processing chunk 444/569 (2215000 to 2220000)...\n",
      "Processing chunk 445/569 (2220000 to 2225000)...\n",
      "Completed 445/569 chunks (78.2%)\n",
      "Processing chunk 446/569 (2225000 to 2230000)...\n",
      "Processing chunk 447/569 (2230000 to 2235000)...\n",
      "Processing chunk 448/569 (2235000 to 2240000)...\n",
      "Processing chunk 449/569 (2240000 to 2245000)...\n",
      "Processing chunk 450/569 (2245000 to 2250000)...\n",
      "Completed 450/569 chunks (79.1%)\n",
      "Processing chunk 451/569 (2250000 to 2255000)...\n",
      "Processing chunk 452/569 (2255000 to 2260000)...\n",
      "Processing chunk 453/569 (2260000 to 2265000)...\n",
      "Processing chunk 454/569 (2265000 to 2270000)...\n",
      "Processing chunk 455/569 (2270000 to 2275000)...\n",
      "Completed 455/569 chunks (80.0%)\n",
      "Processing chunk 456/569 (2275000 to 2280000)...\n",
      "Processing chunk 457/569 (2280000 to 2285000)...\n",
      "Processing chunk 458/569 (2285000 to 2290000)...\n",
      "Processing chunk 459/569 (2290000 to 2295000)...\n",
      "Processing chunk 460/569 (2295000 to 2300000)...\n",
      "Completed 460/569 chunks (80.8%)\n",
      "Processing chunk 461/569 (2300000 to 2305000)...\n",
      "Processing chunk 462/569 (2305000 to 2310000)...\n",
      "Processing chunk 463/569 (2310000 to 2315000)...\n",
      "Processing chunk 464/569 (2315000 to 2320000)...\n",
      "Processing chunk 465/569 (2320000 to 2325000)...\n",
      "Completed 465/569 chunks (81.7%)\n",
      "Processing chunk 466/569 (2325000 to 2330000)...\n",
      "Processing chunk 467/569 (2330000 to 2335000)...\n",
      "Processing chunk 468/569 (2335000 to 2340000)...\n",
      "Processing chunk 469/569 (2340000 to 2345000)...\n",
      "Processing chunk 470/569 (2345000 to 2350000)...\n",
      "Completed 470/569 chunks (82.6%)\n",
      "Processing chunk 471/569 (2350000 to 2355000)...\n",
      "Processing chunk 472/569 (2355000 to 2360000)...\n",
      "Processing chunk 473/569 (2360000 to 2365000)...\n",
      "Processing chunk 474/569 (2365000 to 2370000)...\n",
      "Processing chunk 475/569 (2370000 to 2375000)...\n",
      "Completed 475/569 chunks (83.5%)\n",
      "Processing chunk 476/569 (2375000 to 2380000)...\n",
      "Processing chunk 477/569 (2380000 to 2385000)...\n",
      "Processing chunk 478/569 (2385000 to 2390000)...\n",
      "Processing chunk 479/569 (2390000 to 2395000)...\n",
      "Processing chunk 480/569 (2395000 to 2400000)...\n",
      "Completed 480/569 chunks (84.4%)\n",
      "Processing chunk 481/569 (2400000 to 2405000)...\n",
      "Processing chunk 482/569 (2405000 to 2410000)...\n",
      "Processing chunk 483/569 (2410000 to 2415000)...\n",
      "Processing chunk 484/569 (2415000 to 2420000)...\n",
      "Processing chunk 485/569 (2420000 to 2425000)...\n",
      "Completed 485/569 chunks (85.2%)\n",
      "Processing chunk 486/569 (2425000 to 2430000)...\n",
      "Processing chunk 487/569 (2430000 to 2435000)...\n",
      "Processing chunk 488/569 (2435000 to 2440000)...\n",
      "Processing chunk 489/569 (2440000 to 2445000)...\n",
      "Processing chunk 490/569 (2445000 to 2450000)...\n",
      "Completed 490/569 chunks (86.1%)\n",
      "Processing chunk 491/569 (2450000 to 2455000)...\n",
      "Processing chunk 492/569 (2455000 to 2460000)...\n",
      "Processing chunk 493/569 (2460000 to 2465000)...\n",
      "Processing chunk 494/569 (2465000 to 2470000)...\n",
      "Processing chunk 495/569 (2470000 to 2475000)...\n",
      "Completed 495/569 chunks (87.0%)\n",
      "Processing chunk 496/569 (2475000 to 2480000)...\n",
      "Processing chunk 497/569 (2480000 to 2485000)...\n",
      "Processing chunk 498/569 (2485000 to 2490000)...\n",
      "Processing chunk 499/569 (2490000 to 2495000)...\n",
      "Processing chunk 500/569 (2495000 to 2500000)...\n",
      "Completed 500/569 chunks (87.9%)\n",
      "Processing chunk 501/569 (2500000 to 2505000)...\n",
      "Processing chunk 502/569 (2505000 to 2510000)...\n",
      "Processing chunk 503/569 (2510000 to 2515000)...\n",
      "Processing chunk 504/569 (2515000 to 2520000)...\n",
      "Processing chunk 505/569 (2520000 to 2525000)...\n",
      "Completed 505/569 chunks (88.8%)\n",
      "Processing chunk 506/569 (2525000 to 2530000)...\n",
      "Processing chunk 507/569 (2530000 to 2535000)...\n",
      "Processing chunk 508/569 (2535000 to 2540000)...\n",
      "Processing chunk 509/569 (2540000 to 2545000)...\n",
      "Processing chunk 510/569 (2545000 to 2550000)...\n",
      "Completed 510/569 chunks (89.6%)\n",
      "Processing chunk 511/569 (2550000 to 2555000)...\n",
      "Processing chunk 512/569 (2555000 to 2560000)...\n",
      "Processing chunk 513/569 (2560000 to 2565000)...\n",
      "Processing chunk 514/569 (2565000 to 2570000)...\n",
      "Processing chunk 515/569 (2570000 to 2575000)...\n",
      "Completed 515/569 chunks (90.5%)\n",
      "Processing chunk 516/569 (2575000 to 2580000)...\n",
      "Processing chunk 517/569 (2580000 to 2585000)...\n",
      "Processing chunk 518/569 (2585000 to 2590000)...\n",
      "Processing chunk 519/569 (2590000 to 2595000)...\n",
      "Processing chunk 520/569 (2595000 to 2600000)...\n",
      "Completed 520/569 chunks (91.4%)\n",
      "Processing chunk 521/569 (2600000 to 2605000)...\n",
      "Processing chunk 522/569 (2605000 to 2610000)...\n",
      "Processing chunk 523/569 (2610000 to 2615000)...\n",
      "Processing chunk 524/569 (2615000 to 2620000)...\n",
      "Processing chunk 525/569 (2620000 to 2625000)...\n",
      "Completed 525/569 chunks (92.3%)\n",
      "Processing chunk 526/569 (2625000 to 2630000)...\n",
      "Processing chunk 527/569 (2630000 to 2635000)...\n",
      "Processing chunk 528/569 (2635000 to 2640000)...\n",
      "Processing chunk 529/569 (2640000 to 2645000)...\n",
      "Processing chunk 530/569 (2645000 to 2650000)...\n",
      "Completed 530/569 chunks (93.1%)\n",
      "Processing chunk 531/569 (2650000 to 2655000)...\n",
      "Processing chunk 532/569 (2655000 to 2660000)...\n",
      "Processing chunk 533/569 (2660000 to 2665000)...\n",
      "Processing chunk 534/569 (2665000 to 2670000)...\n",
      "Processing chunk 535/569 (2670000 to 2675000)...\n",
      "Completed 535/569 chunks (94.0%)\n",
      "Processing chunk 536/569 (2675000 to 2680000)...\n",
      "Processing chunk 537/569 (2680000 to 2685000)...\n",
      "Processing chunk 538/569 (2685000 to 2690000)...\n",
      "Processing chunk 539/569 (2690000 to 2695000)...\n",
      "Processing chunk 540/569 (2695000 to 2700000)...\n",
      "Completed 540/569 chunks (94.9%)\n",
      "Processing chunk 541/569 (2700000 to 2705000)...\n",
      "Processing chunk 542/569 (2705000 to 2710000)...\n",
      "Processing chunk 543/569 (2710000 to 2715000)...\n",
      "Processing chunk 544/569 (2715000 to 2720000)...\n",
      "Processing chunk 545/569 (2720000 to 2725000)...\n",
      "Completed 545/569 chunks (95.8%)\n",
      "Processing chunk 546/569 (2725000 to 2730000)...\n",
      "Processing chunk 547/569 (2730000 to 2735000)...\n",
      "Processing chunk 548/569 (2735000 to 2740000)...\n",
      "Processing chunk 549/569 (2740000 to 2745000)...\n",
      "Processing chunk 550/569 (2745000 to 2750000)...\n",
      "Completed 550/569 chunks (96.7%)\n",
      "Processing chunk 551/569 (2750000 to 2755000)...\n",
      "Processing chunk 552/569 (2755000 to 2760000)...\n",
      "Processing chunk 553/569 (2760000 to 2765000)...\n",
      "Processing chunk 554/569 (2765000 to 2770000)...\n",
      "Processing chunk 555/569 (2770000 to 2775000)...\n",
      "Completed 555/569 chunks (97.5%)\n",
      "Processing chunk 556/569 (2775000 to 2780000)...\n",
      "Processing chunk 557/569 (2780000 to 2785000)...\n",
      "Processing chunk 558/569 (2785000 to 2790000)...\n",
      "Processing chunk 559/569 (2790000 to 2795000)...\n",
      "Processing chunk 560/569 (2795000 to 2800000)...\n",
      "Completed 560/569 chunks (98.4%)\n",
      "Processing chunk 561/569 (2800000 to 2805000)...\n",
      "Processing chunk 562/569 (2805000 to 2810000)...\n",
      "Processing chunk 563/569 (2810000 to 2815000)...\n",
      "Processing chunk 564/569 (2815000 to 2820000)...\n",
      "Processing chunk 565/569 (2820000 to 2825000)...\n",
      "Completed 565/569 chunks (99.3%)\n",
      "Processing chunk 566/569 (2825000 to 2830000)...\n",
      "Processing chunk 567/569 (2830000 to 2835000)...\n",
      "Processing chunk 568/569 (2835000 to 2840000)...\n",
      "Processing chunk 569/569 (2840000 to 2843069)...\n",
      "Completed 569/569 chunks (100.0%)\n",
      "Adding flight columns to remaining flight data...\n",
      "Saving data in separate files...\n",
      "Processing filtered file: private/data/processed/flight_sequence_filtered.parquet\n",
      "  Reading rows 0 to 100000 of 2843069...\n",
      "Chunking failed, reading entire file: read_table() got an unexpected keyword argument 'rows'\n",
      "  Writing first chunk to output...\n",
      "Processing remaining file: private/data/processed/flight_sequence_remaining.parquet\n",
      "  Reading rows 0 to 100000 of 4066125...\n",
      "Chunking failed, reading entire file: read_table() got an unexpected keyword argument 'rows'\n",
      "  Appending chunk to output...\n",
      "Error in pandas approach: __cinit__() got an unexpected keyword argument 'append'\n",
      "Falling back to simplest approach...\n",
      "Temporary files removed\n",
      "Validation failed: read_table() got an unexpected keyword argument 'rows'\n",
      "Data saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import re\n",
    "\n",
    "# Define directories\n",
    "processed_data_dir = os.path.join('private', 'data', 'processed')\n",
    "data_dir = os.path.join('private', 'data', 'transformed')\n",
    "\n",
    "print(\"Loading weekly sequence data...\")\n",
    "# Load the weekly sequence data efficiently\n",
    "input_path = os.path.join(processed_data_dir, 'weekly_sequence_2_rma.parquet')\n",
    "\n",
    "essential_columns = ['PartNumber', 'WeekStart', 'Tail', 'ReceivedCount', 'ShippedCount', 'InRepair']\n",
    "rma_sequence_df = pd.read_parquet(input_path, columns=essential_columns)\n",
    "\n",
    "print(\"Weekly sequence loaded\")\n",
    "\n",
    "# Standardize tail numbers function - do this early to avoid duplicate computation\n",
    "def standardize_tail(tail):\n",
    "    if not isinstance(tail, str):\n",
    "        return tail\n",
    "    # Remove all non-alphanumeric chars\n",
    "    tail = re.sub(r'[^A-Za-z0-9]', '', tail)\n",
    "    return tail.upper()\n",
    "\n",
    "# Apply standardization\n",
    "rma_sequence_df['StandardTail'] = rma_sequence_df['Tail'].apply(\n",
    "    lambda x: standardize_tail(x) if pd.notna(x) else x\n",
    ")\n",
    "\n",
    "# Load the flight data with memory optimizations\n",
    "print(\"Loading flight data...\")\n",
    "input_path = os.path.join(data_dir, 'merged_flight_data.parquet')\n",
    "# Only load columns we need\n",
    "flight_columns = ['TailNumber', 'FlightStartTime', 'FlightEndTime', 'FlightDuration',\n",
    "                  'RawResets', 'TotalPassengers', 'BusinessClass', 'EconomyClass',\n",
    "                  'SeatResets', 'Airline', 'AircraftType']\n",
    "merged_flight_data_df = pd.read_parquet(input_path, columns=flight_columns)\n",
    "\n",
    "# Optimize memory usage\n",
    "for col in merged_flight_data_df.select_dtypes(include=['float64']).columns:\n",
    "    merged_flight_data_df[col] = pd.to_numeric(merged_flight_data_df[col], downcast='float')\n",
    "for col in merged_flight_data_df.select_dtypes(include=['int64']).columns:\n",
    "    merged_flight_data_df[col] = pd.to_numeric(merged_flight_data_df[col], downcast='integer')\n",
    "\n",
    "# Use categories for string columns with low cardinality\n",
    "string_cols = ['TailNumber', 'Airline', 'AircraftType']\n",
    "for col in string_cols:\n",
    "    if col in merged_flight_data_df.columns:\n",
    "        merged_flight_data_df[col] = merged_flight_data_df[col].astype('category')\n",
    "\n",
    "print(f\"Memory usage of merged_flight_data_df: {merged_flight_data_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(merged_flight_data_df.head())\n",
    "\n",
    "# Clean and filter the flight data\n",
    "print(\"Cleaning flight data...\")\n",
    "# Filter out future dates\n",
    "current_date = datetime.now()\n",
    "merged_flight_data_df = merged_flight_data_df[merged_flight_data_df['FlightStartTime'] <= current_date]\n",
    "\n",
    "# Remove flights with unrealistic durations\n",
    "merged_flight_data_df = merged_flight_data_df[\n",
    "    (merged_flight_data_df['FlightDuration'] >= timedelta(minutes=30)) &\n",
    "    (merged_flight_data_df['FlightDuration'] <= timedelta(hours=24))\n",
    "]\n",
    "\n",
    "print(f\"Date range: {merged_flight_data_df['FlightStartTime'].min()} to {merged_flight_data_df['FlightEndTime'].max()}\")\n",
    "print(f\"After filtering: {len(merged_flight_data_df)} rows\")\n",
    "\n",
    "# Standardize tail numbers in flight data\n",
    "merged_flight_data_df['StandardTail'] = merged_flight_data_df['TailNumber'].apply(\n",
    "    lambda x: standardize_tail(x) if pd.notna(x) else x\n",
    ")\n",
    "\n",
    "# Create WeekStart column\n",
    "merged_flight_data_df['WeekStart'] = pd.to_datetime(\n",
    "    merged_flight_data_df['FlightStartTime']\n",
    ").dt.to_period('W-MON').dt.start_time\n",
    "\n",
    "# Find overlapping tails\n",
    "flight_unique_tails = set(merged_flight_data_df['StandardTail'].dropna().unique())\n",
    "sequence_unique_tails = set(rma_sequence_df['StandardTail'].dropna().unique())\n",
    "overlap_tails = flight_unique_tails.intersection(sequence_unique_tails)\n",
    "\n",
    "print(f\"Unique tails in flight data: {len(flight_unique_tails)}\")\n",
    "print(f\"Unique tails in sequence data: {len(sequence_unique_tails)}\")\n",
    "print(f\"Overlapping unique tails: {len(overlap_tails)}\")\n",
    "if len(overlap_tails) > 0:\n",
    "    print(f\"Sample overlapping tails: {list(overlap_tails)[:5]}\")\n",
    "\n",
    "# Filter flight data to only overlapping tails before aggregation\n",
    "filtered_flight_data = merged_flight_data_df[merged_flight_data_df['StandardTail'].isin(overlap_tails)]\n",
    "print(f\"Filtered flight data from {len(merged_flight_data_df)} to {len(filtered_flight_data)} rows\")\n",
    "\n",
    "del merged_flight_data_df\n",
    "gc.collect()\n",
    "\n",
    "# Process in chunks\n",
    "tail_list = list(overlap_tails)\n",
    "tail_chunk_size = 50  # Process 50 tails at a time\n",
    "num_tail_chunks = (len(tail_list) + tail_chunk_size - 1) // tail_chunk_size\n",
    "all_flight_weekly_data = []\n",
    "\n",
    "print(f\"Processing {len(tail_list)} tails in {num_tail_chunks} chunks...\")\n",
    "\n",
    "for chunk_idx in range(num_tail_chunks):\n",
    "    start_idx = chunk_idx * tail_chunk_size\n",
    "    end_idx = min((chunk_idx + 1) * tail_chunk_size, len(tail_list))\n",
    "    chunk_tails = tail_list[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Processing tail chunk {chunk_idx+1}/{num_tail_chunks} ({start_idx} to {end_idx})...\")\n",
    "    \n",
    "    # Filter data for this chunk of tails\n",
    "    chunk_data = filtered_flight_data[filtered_flight_data['StandardTail'].isin(chunk_tails)]\n",
    "    \n",
    "    # Aggregate flight data by tail and week\n",
    "    chunk_weekly = chunk_data.groupby(['StandardTail', 'WeekStart']).agg({\n",
    "        'RawResets': lambda x: x.sum() if not x.isna().all() else None,\n",
    "        'TotalPassengers': lambda x: x.fillna(0).sum(),\n",
    "        'BusinessClass': lambda x: x.fillna(0).sum(),\n",
    "        'EconomyClass': lambda x: x.fillna(0).sum(),\n",
    "        'FlightDuration': lambda x: x.dropna().dt.total_seconds().sum() / 3600,\n",
    "        'SeatResets': lambda x: x.fillna(0).sum(),\n",
    "        'Airline': lambda x: x.dropna().iloc[0] if not x.dropna().empty else None,\n",
    "        'AircraftType': lambda x: x.dropna().iloc[0] if not x.dropna().empty else None\n",
    "    }).reset_index()\n",
    "    \n",
    "    all_flight_weekly_data.append(chunk_weekly)\n",
    "    \n",
    "    # Clean up\n",
    "    del chunk_data\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Completed tail chunk {chunk_idx+1}/{num_tail_chunks}\")\n",
    "\n",
    "# Combine all chunks\n",
    "print(\"Combining all chunks...\")\n",
    "if all_flight_weekly_data:\n",
    "    flight_weekly_data = pd.concat(all_flight_weekly_data, ignore_index=True)\n",
    "    del all_flight_weekly_data\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Weekly flight data created:\")\n",
    "    print(flight_weekly_data.head())\n",
    "    \n",
    "    spot_check = flight_weekly_data[\n",
    "        (flight_weekly_data['WeekStart'] >= '2023-01-01') & \n",
    "        (flight_weekly_data['WeekStart'] <= '2023-01-07')\n",
    "    ]\n",
    "    if not spot_check.empty:\n",
    "        print(spot_check.head())\n",
    "else:\n",
    "    print(\"No flight data to process\")\n",
    "    flight_weekly_data = pd.DataFrame(columns=['StandardTail', 'WeekStart', 'RawResets', \n",
    "                                              'TotalPassengers', 'BusinessClass', 'EconomyClass',\n",
    "                                              'FlightDuration', 'SeatResets', 'Airline', 'AircraftType'])\n",
    "\n",
    "flight_lookup = {}\n",
    "for _, row in flight_weekly_data.iterrows():\n",
    "    tail = row['StandardTail']\n",
    "    week = row['WeekStart']\n",
    "    \n",
    "    if tail not in flight_lookup:\n",
    "        flight_lookup[tail] = {}\n",
    "    \n",
    "    flight_lookup[tail][week] = {\n",
    "        'RawResets': row['RawResets'],\n",
    "        'TotalPassengers': row['TotalPassengers'],\n",
    "        'BusinessClass': row['BusinessClass'],\n",
    "        'EconomyClass': row['EconomyClass'],\n",
    "        'FlightDuration': row['FlightDuration'],\n",
    "        'SeatResets': row['SeatResets'],\n",
    "        'Airline': row['Airline'],\n",
    "        'AircraftType': row['AircraftType']\n",
    "    }\n",
    "\n",
    "del flight_weekly_data\n",
    "gc.collect()\n",
    "\n",
    "# Optimized function to find closest flight week\n",
    "def find_closest_flight_week(row, max_weeks_diff=52):\n",
    "    tail = row['StandardTail']\n",
    "    week = row['WeekStart']\n",
    "    \n",
    "    # Skip if tail is not in flight data or is null\n",
    "    if pd.isna(tail) or tail not in flight_lookup:\n",
    "        return [None] * 10  # Return list of None values for all expected columns\n",
    "    \n",
    "    # Get all weeks for this tail\n",
    "    weeks_data = flight_lookup[tail]\n",
    "    if not weeks_data:\n",
    "        return [None] * 10\n",
    "    \n",
    "    # Calculate time difference\n",
    "    week_diffs = []\n",
    "    for w in weeks_data:\n",
    "        days_diff = abs((w - week).days)\n",
    "        weeks_diff = days_diff / 7\n",
    "        if weeks_diff <= max_weeks_diff: \n",
    "            week_diffs.append((weeks_diff, w))\n",
    "    \n",
    "    if not week_diffs:\n",
    "        return [None] * 10\n",
    "    \n",
    "    # Find closest week\n",
    "    closest_diff, closest_week = min(week_diffs, key=lambda x: x[0])\n",
    "    \n",
    "    # Get the flight data for the closest week\n",
    "    flight_data = weeks_data[closest_week]\n",
    "    \n",
    "    return [\n",
    "        flight_data['RawResets'],\n",
    "        flight_data['TotalPassengers'],\n",
    "        flight_data['BusinessClass'],\n",
    "        flight_data['EconomyClass'],\n",
    "        flight_data['FlightDuration'],\n",
    "        flight_data['SeatResets'],\n",
    "        flight_data['Airline'],\n",
    "        flight_data['AircraftType'],\n",
    "        closest_diff,\n",
    "        closest_week\n",
    "    ]\n",
    "\n",
    "# Filter RMA sequence data to overlapping tails\n",
    "rma_filtered = rma_sequence_df[rma_sequence_df['StandardTail'].isin(overlap_tails)].copy()\n",
    "print(f\"Filtered from {len(rma_sequence_df)} to {len(rma_filtered)} rows with overlapping tails\")\n",
    "\n",
    "# Process in smaller chunks\n",
    "chunk_size = 5000  # Smaller chunks to reduce memory pressure\n",
    "num_chunks = (len(rma_filtered) + chunk_size - 1) // chunk_size\n",
    "\n",
    "# Preallocate arrays for more efficient result storage\n",
    "result_columns = ['RawResets', 'TotalPassengers', 'BusinessClass', 'EconomyClass', \n",
    "                 'FlightDuration', 'SeatResets', 'Airline', 'AircraftType', \n",
    "                 'week_diff', 'flight_WeekStart']\n",
    "results = {col: np.empty(len(rma_filtered), dtype=object) for col in result_columns}\n",
    "\n",
    "print(f\"Processing {len(rma_filtered)} rows in {num_chunks} chunks...\")\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, len(rma_filtered))\n",
    "    chunk = rma_filtered.iloc[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Processing chunk {i+1}/{num_chunks} ({start_idx} to {end_idx})...\")\n",
    "    \n",
    "    # Process each row in the chunk\n",
    "    for j, (_, row) in enumerate(chunk.iterrows()):\n",
    "        result_values = find_closest_flight_week(row)\n",
    "        \n",
    "        for col_idx, col in enumerate(result_columns):\n",
    "            results[col][start_idx + j] = result_values[col_idx]\n",
    "    \n",
    "    # Print progress\n",
    "    if (i+1) % 5 == 0 or i+1 == num_chunks:\n",
    "        print(f\"Completed {i+1}/{num_chunks} chunks ({(i+1)/num_chunks*100:.1f}%)\")\n",
    "        gc.collect()\n",
    "\n",
    "# Add results to rma_filtered\n",
    "for col in result_columns:\n",
    "    rma_filtered[col] = results[col]\n",
    "\n",
    "del results\n",
    "gc.collect()\n",
    "\n",
    "# Prepare remaining data with correct data types\n",
    "print(\"Adding flight columns to remaining flight data...\")\n",
    "rma_remaining = rma_sequence_df[~rma_sequence_df['StandardTail'].isin(overlap_tails)].copy()\n",
    "\n",
    "for col in result_columns:\n",
    "    if col == 'flight_WeekStart':\n",
    "        rma_remaining[col] = pd.Series(dtype='datetime64[ns]')\n",
    "    elif col == 'Airline' or col == 'AircraftType':\n",
    "        rma_remaining[col] = pd.Series(dtype='object')\n",
    "    else:\n",
    "        rma_remaining[col] = pd.Series(dtype='float64')\n",
    "\n",
    "print(\"Saving data in separate files...\")\n",
    "filtered_path = os.path.join(processed_data_dir, 'flight_sequence_filtered.parquet')\n",
    "remaining_path = os.path.join(processed_data_dir, 'flight_sequence_remaining.parquet')\n",
    "\n",
    "rma_filtered.to_parquet(filtered_path)\n",
    "rma_remaining.to_parquet(remaining_path)\n",
    "\n",
    "del rma_filtered\n",
    "del rma_remaining\n",
    "del rma_sequence_df\n",
    "gc.collect()\n",
    "\n",
    "# Final output path\n",
    "output_path = os.path.join(processed_data_dir, 'weekly_sequence_3_flight.parquet')\n",
    "\n",
    "# Define a function to divide files into smaller chunks\n",
    "def process_large_file_in_chunks(file_path, chunk_size=100000):\n",
    "    try:\n",
    "        file_info = pd.read_parquet(file_path, columns=[])\n",
    "        total_rows = len(file_info)\n",
    "        del file_info\n",
    "        gc.collect()\n",
    "        \n",
    "        for i in range(0, total_rows, chunk_size):\n",
    "            end_row = min(i + chunk_size, total_rows)\n",
    "            print(f\"  Reading rows {i} to {end_row} of {total_rows}...\")\n",
    "            chunk = pd.read_parquet(file_path, rows=slice(i, end_row))\n",
    "            yield chunk\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Chunking failed, reading entire file: {str(e)}\")\n",
    "        yield pd.read_parquet(file_path)\n",
    "\n",
    "try:\n",
    "    print(f\"Processing filtered file: {filtered_path}\")\n",
    "    first_chunk = True\n",
    "    for chunk in process_large_file_in_chunks(filtered_path):\n",
    "        if first_chunk:\n",
    "            print(f\"  Writing first chunk to output...\")\n",
    "            chunk.to_parquet(output_path)\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            print(f\"  Appending chunk to output...\")\n",
    "            chunk.to_parquet(output_path, append=True)\n",
    "        \n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Append the remaining data\n",
    "    print(f\"Processing remaining file: {remaining_path}\")\n",
    "    for chunk in process_large_file_in_chunks(remaining_path):\n",
    "        print(f\"  Appending chunk to output...\")\n",
    "        # Ensure column data types match\n",
    "        for col in chunk.columns:\n",
    "            if col in result_columns:\n",
    "                try:\n",
    "                    sample = pd.read_parquet(output_path, rows=1)\n",
    "                    if col in sample.columns:\n",
    "                        chunk[col] = chunk[col].astype(sample[col].dtype)\n",
    "                    del sample\n",
    "                except:\n",
    "                    pass \n",
    "                \n",
    "        chunk.to_parquet(output_path, append=True)\n",
    "        \n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"All files combined successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in pandas approach: {str(e)}\")\n",
    "    print(\"Falling back to simplest approach...\")\n",
    "    \n",
    "    try:\n",
    "        filtered_data = pd.read_parquet(filtered_path)\n",
    "        remaining_data = pd.read_parquet(remaining_path)\n",
    "        combined_data = pd.concat([filtered_data, remaining_data], ignore_index=True)\n",
    "        combined_data.to_parquet(output_path)\n",
    "        \n",
    "        del filtered_data\n",
    "        del remaining_data\n",
    "        del combined_data\n",
    "        gc.collect()\n",
    "                \n",
    "    except Exception as e:        \n",
    "        print(f\"Files saved separately at {filtered_path} and {remaining_path}\")        \n",
    "\n",
    "try:\n",
    "    os.remove(filtered_path)\n",
    "    os.remove(remaining_path)\n",
    "    print(\"Temporary files removed\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not remove temporary files: {str(e)}\")\n",
    "\n",
    "# Validate final output\n",
    "try:\n",
    "    sample = pd.read_parquet(output_path, rows=5)\n",
    "    print(\"\\nSample of combined data:\")\n",
    "    print(sample[['StandardTail', 'WeekStart', 'ReceivedCount', 'ShippedCount', 'InRepair']].head())\n",
    "    \n",
    "    # Count rows with flight data\n",
    "    stats = pd.read_parquet(output_path, columns=['RawResets'])\n",
    "    matched_rows = stats['RawResets'].notna().sum()\n",
    "    total_rows = len(stats)\n",
    "    print(f\"\\nRows with flight data: {matched_rows} out of {total_rows} ({matched_rows/total_rows*100:.2f}%)\")\n",
    "    \n",
    "    del sample\n",
    "    del stats\n",
    "    gc.collect()\n",
    "except Exception as e:\n",
    "    print(f\"Validation failed: {str(e)}\")\n",
    "\n",
    "print(\"Data saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Weekly sequence loaded\n",
      "MTBF data loaded\n",
      "   MTBFID Airline  PartNumber PartGroup DetailPartGroup  \\\n",
      "0       1     TSC  00-5103-30       RDU            RDU1   \n",
      "1       2     TSC  00-5105-01       RDU            RDU1   \n",
      "2       3     TSC  00-5105-30       RDU            RDU1   \n",
      "3       4     TSC  00-5108-01       RDU            RDU1   \n",
      "4       5     TSC  00-6204-10       REU             REU   \n",
      "\n",
      "                               Description      Month  PoweredOnHours  \\\n",
      "0                           RDU, 12.1 Inch 2020-05-01             0.0   \n",
      "1                            RDU, 8.9 Inch 2020-05-01             0.0   \n",
      "2                            RDU, 8.9 Inch 2020-05-01             0.0   \n",
      "3                     RDU, 15.3 Inch, PCAP 2020-05-01             0.0   \n",
      "4  Remote Jack Unit Extender Unit 2 (REU2) 2020-05-01             0.0   \n",
      "\n",
      "   FlightHours  Failures  NFF  ContractualMTBF  UpdateCount  \n",
      "0          0.0         0    0            12000            0  \n",
      "1          0.0         1    0            15000            0  \n",
      "2          0.0        11    2            15000            0  \n",
      "3          0.0         0    0            15000            0  \n",
      "4          0.0         0    0            15000            0  \n",
      "Number of duplicate partnumbers: 0\n",
      "Date range: 2013-02-01 00:00:00 to 2023-06-01 00:00:00\n",
      "\n",
      "Unique values in essential columns\n",
      "PartNumber: 173 unique values (0.26%) out of 67503 non-null values\n",
      "Month: 125 unique values (0.19%) out of 67503 non-null values\n",
      "Airline: 67 unique values (0.10%) out of 67503 non-null values\n",
      "ContractualMTBF: 14 unique values (0.02%) out of 67503 non-null values\n",
      "Failures: 144 unique values (0.21%) out of 67503 non-null values\n",
      "FlightHours: 23961 unique values (35.50%) out of 67503 non-null values\n",
      "Unique parts in flight data: 326\n",
      "Unique parts in MTBF data: 173\n",
      "Overlapping unique parts: 83\n",
      "Filtered MTBF data from 67503 to 29798 rows\n",
      "Filtered flight data from 6909194 to 3426061 rows\n",
      "Aggregating MTBF data by PartNumber and WeekStart...\n",
      "Weekly MTBF data created\n",
      "   PartNumber  WeekStart  ContractualMTBF  Failures Airline\n",
      "0  00-5001-00 2013-01-29            35000         6     ALK\n",
      "1  00-5001-00 2013-02-26            35000         4     RAM\n",
      "2  00-5001-00 2013-03-26            35000         5     AUA\n",
      "3  00-5001-00 2013-04-30            35000         6     RAM\n",
      "4  00-5001-00 2013-05-28            35000         3     AUA\n",
      "Processing 3426061 rows in 686 chunks...\n",
      "Processing chunk 1/686 (0 to 5000)...\n",
      "Processing chunk 2/686 (5000 to 10000)...\n",
      "Processing chunk 3/686 (10000 to 15000)...\n",
      "Processing chunk 4/686 (15000 to 20000)...\n",
      "Processing chunk 5/686 (20000 to 25000)...\n",
      "Completed 5/686 chunks (0.7%)\n",
      "Processing chunk 6/686 (25000 to 30000)...\n",
      "Processing chunk 7/686 (30000 to 35000)...\n",
      "Processing chunk 8/686 (35000 to 40000)...\n",
      "Processing chunk 9/686 (40000 to 45000)...\n",
      "Processing chunk 10/686 (45000 to 50000)...\n",
      "Completed 10/686 chunks (1.5%)\n",
      "Processing chunk 11/686 (50000 to 55000)...\n",
      "Processing chunk 12/686 (55000 to 60000)...\n",
      "Processing chunk 13/686 (60000 to 65000)...\n",
      "Processing chunk 14/686 (65000 to 70000)...\n",
      "Processing chunk 15/686 (70000 to 75000)...\n",
      "Completed 15/686 chunks (2.2%)\n",
      "Processing chunk 16/686 (75000 to 80000)...\n",
      "Processing chunk 17/686 (80000 to 85000)...\n",
      "Processing chunk 18/686 (85000 to 90000)...\n",
      "Processing chunk 19/686 (90000 to 95000)...\n",
      "Processing chunk 20/686 (95000 to 100000)...\n",
      "Completed 20/686 chunks (2.9%)\n",
      "Processing chunk 21/686 (100000 to 105000)...\n",
      "Processing chunk 22/686 (105000 to 110000)...\n",
      "Processing chunk 23/686 (110000 to 115000)...\n",
      "Processing chunk 24/686 (115000 to 120000)...\n",
      "Processing chunk 25/686 (120000 to 125000)...\n",
      "Completed 25/686 chunks (3.6%)\n",
      "Processing chunk 26/686 (125000 to 130000)...\n",
      "Processing chunk 27/686 (130000 to 135000)...\n",
      "Processing chunk 28/686 (135000 to 140000)...\n",
      "Processing chunk 29/686 (140000 to 145000)...\n",
      "Processing chunk 30/686 (145000 to 150000)...\n",
      "Completed 30/686 chunks (4.4%)\n",
      "Processing chunk 31/686 (150000 to 155000)...\n",
      "Processing chunk 32/686 (155000 to 160000)...\n",
      "Processing chunk 33/686 (160000 to 165000)...\n",
      "Processing chunk 34/686 (165000 to 170000)...\n",
      "Processing chunk 35/686 (170000 to 175000)...\n",
      "Completed 35/686 chunks (5.1%)\n",
      "Processing chunk 36/686 (175000 to 180000)...\n",
      "Processing chunk 37/686 (180000 to 185000)...\n",
      "Processing chunk 38/686 (185000 to 190000)...\n",
      "Processing chunk 39/686 (190000 to 195000)...\n",
      "Processing chunk 40/686 (195000 to 200000)...\n",
      "Completed 40/686 chunks (5.8%)\n",
      "Processing chunk 41/686 (200000 to 205000)...\n",
      "Processing chunk 42/686 (205000 to 210000)...\n",
      "Processing chunk 43/686 (210000 to 215000)...\n",
      "Processing chunk 44/686 (215000 to 220000)...\n",
      "Processing chunk 45/686 (220000 to 225000)...\n",
      "Completed 45/686 chunks (6.6%)\n",
      "Processing chunk 46/686 (225000 to 230000)...\n",
      "Processing chunk 47/686 (230000 to 235000)...\n",
      "Processing chunk 48/686 (235000 to 240000)...\n",
      "Processing chunk 49/686 (240000 to 245000)...\n",
      "Processing chunk 50/686 (245000 to 250000)...\n",
      "Completed 50/686 chunks (7.3%)\n",
      "Processing chunk 51/686 (250000 to 255000)...\n",
      "Processing chunk 52/686 (255000 to 260000)...\n",
      "Processing chunk 53/686 (260000 to 265000)...\n",
      "Processing chunk 54/686 (265000 to 270000)...\n",
      "Processing chunk 55/686 (270000 to 275000)...\n",
      "Completed 55/686 chunks (8.0%)\n",
      "Processing chunk 56/686 (275000 to 280000)...\n",
      "Processing chunk 57/686 (280000 to 285000)...\n",
      "Processing chunk 58/686 (285000 to 290000)...\n",
      "Processing chunk 59/686 (290000 to 295000)...\n",
      "Processing chunk 60/686 (295000 to 300000)...\n",
      "Completed 60/686 chunks (8.7%)\n",
      "Processing chunk 61/686 (300000 to 305000)...\n",
      "Processing chunk 62/686 (305000 to 310000)...\n",
      "Processing chunk 63/686 (310000 to 315000)...\n",
      "Processing chunk 64/686 (315000 to 320000)...\n",
      "Processing chunk 65/686 (320000 to 325000)...\n",
      "Completed 65/686 chunks (9.5%)\n",
      "Processing chunk 66/686 (325000 to 330000)...\n",
      "Processing chunk 67/686 (330000 to 335000)...\n",
      "Processing chunk 68/686 (335000 to 340000)...\n",
      "Processing chunk 69/686 (340000 to 345000)...\n",
      "Processing chunk 70/686 (345000 to 350000)...\n",
      "Completed 70/686 chunks (10.2%)\n",
      "Processing chunk 71/686 (350000 to 355000)...\n",
      "Processing chunk 72/686 (355000 to 360000)...\n",
      "Processing chunk 73/686 (360000 to 365000)...\n",
      "Processing chunk 74/686 (365000 to 370000)...\n",
      "Processing chunk 75/686 (370000 to 375000)...\n",
      "Completed 75/686 chunks (10.9%)\n",
      "Processing chunk 76/686 (375000 to 380000)...\n",
      "Processing chunk 77/686 (380000 to 385000)...\n",
      "Processing chunk 78/686 (385000 to 390000)...\n",
      "Processing chunk 79/686 (390000 to 395000)...\n",
      "Processing chunk 80/686 (395000 to 400000)...\n",
      "Completed 80/686 chunks (11.7%)\n",
      "Processing chunk 81/686 (400000 to 405000)...\n",
      "Processing chunk 82/686 (405000 to 410000)...\n",
      "Processing chunk 83/686 (410000 to 415000)...\n",
      "Processing chunk 84/686 (415000 to 420000)...\n",
      "Processing chunk 85/686 (420000 to 425000)...\n",
      "Completed 85/686 chunks (12.4%)\n",
      "Processing chunk 86/686 (425000 to 430000)...\n",
      "Processing chunk 87/686 (430000 to 435000)...\n",
      "Processing chunk 88/686 (435000 to 440000)...\n",
      "Processing chunk 89/686 (440000 to 445000)...\n",
      "Processing chunk 90/686 (445000 to 450000)...\n",
      "Completed 90/686 chunks (13.1%)\n",
      "Processing chunk 91/686 (450000 to 455000)...\n",
      "Processing chunk 92/686 (455000 to 460000)...\n",
      "Processing chunk 93/686 (460000 to 465000)...\n",
      "Processing chunk 94/686 (465000 to 470000)...\n",
      "Processing chunk 95/686 (470000 to 475000)...\n",
      "Completed 95/686 chunks (13.8%)\n",
      "Processing chunk 96/686 (475000 to 480000)...\n",
      "Processing chunk 97/686 (480000 to 485000)...\n",
      "Processing chunk 98/686 (485000 to 490000)...\n",
      "Processing chunk 99/686 (490000 to 495000)...\n",
      "Processing chunk 100/686 (495000 to 500000)...\n",
      "Completed 100/686 chunks (14.6%)\n",
      "Processing chunk 101/686 (500000 to 505000)...\n",
      "Processing chunk 102/686 (505000 to 510000)...\n",
      "Processing chunk 103/686 (510000 to 515000)...\n",
      "Processing chunk 104/686 (515000 to 520000)...\n",
      "Processing chunk 105/686 (520000 to 525000)...\n",
      "Completed 105/686 chunks (15.3%)\n",
      "Processing chunk 106/686 (525000 to 530000)...\n",
      "Processing chunk 107/686 (530000 to 535000)...\n",
      "Processing chunk 108/686 (535000 to 540000)...\n",
      "Processing chunk 109/686 (540000 to 545000)...\n",
      "Processing chunk 110/686 (545000 to 550000)...\n",
      "Completed 110/686 chunks (16.0%)\n",
      "Processing chunk 111/686 (550000 to 555000)...\n",
      "Processing chunk 112/686 (555000 to 560000)...\n",
      "Processing chunk 113/686 (560000 to 565000)...\n",
      "Processing chunk 114/686 (565000 to 570000)...\n",
      "Processing chunk 115/686 (570000 to 575000)...\n",
      "Completed 115/686 chunks (16.8%)\n",
      "Processing chunk 116/686 (575000 to 580000)...\n",
      "Processing chunk 117/686 (580000 to 585000)...\n",
      "Processing chunk 118/686 (585000 to 590000)...\n",
      "Processing chunk 119/686 (590000 to 595000)...\n",
      "Processing chunk 120/686 (595000 to 600000)...\n",
      "Completed 120/686 chunks (17.5%)\n",
      "Processing chunk 121/686 (600000 to 605000)...\n",
      "Processing chunk 122/686 (605000 to 610000)...\n",
      "Processing chunk 123/686 (610000 to 615000)...\n",
      "Processing chunk 124/686 (615000 to 620000)...\n",
      "Processing chunk 125/686 (620000 to 625000)...\n",
      "Completed 125/686 chunks (18.2%)\n",
      "Processing chunk 126/686 (625000 to 630000)...\n",
      "Processing chunk 127/686 (630000 to 635000)...\n",
      "Processing chunk 128/686 (635000 to 640000)...\n",
      "Processing chunk 129/686 (640000 to 645000)...\n",
      "Processing chunk 130/686 (645000 to 650000)...\n",
      "Completed 130/686 chunks (19.0%)\n",
      "Processing chunk 131/686 (650000 to 655000)...\n",
      "Processing chunk 132/686 (655000 to 660000)...\n",
      "Processing chunk 133/686 (660000 to 665000)...\n",
      "Processing chunk 134/686 (665000 to 670000)...\n",
      "Processing chunk 135/686 (670000 to 675000)...\n",
      "Completed 135/686 chunks (19.7%)\n",
      "Processing chunk 136/686 (675000 to 680000)...\n",
      "Processing chunk 137/686 (680000 to 685000)...\n",
      "Processing chunk 138/686 (685000 to 690000)...\n",
      "Processing chunk 139/686 (690000 to 695000)...\n",
      "Processing chunk 140/686 (695000 to 700000)...\n",
      "Completed 140/686 chunks (20.4%)\n",
      "Processing chunk 141/686 (700000 to 705000)...\n",
      "Processing chunk 142/686 (705000 to 710000)...\n",
      "Processing chunk 143/686 (710000 to 715000)...\n",
      "Processing chunk 144/686 (715000 to 720000)...\n",
      "Processing chunk 145/686 (720000 to 725000)...\n",
      "Completed 145/686 chunks (21.1%)\n",
      "Processing chunk 146/686 (725000 to 730000)...\n",
      "Processing chunk 147/686 (730000 to 735000)...\n",
      "Processing chunk 148/686 (735000 to 740000)...\n",
      "Processing chunk 149/686 (740000 to 745000)...\n",
      "Processing chunk 150/686 (745000 to 750000)...\n",
      "Completed 150/686 chunks (21.9%)\n",
      "Processing chunk 151/686 (750000 to 755000)...\n",
      "Processing chunk 152/686 (755000 to 760000)...\n",
      "Processing chunk 153/686 (760000 to 765000)...\n",
      "Processing chunk 154/686 (765000 to 770000)...\n",
      "Processing chunk 155/686 (770000 to 775000)...\n",
      "Completed 155/686 chunks (22.6%)\n",
      "Processing chunk 156/686 (775000 to 780000)...\n",
      "Processing chunk 157/686 (780000 to 785000)...\n",
      "Processing chunk 158/686 (785000 to 790000)...\n",
      "Processing chunk 159/686 (790000 to 795000)...\n",
      "Processing chunk 160/686 (795000 to 800000)...\n",
      "Completed 160/686 chunks (23.3%)\n",
      "Processing chunk 161/686 (800000 to 805000)...\n",
      "Processing chunk 162/686 (805000 to 810000)...\n",
      "Processing chunk 163/686 (810000 to 815000)...\n",
      "Processing chunk 164/686 (815000 to 820000)...\n",
      "Processing chunk 165/686 (820000 to 825000)...\n",
      "Completed 165/686 chunks (24.1%)\n",
      "Processing chunk 166/686 (825000 to 830000)...\n",
      "Processing chunk 167/686 (830000 to 835000)...\n",
      "Processing chunk 168/686 (835000 to 840000)...\n",
      "Processing chunk 169/686 (840000 to 845000)...\n",
      "Processing chunk 170/686 (845000 to 850000)...\n",
      "Completed 170/686 chunks (24.8%)\n",
      "Processing chunk 171/686 (850000 to 855000)...\n",
      "Processing chunk 172/686 (855000 to 860000)...\n",
      "Processing chunk 173/686 (860000 to 865000)...\n",
      "Processing chunk 174/686 (865000 to 870000)...\n",
      "Processing chunk 175/686 (870000 to 875000)...\n",
      "Completed 175/686 chunks (25.5%)\n",
      "Processing chunk 176/686 (875000 to 880000)...\n",
      "Processing chunk 177/686 (880000 to 885000)...\n",
      "Processing chunk 178/686 (885000 to 890000)...\n",
      "Processing chunk 179/686 (890000 to 895000)...\n",
      "Processing chunk 180/686 (895000 to 900000)...\n",
      "Completed 180/686 chunks (26.2%)\n",
      "Processing chunk 181/686 (900000 to 905000)...\n",
      "Processing chunk 182/686 (905000 to 910000)...\n",
      "Processing chunk 183/686 (910000 to 915000)...\n",
      "Processing chunk 184/686 (915000 to 920000)...\n",
      "Processing chunk 185/686 (920000 to 925000)...\n",
      "Completed 185/686 chunks (27.0%)\n",
      "Processing chunk 186/686 (925000 to 930000)...\n",
      "Processing chunk 187/686 (930000 to 935000)...\n",
      "Processing chunk 188/686 (935000 to 940000)...\n",
      "Processing chunk 189/686 (940000 to 945000)...\n",
      "Processing chunk 190/686 (945000 to 950000)...\n",
      "Completed 190/686 chunks (27.7%)\n",
      "Processing chunk 191/686 (950000 to 955000)...\n",
      "Processing chunk 192/686 (955000 to 960000)...\n",
      "Processing chunk 193/686 (960000 to 965000)...\n",
      "Processing chunk 194/686 (965000 to 970000)...\n",
      "Processing chunk 195/686 (970000 to 975000)...\n",
      "Completed 195/686 chunks (28.4%)\n",
      "Processing chunk 196/686 (975000 to 980000)...\n",
      "Processing chunk 197/686 (980000 to 985000)...\n",
      "Processing chunk 198/686 (985000 to 990000)...\n",
      "Processing chunk 199/686 (990000 to 995000)...\n",
      "Processing chunk 200/686 (995000 to 1000000)...\n",
      "Completed 200/686 chunks (29.2%)\n",
      "Processing chunk 201/686 (1000000 to 1005000)...\n",
      "Processing chunk 202/686 (1005000 to 1010000)...\n",
      "Processing chunk 203/686 (1010000 to 1015000)...\n",
      "Processing chunk 204/686 (1015000 to 1020000)...\n",
      "Processing chunk 205/686 (1020000 to 1025000)...\n",
      "Completed 205/686 chunks (29.9%)\n",
      "Processing chunk 206/686 (1025000 to 1030000)...\n",
      "Processing chunk 207/686 (1030000 to 1035000)...\n",
      "Processing chunk 208/686 (1035000 to 1040000)...\n",
      "Processing chunk 209/686 (1040000 to 1045000)...\n",
      "Processing chunk 210/686 (1045000 to 1050000)...\n",
      "Completed 210/686 chunks (30.6%)\n",
      "Processing chunk 211/686 (1050000 to 1055000)...\n",
      "Processing chunk 212/686 (1055000 to 1060000)...\n",
      "Processing chunk 213/686 (1060000 to 1065000)...\n",
      "Processing chunk 214/686 (1065000 to 1070000)...\n",
      "Processing chunk 215/686 (1070000 to 1075000)...\n",
      "Completed 215/686 chunks (31.3%)\n",
      "Processing chunk 216/686 (1075000 to 1080000)...\n",
      "Processing chunk 217/686 (1080000 to 1085000)...\n",
      "Processing chunk 218/686 (1085000 to 1090000)...\n",
      "Processing chunk 219/686 (1090000 to 1095000)...\n",
      "Processing chunk 220/686 (1095000 to 1100000)...\n",
      "Completed 220/686 chunks (32.1%)\n",
      "Processing chunk 221/686 (1100000 to 1105000)...\n",
      "Processing chunk 222/686 (1105000 to 1110000)...\n",
      "Processing chunk 223/686 (1110000 to 1115000)...\n",
      "Processing chunk 224/686 (1115000 to 1120000)...\n",
      "Processing chunk 225/686 (1120000 to 1125000)...\n",
      "Completed 225/686 chunks (32.8%)\n",
      "Processing chunk 226/686 (1125000 to 1130000)...\n",
      "Processing chunk 227/686 (1130000 to 1135000)...\n",
      "Processing chunk 228/686 (1135000 to 1140000)...\n",
      "Processing chunk 229/686 (1140000 to 1145000)...\n",
      "Processing chunk 230/686 (1145000 to 1150000)...\n",
      "Completed 230/686 chunks (33.5%)\n",
      "Processing chunk 231/686 (1150000 to 1155000)...\n",
      "Processing chunk 232/686 (1155000 to 1160000)...\n",
      "Processing chunk 233/686 (1160000 to 1165000)...\n",
      "Processing chunk 234/686 (1165000 to 1170000)...\n",
      "Processing chunk 235/686 (1170000 to 1175000)...\n",
      "Completed 235/686 chunks (34.3%)\n",
      "Processing chunk 236/686 (1175000 to 1180000)...\n",
      "Processing chunk 237/686 (1180000 to 1185000)...\n",
      "Processing chunk 238/686 (1185000 to 1190000)...\n",
      "Processing chunk 239/686 (1190000 to 1195000)...\n",
      "Processing chunk 240/686 (1195000 to 1200000)...\n",
      "Completed 240/686 chunks (35.0%)\n",
      "Processing chunk 241/686 (1200000 to 1205000)...\n",
      "Processing chunk 242/686 (1205000 to 1210000)...\n",
      "Processing chunk 243/686 (1210000 to 1215000)...\n",
      "Processing chunk 244/686 (1215000 to 1220000)...\n",
      "Processing chunk 245/686 (1220000 to 1225000)...\n",
      "Completed 245/686 chunks (35.7%)\n",
      "Processing chunk 246/686 (1225000 to 1230000)...\n",
      "Processing chunk 247/686 (1230000 to 1235000)...\n",
      "Processing chunk 248/686 (1235000 to 1240000)...\n",
      "Processing chunk 249/686 (1240000 to 1245000)...\n",
      "Processing chunk 250/686 (1245000 to 1250000)...\n",
      "Completed 250/686 chunks (36.4%)\n",
      "Processing chunk 251/686 (1250000 to 1255000)...\n",
      "Processing chunk 252/686 (1255000 to 1260000)...\n",
      "Processing chunk 253/686 (1260000 to 1265000)...\n",
      "Processing chunk 254/686 (1265000 to 1270000)...\n",
      "Processing chunk 255/686 (1270000 to 1275000)...\n",
      "Completed 255/686 chunks (37.2%)\n",
      "Processing chunk 256/686 (1275000 to 1280000)...\n",
      "Processing chunk 257/686 (1280000 to 1285000)...\n",
      "Processing chunk 258/686 (1285000 to 1290000)...\n",
      "Processing chunk 259/686 (1290000 to 1295000)...\n",
      "Processing chunk 260/686 (1295000 to 1300000)...\n",
      "Completed 260/686 chunks (37.9%)\n",
      "Processing chunk 261/686 (1300000 to 1305000)...\n",
      "Processing chunk 262/686 (1305000 to 1310000)...\n",
      "Processing chunk 263/686 (1310000 to 1315000)...\n",
      "Processing chunk 264/686 (1315000 to 1320000)...\n",
      "Processing chunk 265/686 (1320000 to 1325000)...\n",
      "Completed 265/686 chunks (38.6%)\n",
      "Processing chunk 266/686 (1325000 to 1330000)...\n",
      "Processing chunk 267/686 (1330000 to 1335000)...\n",
      "Processing chunk 268/686 (1335000 to 1340000)...\n",
      "Processing chunk 269/686 (1340000 to 1345000)...\n",
      "Processing chunk 270/686 (1345000 to 1350000)...\n",
      "Completed 270/686 chunks (39.4%)\n",
      "Processing chunk 271/686 (1350000 to 1355000)...\n",
      "Processing chunk 272/686 (1355000 to 1360000)...\n",
      "Processing chunk 273/686 (1360000 to 1365000)...\n",
      "Processing chunk 274/686 (1365000 to 1370000)...\n",
      "Processing chunk 275/686 (1370000 to 1375000)...\n",
      "Completed 275/686 chunks (40.1%)\n",
      "Processing chunk 276/686 (1375000 to 1380000)...\n",
      "Processing chunk 277/686 (1380000 to 1385000)...\n",
      "Processing chunk 278/686 (1385000 to 1390000)...\n",
      "Processing chunk 279/686 (1390000 to 1395000)...\n",
      "Processing chunk 280/686 (1395000 to 1400000)...\n",
      "Completed 280/686 chunks (40.8%)\n",
      "Processing chunk 281/686 (1400000 to 1405000)...\n",
      "Processing chunk 282/686 (1405000 to 1410000)...\n",
      "Processing chunk 283/686 (1410000 to 1415000)...\n",
      "Processing chunk 284/686 (1415000 to 1420000)...\n",
      "Processing chunk 285/686 (1420000 to 1425000)...\n",
      "Completed 285/686 chunks (41.5%)\n",
      "Processing chunk 286/686 (1425000 to 1430000)...\n",
      "Processing chunk 287/686 (1430000 to 1435000)...\n",
      "Processing chunk 288/686 (1435000 to 1440000)...\n",
      "Processing chunk 289/686 (1440000 to 1445000)...\n",
      "Processing chunk 290/686 (1445000 to 1450000)...\n",
      "Completed 290/686 chunks (42.3%)\n",
      "Processing chunk 291/686 (1450000 to 1455000)...\n",
      "Processing chunk 292/686 (1455000 to 1460000)...\n",
      "Processing chunk 293/686 (1460000 to 1465000)...\n",
      "Processing chunk 294/686 (1465000 to 1470000)...\n",
      "Processing chunk 295/686 (1470000 to 1475000)...\n",
      "Completed 295/686 chunks (43.0%)\n",
      "Processing chunk 296/686 (1475000 to 1480000)...\n",
      "Processing chunk 297/686 (1480000 to 1485000)...\n",
      "Processing chunk 298/686 (1485000 to 1490000)...\n",
      "Processing chunk 299/686 (1490000 to 1495000)...\n",
      "Processing chunk 300/686 (1495000 to 1500000)...\n",
      "Completed 300/686 chunks (43.7%)\n",
      "Processing chunk 301/686 (1500000 to 1505000)...\n",
      "Processing chunk 302/686 (1505000 to 1510000)...\n",
      "Processing chunk 303/686 (1510000 to 1515000)...\n",
      "Processing chunk 304/686 (1515000 to 1520000)...\n",
      "Processing chunk 305/686 (1520000 to 1525000)...\n",
      "Completed 305/686 chunks (44.5%)\n",
      "Processing chunk 306/686 (1525000 to 1530000)...\n",
      "Processing chunk 307/686 (1530000 to 1535000)...\n",
      "Processing chunk 308/686 (1535000 to 1540000)...\n",
      "Processing chunk 309/686 (1540000 to 1545000)...\n",
      "Processing chunk 310/686 (1545000 to 1550000)...\n",
      "Completed 310/686 chunks (45.2%)\n",
      "Processing chunk 311/686 (1550000 to 1555000)...\n",
      "Processing chunk 312/686 (1555000 to 1560000)...\n",
      "Processing chunk 313/686 (1560000 to 1565000)...\n",
      "Processing chunk 314/686 (1565000 to 1570000)...\n",
      "Processing chunk 315/686 (1570000 to 1575000)...\n",
      "Completed 315/686 chunks (45.9%)\n",
      "Processing chunk 316/686 (1575000 to 1580000)...\n",
      "Processing chunk 317/686 (1580000 to 1585000)...\n",
      "Processing chunk 318/686 (1585000 to 1590000)...\n",
      "Processing chunk 319/686 (1590000 to 1595000)...\n",
      "Processing chunk 320/686 (1595000 to 1600000)...\n",
      "Completed 320/686 chunks (46.6%)\n",
      "Processing chunk 321/686 (1600000 to 1605000)...\n",
      "Processing chunk 322/686 (1605000 to 1610000)...\n",
      "Processing chunk 323/686 (1610000 to 1615000)...\n",
      "Processing chunk 324/686 (1615000 to 1620000)...\n",
      "Processing chunk 325/686 (1620000 to 1625000)...\n",
      "Completed 325/686 chunks (47.4%)\n",
      "Processing chunk 326/686 (1625000 to 1630000)...\n",
      "Processing chunk 327/686 (1630000 to 1635000)...\n",
      "Processing chunk 328/686 (1635000 to 1640000)...\n",
      "Processing chunk 329/686 (1640000 to 1645000)...\n",
      "Processing chunk 330/686 (1645000 to 1650000)...\n",
      "Completed 330/686 chunks (48.1%)\n",
      "Processing chunk 331/686 (1650000 to 1655000)...\n",
      "Processing chunk 332/686 (1655000 to 1660000)...\n",
      "Processing chunk 333/686 (1660000 to 1665000)...\n",
      "Processing chunk 334/686 (1665000 to 1670000)...\n",
      "Processing chunk 335/686 (1670000 to 1675000)...\n",
      "Completed 335/686 chunks (48.8%)\n",
      "Processing chunk 336/686 (1675000 to 1680000)...\n",
      "Processing chunk 337/686 (1680000 to 1685000)...\n",
      "Processing chunk 338/686 (1685000 to 1690000)...\n",
      "Processing chunk 339/686 (1690000 to 1695000)...\n",
      "Processing chunk 340/686 (1695000 to 1700000)...\n",
      "Completed 340/686 chunks (49.6%)\n",
      "Processing chunk 341/686 (1700000 to 1705000)...\n",
      "Processing chunk 342/686 (1705000 to 1710000)...\n",
      "Processing chunk 343/686 (1710000 to 1715000)...\n",
      "Processing chunk 344/686 (1715000 to 1720000)...\n",
      "Processing chunk 345/686 (1720000 to 1725000)...\n",
      "Completed 345/686 chunks (50.3%)\n",
      "Processing chunk 346/686 (1725000 to 1730000)...\n",
      "Processing chunk 347/686 (1730000 to 1735000)...\n",
      "Processing chunk 348/686 (1735000 to 1740000)...\n",
      "Processing chunk 349/686 (1740000 to 1745000)...\n",
      "Processing chunk 350/686 (1745000 to 1750000)...\n",
      "Completed 350/686 chunks (51.0%)\n",
      "Processing chunk 351/686 (1750000 to 1755000)...\n",
      "Processing chunk 352/686 (1755000 to 1760000)...\n",
      "Processing chunk 353/686 (1760000 to 1765000)...\n",
      "Processing chunk 354/686 (1765000 to 1770000)...\n",
      "Processing chunk 355/686 (1770000 to 1775000)...\n",
      "Completed 355/686 chunks (51.7%)\n",
      "Processing chunk 356/686 (1775000 to 1780000)...\n",
      "Processing chunk 357/686 (1780000 to 1785000)...\n",
      "Processing chunk 358/686 (1785000 to 1790000)...\n",
      "Processing chunk 359/686 (1790000 to 1795000)...\n",
      "Processing chunk 360/686 (1795000 to 1800000)...\n",
      "Completed 360/686 chunks (52.5%)\n",
      "Processing chunk 361/686 (1800000 to 1805000)...\n",
      "Processing chunk 362/686 (1805000 to 1810000)...\n",
      "Processing chunk 363/686 (1810000 to 1815000)...\n",
      "Processing chunk 364/686 (1815000 to 1820000)...\n",
      "Processing chunk 365/686 (1820000 to 1825000)...\n",
      "Completed 365/686 chunks (53.2%)\n",
      "Processing chunk 366/686 (1825000 to 1830000)...\n",
      "Processing chunk 367/686 (1830000 to 1835000)...\n",
      "Processing chunk 368/686 (1835000 to 1840000)...\n",
      "Processing chunk 369/686 (1840000 to 1845000)...\n",
      "Processing chunk 370/686 (1845000 to 1850000)...\n",
      "Completed 370/686 chunks (53.9%)\n",
      "Processing chunk 371/686 (1850000 to 1855000)...\n",
      "Processing chunk 372/686 (1855000 to 1860000)...\n",
      "Processing chunk 373/686 (1860000 to 1865000)...\n",
      "Processing chunk 374/686 (1865000 to 1870000)...\n",
      "Processing chunk 375/686 (1870000 to 1875000)...\n",
      "Completed 375/686 chunks (54.7%)\n",
      "Processing chunk 376/686 (1875000 to 1880000)...\n",
      "Processing chunk 377/686 (1880000 to 1885000)...\n",
      "Processing chunk 378/686 (1885000 to 1890000)...\n",
      "Processing chunk 379/686 (1890000 to 1895000)...\n",
      "Processing chunk 380/686 (1895000 to 1900000)...\n",
      "Completed 380/686 chunks (55.4%)\n",
      "Processing chunk 381/686 (1900000 to 1905000)...\n",
      "Processing chunk 382/686 (1905000 to 1910000)...\n",
      "Processing chunk 383/686 (1910000 to 1915000)...\n",
      "Processing chunk 384/686 (1915000 to 1920000)...\n",
      "Processing chunk 385/686 (1920000 to 1925000)...\n",
      "Completed 385/686 chunks (56.1%)\n",
      "Processing chunk 386/686 (1925000 to 1930000)...\n",
      "Processing chunk 387/686 (1930000 to 1935000)...\n",
      "Processing chunk 388/686 (1935000 to 1940000)...\n",
      "Processing chunk 389/686 (1940000 to 1945000)...\n",
      "Processing chunk 390/686 (1945000 to 1950000)...\n",
      "Completed 390/686 chunks (56.9%)\n",
      "Processing chunk 391/686 (1950000 to 1955000)...\n",
      "Processing chunk 392/686 (1955000 to 1960000)...\n",
      "Processing chunk 393/686 (1960000 to 1965000)...\n",
      "Processing chunk 394/686 (1965000 to 1970000)...\n",
      "Processing chunk 395/686 (1970000 to 1975000)...\n",
      "Completed 395/686 chunks (57.6%)\n",
      "Processing chunk 396/686 (1975000 to 1980000)...\n",
      "Processing chunk 397/686 (1980000 to 1985000)...\n",
      "Processing chunk 398/686 (1985000 to 1990000)...\n",
      "Processing chunk 399/686 (1990000 to 1995000)...\n",
      "Processing chunk 400/686 (1995000 to 2000000)...\n",
      "Completed 400/686 chunks (58.3%)\n",
      "Processing chunk 401/686 (2000000 to 2005000)...\n",
      "Processing chunk 402/686 (2005000 to 2010000)...\n",
      "Processing chunk 403/686 (2010000 to 2015000)...\n",
      "Processing chunk 404/686 (2015000 to 2020000)...\n",
      "Processing chunk 405/686 (2020000 to 2025000)...\n",
      "Completed 405/686 chunks (59.0%)\n",
      "Processing chunk 406/686 (2025000 to 2030000)...\n",
      "Processing chunk 407/686 (2030000 to 2035000)...\n",
      "Processing chunk 408/686 (2035000 to 2040000)...\n",
      "Processing chunk 409/686 (2040000 to 2045000)...\n",
      "Processing chunk 410/686 (2045000 to 2050000)...\n",
      "Completed 410/686 chunks (59.8%)\n",
      "Processing chunk 411/686 (2050000 to 2055000)...\n",
      "Processing chunk 412/686 (2055000 to 2060000)...\n",
      "Processing chunk 413/686 (2060000 to 2065000)...\n",
      "Processing chunk 414/686 (2065000 to 2070000)...\n",
      "Processing chunk 415/686 (2070000 to 2075000)...\n",
      "Completed 415/686 chunks (60.5%)\n",
      "Processing chunk 416/686 (2075000 to 2080000)...\n",
      "Processing chunk 417/686 (2080000 to 2085000)...\n",
      "Processing chunk 418/686 (2085000 to 2090000)...\n",
      "Processing chunk 419/686 (2090000 to 2095000)...\n",
      "Processing chunk 420/686 (2095000 to 2100000)...\n",
      "Completed 420/686 chunks (61.2%)\n",
      "Processing chunk 421/686 (2100000 to 2105000)...\n",
      "Processing chunk 422/686 (2105000 to 2110000)...\n",
      "Processing chunk 423/686 (2110000 to 2115000)...\n",
      "Processing chunk 424/686 (2115000 to 2120000)...\n",
      "Processing chunk 425/686 (2120000 to 2125000)...\n",
      "Completed 425/686 chunks (62.0%)\n",
      "Processing chunk 426/686 (2125000 to 2130000)...\n",
      "Processing chunk 427/686 (2130000 to 2135000)...\n",
      "Processing chunk 428/686 (2135000 to 2140000)...\n",
      "Processing chunk 429/686 (2140000 to 2145000)...\n",
      "Processing chunk 430/686 (2145000 to 2150000)...\n",
      "Completed 430/686 chunks (62.7%)\n",
      "Processing chunk 431/686 (2150000 to 2155000)...\n",
      "Processing chunk 432/686 (2155000 to 2160000)...\n",
      "Processing chunk 433/686 (2160000 to 2165000)...\n",
      "Processing chunk 434/686 (2165000 to 2170000)...\n",
      "Processing chunk 435/686 (2170000 to 2175000)...\n",
      "Completed 435/686 chunks (63.4%)\n",
      "Processing chunk 436/686 (2175000 to 2180000)...\n",
      "Processing chunk 437/686 (2180000 to 2185000)...\n",
      "Processing chunk 438/686 (2185000 to 2190000)...\n",
      "Processing chunk 439/686 (2190000 to 2195000)...\n",
      "Processing chunk 440/686 (2195000 to 2200000)...\n",
      "Completed 440/686 chunks (64.1%)\n",
      "Processing chunk 441/686 (2200000 to 2205000)...\n",
      "Processing chunk 442/686 (2205000 to 2210000)...\n",
      "Processing chunk 443/686 (2210000 to 2215000)...\n",
      "Processing chunk 444/686 (2215000 to 2220000)...\n",
      "Processing chunk 445/686 (2220000 to 2225000)...\n",
      "Completed 445/686 chunks (64.9%)\n",
      "Processing chunk 446/686 (2225000 to 2230000)...\n",
      "Processing chunk 447/686 (2230000 to 2235000)...\n",
      "Processing chunk 448/686 (2235000 to 2240000)...\n",
      "Processing chunk 449/686 (2240000 to 2245000)...\n",
      "Processing chunk 450/686 (2245000 to 2250000)...\n",
      "Completed 450/686 chunks (65.6%)\n",
      "Processing chunk 451/686 (2250000 to 2255000)...\n",
      "Processing chunk 452/686 (2255000 to 2260000)...\n",
      "Processing chunk 453/686 (2260000 to 2265000)...\n",
      "Processing chunk 454/686 (2265000 to 2270000)...\n",
      "Processing chunk 455/686 (2270000 to 2275000)...\n",
      "Completed 455/686 chunks (66.3%)\n",
      "Processing chunk 456/686 (2275000 to 2280000)...\n",
      "Processing chunk 457/686 (2280000 to 2285000)...\n",
      "Processing chunk 458/686 (2285000 to 2290000)...\n",
      "Processing chunk 459/686 (2290000 to 2295000)...\n",
      "Processing chunk 460/686 (2295000 to 2300000)...\n",
      "Completed 460/686 chunks (67.1%)\n",
      "Processing chunk 461/686 (2300000 to 2305000)...\n",
      "Processing chunk 462/686 (2305000 to 2310000)...\n",
      "Processing chunk 463/686 (2310000 to 2315000)...\n",
      "Processing chunk 464/686 (2315000 to 2320000)...\n",
      "Processing chunk 465/686 (2320000 to 2325000)...\n",
      "Completed 465/686 chunks (67.8%)\n",
      "Processing chunk 466/686 (2325000 to 2330000)...\n",
      "Processing chunk 467/686 (2330000 to 2335000)...\n",
      "Processing chunk 468/686 (2335000 to 2340000)...\n",
      "Processing chunk 469/686 (2340000 to 2345000)...\n",
      "Processing chunk 470/686 (2345000 to 2350000)...\n",
      "Completed 470/686 chunks (68.5%)\n",
      "Processing chunk 471/686 (2350000 to 2355000)...\n",
      "Processing chunk 472/686 (2355000 to 2360000)...\n",
      "Processing chunk 473/686 (2360000 to 2365000)...\n",
      "Processing chunk 474/686 (2365000 to 2370000)...\n",
      "Processing chunk 475/686 (2370000 to 2375000)...\n",
      "Completed 475/686 chunks (69.2%)\n",
      "Processing chunk 476/686 (2375000 to 2380000)...\n",
      "Processing chunk 477/686 (2380000 to 2385000)...\n",
      "Processing chunk 478/686 (2385000 to 2390000)...\n",
      "Processing chunk 479/686 (2390000 to 2395000)...\n",
      "Processing chunk 480/686 (2395000 to 2400000)...\n",
      "Completed 480/686 chunks (70.0%)\n",
      "Processing chunk 481/686 (2400000 to 2405000)...\n",
      "Processing chunk 482/686 (2405000 to 2410000)...\n",
      "Processing chunk 483/686 (2410000 to 2415000)...\n",
      "Processing chunk 484/686 (2415000 to 2420000)...\n",
      "Processing chunk 485/686 (2420000 to 2425000)...\n",
      "Completed 485/686 chunks (70.7%)\n",
      "Processing chunk 486/686 (2425000 to 2430000)...\n",
      "Processing chunk 487/686 (2430000 to 2435000)...\n",
      "Processing chunk 488/686 (2435000 to 2440000)...\n",
      "Processing chunk 489/686 (2440000 to 2445000)...\n",
      "Processing chunk 490/686 (2445000 to 2450000)...\n",
      "Completed 490/686 chunks (71.4%)\n",
      "Processing chunk 491/686 (2450000 to 2455000)...\n",
      "Processing chunk 492/686 (2455000 to 2460000)...\n",
      "Processing chunk 493/686 (2460000 to 2465000)...\n",
      "Processing chunk 494/686 (2465000 to 2470000)...\n",
      "Processing chunk 495/686 (2470000 to 2475000)...\n",
      "Completed 495/686 chunks (72.2%)\n",
      "Processing chunk 496/686 (2475000 to 2480000)...\n",
      "Processing chunk 497/686 (2480000 to 2485000)...\n",
      "Processing chunk 498/686 (2485000 to 2490000)...\n",
      "Processing chunk 499/686 (2490000 to 2495000)...\n",
      "Processing chunk 500/686 (2495000 to 2500000)...\n",
      "Completed 500/686 chunks (72.9%)\n",
      "Processing chunk 501/686 (2500000 to 2505000)...\n",
      "Processing chunk 502/686 (2505000 to 2510000)...\n",
      "Processing chunk 503/686 (2510000 to 2515000)...\n",
      "Processing chunk 504/686 (2515000 to 2520000)...\n",
      "Processing chunk 505/686 (2520000 to 2525000)...\n",
      "Completed 505/686 chunks (73.6%)\n",
      "Processing chunk 506/686 (2525000 to 2530000)...\n",
      "Processing chunk 507/686 (2530000 to 2535000)...\n",
      "Processing chunk 508/686 (2535000 to 2540000)...\n",
      "Processing chunk 509/686 (2540000 to 2545000)...\n",
      "Processing chunk 510/686 (2545000 to 2550000)...\n",
      "Completed 510/686 chunks (74.3%)\n",
      "Processing chunk 511/686 (2550000 to 2555000)...\n",
      "Processing chunk 512/686 (2555000 to 2560000)...\n",
      "Processing chunk 513/686 (2560000 to 2565000)...\n",
      "Processing chunk 514/686 (2565000 to 2570000)...\n",
      "Processing chunk 515/686 (2570000 to 2575000)...\n",
      "Completed 515/686 chunks (75.1%)\n",
      "Processing chunk 516/686 (2575000 to 2580000)...\n",
      "Processing chunk 517/686 (2580000 to 2585000)...\n",
      "Processing chunk 518/686 (2585000 to 2590000)...\n",
      "Processing chunk 519/686 (2590000 to 2595000)...\n",
      "Processing chunk 520/686 (2595000 to 2600000)...\n",
      "Completed 520/686 chunks (75.8%)\n",
      "Processing chunk 521/686 (2600000 to 2605000)...\n",
      "Processing chunk 522/686 (2605000 to 2610000)...\n",
      "Processing chunk 523/686 (2610000 to 2615000)...\n",
      "Processing chunk 524/686 (2615000 to 2620000)...\n",
      "Processing chunk 525/686 (2620000 to 2625000)...\n",
      "Completed 525/686 chunks (76.5%)\n",
      "Processing chunk 526/686 (2625000 to 2630000)...\n",
      "Processing chunk 527/686 (2630000 to 2635000)...\n",
      "Processing chunk 528/686 (2635000 to 2640000)...\n",
      "Processing chunk 529/686 (2640000 to 2645000)...\n",
      "Processing chunk 530/686 (2645000 to 2650000)...\n",
      "Completed 530/686 chunks (77.3%)\n",
      "Processing chunk 531/686 (2650000 to 2655000)...\n",
      "Processing chunk 532/686 (2655000 to 2660000)...\n",
      "Processing chunk 533/686 (2660000 to 2665000)...\n",
      "Processing chunk 534/686 (2665000 to 2670000)...\n",
      "Processing chunk 535/686 (2670000 to 2675000)...\n",
      "Completed 535/686 chunks (78.0%)\n",
      "Processing chunk 536/686 (2675000 to 2680000)...\n",
      "Processing chunk 537/686 (2680000 to 2685000)...\n",
      "Processing chunk 538/686 (2685000 to 2690000)...\n",
      "Processing chunk 539/686 (2690000 to 2695000)...\n",
      "Processing chunk 540/686 (2695000 to 2700000)...\n",
      "Completed 540/686 chunks (78.7%)\n",
      "Processing chunk 541/686 (2700000 to 2705000)...\n",
      "Processing chunk 542/686 (2705000 to 2710000)...\n",
      "Processing chunk 543/686 (2710000 to 2715000)...\n",
      "Processing chunk 544/686 (2715000 to 2720000)...\n",
      "Processing chunk 545/686 (2720000 to 2725000)...\n",
      "Completed 545/686 chunks (79.4%)\n",
      "Processing chunk 546/686 (2725000 to 2730000)...\n",
      "Processing chunk 547/686 (2730000 to 2735000)...\n",
      "Processing chunk 548/686 (2735000 to 2740000)...\n",
      "Processing chunk 549/686 (2740000 to 2745000)...\n",
      "Processing chunk 550/686 (2745000 to 2750000)...\n",
      "Completed 550/686 chunks (80.2%)\n",
      "Processing chunk 551/686 (2750000 to 2755000)...\n",
      "Processing chunk 552/686 (2755000 to 2760000)...\n",
      "Processing chunk 553/686 (2760000 to 2765000)...\n",
      "Processing chunk 554/686 (2765000 to 2770000)...\n",
      "Processing chunk 555/686 (2770000 to 2775000)...\n",
      "Completed 555/686 chunks (80.9%)\n",
      "Processing chunk 556/686 (2775000 to 2780000)...\n",
      "Processing chunk 557/686 (2780000 to 2785000)...\n",
      "Processing chunk 558/686 (2785000 to 2790000)...\n",
      "Processing chunk 559/686 (2790000 to 2795000)...\n",
      "Processing chunk 560/686 (2795000 to 2800000)...\n",
      "Completed 560/686 chunks (81.6%)\n",
      "Processing chunk 561/686 (2800000 to 2805000)...\n",
      "Processing chunk 562/686 (2805000 to 2810000)...\n",
      "Processing chunk 563/686 (2810000 to 2815000)...\n",
      "Processing chunk 564/686 (2815000 to 2820000)...\n",
      "Processing chunk 565/686 (2820000 to 2825000)...\n",
      "Completed 565/686 chunks (82.4%)\n",
      "Processing chunk 566/686 (2825000 to 2830000)...\n",
      "Processing chunk 567/686 (2830000 to 2835000)...\n",
      "Processing chunk 568/686 (2835000 to 2840000)...\n",
      "Processing chunk 569/686 (2840000 to 2845000)...\n",
      "Processing chunk 570/686 (2845000 to 2850000)...\n",
      "Completed 570/686 chunks (83.1%)\n",
      "Processing chunk 571/686 (2850000 to 2855000)...\n",
      "Processing chunk 572/686 (2855000 to 2860000)...\n",
      "Processing chunk 573/686 (2860000 to 2865000)...\n",
      "Processing chunk 574/686 (2865000 to 2870000)...\n",
      "Processing chunk 575/686 (2870000 to 2875000)...\n",
      "Completed 575/686 chunks (83.8%)\n",
      "Processing chunk 576/686 (2875000 to 2880000)...\n",
      "Processing chunk 577/686 (2880000 to 2885000)...\n",
      "Processing chunk 578/686 (2885000 to 2890000)...\n",
      "Processing chunk 579/686 (2890000 to 2895000)...\n",
      "Processing chunk 580/686 (2895000 to 2900000)...\n",
      "Completed 580/686 chunks (84.5%)\n",
      "Processing chunk 581/686 (2900000 to 2905000)...\n",
      "Processing chunk 582/686 (2905000 to 2910000)...\n",
      "Processing chunk 583/686 (2910000 to 2915000)...\n",
      "Processing chunk 584/686 (2915000 to 2920000)...\n",
      "Processing chunk 585/686 (2920000 to 2925000)...\n",
      "Completed 585/686 chunks (85.3%)\n",
      "Processing chunk 586/686 (2925000 to 2930000)...\n",
      "Processing chunk 587/686 (2930000 to 2935000)...\n",
      "Processing chunk 588/686 (2935000 to 2940000)...\n",
      "Processing chunk 589/686 (2940000 to 2945000)...\n",
      "Processing chunk 590/686 (2945000 to 2950000)...\n",
      "Completed 590/686 chunks (86.0%)\n",
      "Processing chunk 591/686 (2950000 to 2955000)...\n",
      "Processing chunk 592/686 (2955000 to 2960000)...\n",
      "Processing chunk 593/686 (2960000 to 2965000)...\n",
      "Processing chunk 594/686 (2965000 to 2970000)...\n",
      "Processing chunk 595/686 (2970000 to 2975000)...\n",
      "Completed 595/686 chunks (86.7%)\n",
      "Processing chunk 596/686 (2975000 to 2980000)...\n",
      "Processing chunk 597/686 (2980000 to 2985000)...\n",
      "Processing chunk 598/686 (2985000 to 2990000)...\n",
      "Processing chunk 599/686 (2990000 to 2995000)...\n",
      "Processing chunk 600/686 (2995000 to 3000000)...\n",
      "Completed 600/686 chunks (87.5%)\n",
      "Processing chunk 601/686 (3000000 to 3005000)...\n",
      "Processing chunk 602/686 (3005000 to 3010000)...\n",
      "Processing chunk 603/686 (3010000 to 3015000)...\n",
      "Processing chunk 604/686 (3015000 to 3020000)...\n",
      "Processing chunk 605/686 (3020000 to 3025000)...\n",
      "Completed 605/686 chunks (88.2%)\n",
      "Processing chunk 606/686 (3025000 to 3030000)...\n",
      "Processing chunk 607/686 (3030000 to 3035000)...\n",
      "Processing chunk 608/686 (3035000 to 3040000)...\n",
      "Processing chunk 609/686 (3040000 to 3045000)...\n",
      "Processing chunk 610/686 (3045000 to 3050000)...\n",
      "Completed 610/686 chunks (88.9%)\n",
      "Processing chunk 611/686 (3050000 to 3055000)...\n",
      "Processing chunk 612/686 (3055000 to 3060000)...\n",
      "Processing chunk 613/686 (3060000 to 3065000)...\n",
      "Processing chunk 614/686 (3065000 to 3070000)...\n",
      "Processing chunk 615/686 (3070000 to 3075000)...\n",
      "Completed 615/686 chunks (89.7%)\n",
      "Processing chunk 616/686 (3075000 to 3080000)...\n",
      "Processing chunk 617/686 (3080000 to 3085000)...\n",
      "Processing chunk 618/686 (3085000 to 3090000)...\n",
      "Processing chunk 619/686 (3090000 to 3095000)...\n",
      "Processing chunk 620/686 (3095000 to 3100000)...\n",
      "Completed 620/686 chunks (90.4%)\n",
      "Processing chunk 621/686 (3100000 to 3105000)...\n",
      "Processing chunk 622/686 (3105000 to 3110000)...\n",
      "Processing chunk 623/686 (3110000 to 3115000)...\n",
      "Processing chunk 624/686 (3115000 to 3120000)...\n",
      "Processing chunk 625/686 (3120000 to 3125000)...\n",
      "Completed 625/686 chunks (91.1%)\n",
      "Processing chunk 626/686 (3125000 to 3130000)...\n",
      "Processing chunk 627/686 (3130000 to 3135000)...\n",
      "Processing chunk 628/686 (3135000 to 3140000)...\n",
      "Processing chunk 629/686 (3140000 to 3145000)...\n",
      "Processing chunk 630/686 (3145000 to 3150000)...\n",
      "Completed 630/686 chunks (91.8%)\n",
      "Processing chunk 631/686 (3150000 to 3155000)...\n",
      "Processing chunk 632/686 (3155000 to 3160000)...\n",
      "Processing chunk 633/686 (3160000 to 3165000)...\n",
      "Processing chunk 634/686 (3165000 to 3170000)...\n",
      "Processing chunk 635/686 (3170000 to 3175000)...\n",
      "Completed 635/686 chunks (92.6%)\n",
      "Processing chunk 636/686 (3175000 to 3180000)...\n",
      "Processing chunk 637/686 (3180000 to 3185000)...\n",
      "Processing chunk 638/686 (3185000 to 3190000)...\n",
      "Processing chunk 639/686 (3190000 to 3195000)...\n",
      "Processing chunk 640/686 (3195000 to 3200000)...\n",
      "Completed 640/686 chunks (93.3%)\n",
      "Processing chunk 641/686 (3200000 to 3205000)...\n",
      "Processing chunk 642/686 (3205000 to 3210000)...\n",
      "Processing chunk 643/686 (3210000 to 3215000)...\n",
      "Processing chunk 644/686 (3215000 to 3220000)...\n",
      "Processing chunk 645/686 (3220000 to 3225000)...\n",
      "Completed 645/686 chunks (94.0%)\n",
      "Processing chunk 646/686 (3225000 to 3230000)...\n",
      "Processing chunk 647/686 (3230000 to 3235000)...\n",
      "Processing chunk 648/686 (3235000 to 3240000)...\n",
      "Processing chunk 649/686 (3240000 to 3245000)...\n",
      "Processing chunk 650/686 (3245000 to 3250000)...\n",
      "Completed 650/686 chunks (94.8%)\n",
      "Processing chunk 651/686 (3250000 to 3255000)...\n",
      "Processing chunk 652/686 (3255000 to 3260000)...\n",
      "Processing chunk 653/686 (3260000 to 3265000)...\n",
      "Processing chunk 654/686 (3265000 to 3270000)...\n",
      "Processing chunk 655/686 (3270000 to 3275000)...\n",
      "Completed 655/686 chunks (95.5%)\n",
      "Processing chunk 656/686 (3275000 to 3280000)...\n",
      "Processing chunk 657/686 (3280000 to 3285000)...\n",
      "Processing chunk 658/686 (3285000 to 3290000)...\n",
      "Processing chunk 659/686 (3290000 to 3295000)...\n",
      "Processing chunk 660/686 (3295000 to 3300000)...\n",
      "Completed 660/686 chunks (96.2%)\n",
      "Processing chunk 661/686 (3300000 to 3305000)...\n",
      "Processing chunk 662/686 (3305000 to 3310000)...\n",
      "Processing chunk 663/686 (3310000 to 3315000)...\n",
      "Processing chunk 664/686 (3315000 to 3320000)...\n",
      "Processing chunk 665/686 (3320000 to 3325000)...\n",
      "Completed 665/686 chunks (96.9%)\n",
      "Processing chunk 666/686 (3325000 to 3330000)...\n",
      "Processing chunk 667/686 (3330000 to 3335000)...\n",
      "Processing chunk 668/686 (3335000 to 3340000)...\n",
      "Processing chunk 669/686 (3340000 to 3345000)...\n",
      "Processing chunk 670/686 (3345000 to 3350000)...\n",
      "Completed 670/686 chunks (97.7%)\n",
      "Processing chunk 671/686 (3350000 to 3355000)...\n",
      "Processing chunk 672/686 (3355000 to 3360000)...\n",
      "Processing chunk 673/686 (3360000 to 3365000)...\n",
      "Processing chunk 674/686 (3365000 to 3370000)...\n",
      "Processing chunk 675/686 (3370000 to 3375000)...\n",
      "Completed 675/686 chunks (98.4%)\n",
      "Processing chunk 676/686 (3375000 to 3380000)...\n",
      "Processing chunk 677/686 (3380000 to 3385000)...\n",
      "Processing chunk 678/686 (3385000 to 3390000)...\n",
      "Processing chunk 679/686 (3390000 to 3395000)...\n",
      "Processing chunk 680/686 (3395000 to 3400000)...\n",
      "Completed 680/686 chunks (99.1%)\n",
      "Processing chunk 681/686 (3400000 to 3405000)...\n",
      "Processing chunk 682/686 (3405000 to 3410000)...\n",
      "Processing chunk 683/686 (3410000 to 3415000)...\n",
      "Processing chunk 684/686 (3415000 to 3420000)...\n",
      "Processing chunk 685/686 (3420000 to 3425000)...\n",
      "Completed 685/686 chunks (99.9%)\n",
      "Processing chunk 686/686 (3425000 to 3426061)...\n",
      "Completed 686/686 chunks (100.0%)\n",
      "Adding MTBF columns to remaining flight data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96216/1282656288.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  flight_remaining[col] = None\n",
      "/tmp/ipykernel_96216/1282656288.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  flight_remaining[col] = None\n",
      "/tmp/ipykernel_96216/1282656288.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  flight_remaining[col] = None\n",
      "/tmp/ipykernel_96216/1282656288.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  flight_remaining[col] = None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining datasets...\n",
      "Rows with MTBF data after matching: 989942 out of 6909194 (14.33%)\n",
      "\n",
      "Week difference statistics:\n",
      "count     989942.000000\n",
      "unique       104.000000\n",
      "top            1.857143\n",
      "freq      140950.000000\n",
      "Name: mtbf_week_diff, dtype: float64\n",
      "\n",
      "Distribution of week differences:\n",
      "0-4 weeks: 624798 (63.1%)\n",
      "4-12 weeks: 61796 (6.2%)\n",
      "12-26 weeks: 107395 (10.8%)\n",
      "26-52 weeks: 195953 (19.8%)\n",
      "52+ weeks: 0 (0.0%)\n",
      "         PartNumber  WeekStart mtbf_WeekStart mtbf_week_diff  \\\n",
      "3388596  00-6243-10 2019-07-08     2019-12-31      25.142857   \n",
      "4350860  00-6233-02 2023-12-25     2023-05-30      29.857143   \n",
      "489440   00-5024-01 2020-03-16     2020-03-31       2.142857   \n",
      "342087   00-6203-01 2019-03-04     2019-02-26       0.857143   \n",
      "3939376  00-5155-12 2022-02-28     2022-05-31      13.142857   \n",
      "\n",
      "        mtbf_ContractualMTBF mtbf_Failures  \n",
      "3388596                    0             0  \n",
      "4350860                15000             0  \n",
      "489440                 35000             0  \n",
      "342087                     0             6  \n",
      "3939376                30000             0  \n",
      "Saving merged data...\n",
      "Data saved!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define paths\n",
    "processed_data_dir = os.path.join('private', 'data', 'processed')\n",
    "\n",
    "print(\"Loading data...\")\n",
    "input_path = os.path.join(processed_data_dir, 'weekly_sequence_3_flight.parquet')\n",
    "flight_sequence_df = pd.read_parquet(input_path)\n",
    "print(\"Weekly sequence loaded\")\n",
    "\n",
    "# Make sure data_dir is defined correctly\n",
    "data_dir = os.path.join('private', 'data', 'transformed') \n",
    "input_path = os.path.join(data_dir, 'mtbf.parquet')\n",
    "mtbf_df = pd.read_parquet(input_path)\n",
    "print(\"MTBF data loaded\")\n",
    "\n",
    "for df in [flight_sequence_df, mtbf_df]:\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "\n",
    "print(mtbf_df.head())\n",
    "duplicates = mtbf_df[mtbf_df.duplicated(subset=['PartNumber', 'Month', 'Airline'], keep=False)]\n",
    "print(f\"Number of duplicate partnumbers: {len(duplicates)}\")\n",
    "\n",
    "# date range\n",
    "min_date = mtbf_df['Month'].min()\n",
    "max_date = mtbf_df['Month'].max()\n",
    "print(f\"Date range: {min_date} to {max_date}\")\n",
    "\n",
    "# Create a weekly date range\n",
    "mtbf_df['WeekStart'] = pd.to_datetime(mtbf_df['Month'].dt.to_period('W-MON').dt.start_time)\n",
    "\n",
    "# Unique values summary\n",
    "print(\"\\nUnique values in essential columns\")\n",
    "for col in ['PartNumber', 'Month', 'Airline', 'ContractualMTBF', 'Failures', 'FlightHours']:\n",
    "    total_values = mtbf_df[col].count()  # Count non-NaN values\n",
    "    unique_values = mtbf_df[col].nunique()\n",
    "    if total_values > 0:\n",
    "        unique_percent = (unique_values / total_values) * 100\n",
    "    else:\n",
    "        unique_percent = 0\n",
    "    print(f\"{col}: {unique_values} unique values ({unique_percent:.2f}%) out of {total_values} non-null values\")\n",
    "\n",
    "# Find overlapping partnumbers\n",
    "flight_unique_parts = set(flight_sequence_df['PartNumber'].dropna().unique())\n",
    "mtbf_unique_parts = set(mtbf_df['PartNumber'].dropna().unique())\n",
    "overlap_parts = flight_unique_parts.intersection(mtbf_unique_parts)\n",
    "\n",
    "print(f\"Unique parts in flight data: {len(flight_unique_parts)}\")\n",
    "print(f\"Unique parts in MTBF data: {len(mtbf_unique_parts)}\")\n",
    "print(f\"Overlapping unique parts: {len(overlap_parts)}\")\n",
    "\n",
    "mtbf_df_filtered = mtbf_df[mtbf_df['PartNumber'].isin(overlap_parts)].copy()\n",
    "print(f\"Filtered MTBF data from {len(mtbf_df)} to {len(mtbf_df_filtered)} rows\")\n",
    "\n",
    "# Add the WeekStart column to flight_sequence_df to avoid doing it multiple times\n",
    "if 'WeekStart' not in flight_sequence_df.columns:\n",
    "    flight_sequence_df['WeekStart'] = pd.to_datetime(flight_sequence_df['Date'].dt.to_period('W-MON').dt.start_time)\n",
    "\n",
    "flight_filtered = flight_sequence_df[flight_sequence_df['PartNumber'].isin(overlap_parts)].copy()\n",
    "print(f\"Filtered flight data from {len(flight_sequence_df)} to {len(flight_filtered)} rows\")\n",
    "\n",
    "# Aggregate MTBF data by PartNumber and WeekStart\n",
    "print(\"Aggregating MTBF data by PartNumber and WeekStart...\")\n",
    "mtbf_weekly_data = mtbf_df_filtered.groupby(['PartNumber', 'WeekStart']).agg({\n",
    "    'ContractualMTBF': 'first',  # Contractual MTBF per week\n",
    "    'Failures': 'sum',  # Total failures per week\n",
    "    'Airline': 'first',  # Airline\n",
    "}).reset_index()\n",
    "\n",
    "print(\"Weekly MTBF data created\")\n",
    "print(mtbf_weekly_data.head())\n",
    "\n",
    "del mtbf_df\n",
    "del mtbf_df_filtered\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "mtbf_lookup = {}\n",
    "for _, row in mtbf_weekly_data.iterrows():\n",
    "    part = row['PartNumber']\n",
    "    week = row['WeekStart']\n",
    "    \n",
    "    if part not in mtbf_lookup:\n",
    "        mtbf_lookup[part] = {}\n",
    "    \n",
    "    mtbf_lookup[part][week] = {\n",
    "        'ContractualMTBF': row['ContractualMTBF'],\n",
    "        'Failures': row['Failures'],\n",
    "        'Airline': row['Airline']\n",
    "    }\n",
    "\n",
    "del mtbf_weekly_data\n",
    "gc.collect()\n",
    "\n",
    "def find_closest_mtbf_week(row, max_weeks_diff=52):\n",
    "    part = row['PartNumber']\n",
    "    week = row['WeekStart']\n",
    "    airline = row.get('Airline')\n",
    "    \n",
    "    # Skip if part is not in MTBF data\n",
    "    if pd.isna(part) or part not in mtbf_lookup:\n",
    "        return pd.Series([None, None, None, None], \n",
    "                         index=['mtbf_ContractualMTBF', 'mtbf_Failures', 'mtbf_week_diff', 'mtbf_WeekStart'])\n",
    "    \n",
    "    # Find all weeks for this part\n",
    "    weeks_data = mtbf_lookup[part]\n",
    "    if not weeks_data:\n",
    "        return pd.Series([None, None, None, None], \n",
    "                         index=['mtbf_ContractualMTBF', 'mtbf_Failures', 'mtbf_week_diff', 'mtbf_WeekStart'])\n",
    "    \n",
    "    # Filter by airline\n",
    "    if not pd.isna(airline):\n",
    "        matching_weeks = {w: data for w, data in weeks_data.items() \n",
    "                         if data['Airline'] == airline}\n",
    "        if matching_weeks:\n",
    "            weeks_data = matching_weeks\n",
    "    \n",
    "    # Calculate time differences\n",
    "    week_diffs = []\n",
    "    for w in weeks_data:\n",
    "        days_diff = abs((w - week).days)\n",
    "        weeks_diff = days_diff / 7\n",
    "        if weeks_diff <= max_weeks_diff:\n",
    "            week_diffs.append((weeks_diff, w))\n",
    "    \n",
    "    if not week_diffs:\n",
    "        return pd.Series([None, None, None, None], \n",
    "                         index=['mtbf_ContractualMTBF', 'mtbf_Failures', 'mtbf_week_diff', 'mtbf_WeekStart'])\n",
    "    \n",
    "    # Find closest week\n",
    "    closest_diff, closest_week = min(week_diffs, key=lambda x: x[0])\n",
    "    mtbf_data = weeks_data[closest_week]\n",
    "    \n",
    "    return pd.Series([\n",
    "        mtbf_data['ContractualMTBF'],\n",
    "        mtbf_data['Failures'],\n",
    "        closest_diff,\n",
    "        closest_week\n",
    "    ], index=['mtbf_ContractualMTBF', 'mtbf_Failures', 'mtbf_week_diff', 'mtbf_WeekStart'])\n",
    "\n",
    "# Process in chunks\n",
    "chunk_size = 5000 \n",
    "num_chunks = (len(flight_filtered) + chunk_size - 1) // chunk_size\n",
    "\n",
    "mtbf_ContractualMTBF = np.empty(len(flight_filtered), dtype=object)\n",
    "mtbf_Failures = np.empty(len(flight_filtered), dtype=object)\n",
    "mtbf_week_diff = np.empty(len(flight_filtered), dtype=object)\n",
    "mtbf_WeekStart = np.empty(len(flight_filtered), dtype=object)\n",
    "\n",
    "print(f\"Processing {len(flight_filtered)} rows in {num_chunks} chunks...\")\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, len(flight_filtered))\n",
    "    chunk = flight_filtered.iloc[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Processing chunk {i+1}/{num_chunks} ({start_idx} to {end_idx})...\")\n",
    "    \n",
    "    # Process each row in the chunk\n",
    "    for j, (idx, row) in enumerate(chunk.iterrows()):\n",
    "        result = find_closest_mtbf_week(row)\n",
    "        mtbf_ContractualMTBF[start_idx + j] = result['mtbf_ContractualMTBF']\n",
    "        mtbf_Failures[start_idx + j] = result['mtbf_Failures']\n",
    "        mtbf_week_diff[start_idx + j] = result['mtbf_week_diff']\n",
    "        mtbf_WeekStart[start_idx + j] = result['mtbf_WeekStart']\n",
    "    \n",
    "    # Print progress update and clear memory\n",
    "    if (i+1) % 5 == 0 or i+1 == num_chunks:\n",
    "        print(f\"Completed {i+1}/{num_chunks} chunks ({(i+1)/num_chunks*100:.1f}%)\")\n",
    "        gc.collect()  # Force garbage collection\n",
    "\n",
    "# Add results to flight_filtered\n",
    "flight_filtered['mtbf_ContractualMTBF'] = mtbf_ContractualMTBF\n",
    "flight_filtered['mtbf_Failures'] = mtbf_Failures\n",
    "flight_filtered['mtbf_week_diff'] = mtbf_week_diff\n",
    "flight_filtered['mtbf_WeekStart'] = mtbf_WeekStart\n",
    "\n",
    "# Add columns to flight_remaining\n",
    "print(\"Adding MTBF columns to remaining flight data...\")\n",
    "flight_remaining = flight_sequence_df[~flight_sequence_df['PartNumber'].isin(overlap_parts)]\n",
    "for col in ['mtbf_ContractualMTBF', 'mtbf_Failures', 'mtbf_week_diff', 'mtbf_WeekStart']:\n",
    "    flight_remaining[col] = None\n",
    "\n",
    "# Combine the data efficiently\n",
    "print(\"Combining datasets...\")\n",
    "indexes_to_update = flight_filtered.index\n",
    "mtbf_sequence_df = flight_sequence_df.copy()\n",
    "\n",
    "# Update only the necessary rows with MTBF data\n",
    "for col in ['mtbf_ContractualMTBF', 'mtbf_Failures', 'mtbf_week_diff', 'mtbf_WeekStart']:\n",
    "    mtbf_sequence_df.loc[indexes_to_update, col] = flight_filtered[col].values\n",
    "\n",
    "matched_rows = mtbf_sequence_df['mtbf_ContractualMTBF'].notna().sum()\n",
    "print(f\"Rows with MTBF data after matching: {matched_rows} out of {len(mtbf_sequence_df)} ({matched_rows/len(mtbf_sequence_df)*100:.2f}%)\")\n",
    "\n",
    "# Show distribution of week differences\n",
    "if 'mtbf_week_diff' in mtbf_sequence_df.columns and mtbf_sequence_df['mtbf_week_diff'].notna().any():\n",
    "    week_diff_stats = mtbf_sequence_df['mtbf_week_diff'].describe()\n",
    "    print(\"\\nWeek difference statistics:\")\n",
    "    print(week_diff_stats)\n",
    "    \n",
    "    print(\"\\nDistribution of week differences:\")\n",
    "    week_ranges = [(0, 4), (4, 12), (12, 26), (26, 52)]\n",
    "    for start, end in week_ranges:\n",
    "        count = ((mtbf_sequence_df['mtbf_week_diff'] >= start) & (mtbf_sequence_df['mtbf_week_diff'] < end)).sum()\n",
    "        pct = count / matched_rows * 100 if matched_rows > 0 else 0\n",
    "        print(f\"{start}-{end} weeks: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    count = (mtbf_sequence_df['mtbf_week_diff'] >= 52).sum()\n",
    "    pct = count / matched_rows * 100 if matched_rows > 0 else 0\n",
    "    print(f\"52+ weeks: {count} ({pct:.1f}%)\")\n",
    "\n",
    "if matched_rows > 0:\n",
    "    sample_matched = mtbf_sequence_df[mtbf_sequence_df['mtbf_ContractualMTBF'].notna()].sample(min(5, matched_rows))\n",
    "    print(sample_matched[['PartNumber', 'WeekStart', 'mtbf_WeekStart', 'mtbf_week_diff', 'mtbf_ContractualMTBF', 'mtbf_Failures']])\n",
    "\n",
    "# Save the results\n",
    "print(\"Saving merged data...\")\n",
    "output_path = os.path.join(processed_data_dir, 'weekly_sequence_4_mtbf.parquet')\n",
    "mtbf_sequence_df.to_parquet(output_path)\n",
    "print(\"Data saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weekly sequence data...\n",
      "Weekly sequence loaded\n",
      "Date range: 2012-03-26 00:00:00 to 2058-11-25 00:00:00\n",
      "Number of unique parts: 326\n",
      "Calculating yearly completeness...\n",
      "\n",
      "Yearly completeness by field:\n",
      "      ReceivedCount  ShippedCount  InRepair  RawResets  SeatResets  \\\n",
      "2012         90.94%        90.94%    90.94%      0.00%       0.00%   \n",
      "2013        100.00%       100.00%   100.00%      0.00%       0.00%   \n",
      "2014        100.00%       100.00%   100.00%      0.00%       0.00%   \n",
      "2015        100.00%       100.00%   100.00%      0.00%       0.00%   \n",
      "2016        100.00%       100.00%   100.00%      0.00%       0.00%   \n",
      "2017        100.00%       100.00%   100.00%      0.00%       0.00%   \n",
      "2018        100.00%       100.00%   100.00%      0.00%       0.98%   \n",
      "2019        100.00%       100.00%   100.00%      0.01%       1.26%   \n",
      "2020        100.00%       100.00%   100.00%      0.12%       0.13%   \n",
      "2021        100.00%       100.00%   100.00%      0.07%       0.07%   \n",
      "2022        100.00%       100.00%   100.00%      0.00%       0.00%   \n",
      "2023        100.00%       100.00%   100.00%      7.58%       8.65%   \n",
      "2024        100.00%       100.00%   100.00%     10.98%      12.00%   \n",
      "2025         25.80%        25.80%    25.80%      9.02%       9.30%   \n",
      "2026          0.00%         0.00%     0.00%      2.57%       2.68%   \n",
      "\n",
      "      TotalPassengers  \n",
      "2012            0.00%  \n",
      "2013            0.00%  \n",
      "2014            0.00%  \n",
      "2015            0.00%  \n",
      "2016            0.00%  \n",
      "2017            0.00%  \n",
      "2018            0.98%  \n",
      "2019            1.26%  \n",
      "2020            0.13%  \n",
      "2021            0.07%  \n",
      "2022            0.00%  \n",
      "2023            8.65%  \n",
      "2024           12.00%  \n",
      "2025            9.30%  \n",
      "2026            2.68%  \n",
      "No year ranges found\n",
      "No complete years found, using default year range\n",
      "Training years: []\n",
      "Validation years: []\n",
      "Filtering data for correlation analysis...\n",
      "Using 100000 rows for correlation analysis\n",
      "Preparing data for correlation...\n",
      "Column ReceivedCount: dtype=float32, na_count=11289\n",
      "Column ShippedCount: dtype=float32, na_count=11289\n",
      "Column InRepair: dtype=float32, na_count=11289\n",
      "Column RawResets: dtype=float32, na_count=95583\n",
      "Column SeatResets: dtype=float32, na_count=95059\n",
      "Column TotalPassengers: dtype=float32, na_count=95059\n",
      "\n",
      "Correlation matrix for key fields:\n",
      "                 ReceivedCount  ShippedCount  InRepair  RawResets  SeatResets  \\\n",
      "ReceivedCount          100.00%        40.11%    58.09%      2.04%       1.80%   \n",
      "ShippedCount            40.11%       100.00%    48.04%      2.39%       2.20%   \n",
      "InRepair                58.09%        48.04%   100.00%      1.36%       1.06%   \n",
      "RawResets                2.04%         2.39%     1.36%    100.00%      98.23%   \n",
      "SeatResets               1.80%         2.20%     1.06%     98.23%     100.00%   \n",
      "TotalPassengers          0.66%         0.67%     1.60%     37.66%      35.53%   \n",
      "\n",
      "                 TotalPassengers  \n",
      "ReceivedCount              0.66%  \n",
      "ShippedCount               0.67%  \n",
      "InRepair                   1.60%  \n",
      "RawResets                 37.66%  \n",
      "SeatResets                35.53%  \n",
      "TotalPassengers          100.00%  \n",
      "\n",
      "Top correlations with ReceivedCount:\n",
      "InRepair          58.09%\n",
      "ShippedCount      40.11%\n",
      "RawResets          2.04%\n",
      "SeatResets         1.80%\n",
      "TotalPassengers    0.66%\n",
      "Name: ReceivedCount, dtype: float64\n",
      "\n",
      "Top correlations with ShippedCount:\n",
      "InRepair          48.04%\n",
      "ReceivedCount     40.11%\n",
      "RawResets          2.39%\n",
      "SeatResets         2.20%\n",
      "TotalPassengers    0.67%\n",
      "Name: ShippedCount, dtype: float64\n",
      "\n",
      "Top correlations with InRepair:\n",
      "ReceivedCount     58.09%\n",
      "ShippedCount      48.04%\n",
      "TotalPassengers    1.60%\n",
      "RawResets          1.36%\n",
      "SeatResets         1.06%\n",
      "Name: InRepair, dtype: float64\n",
      "\n",
      "Top correlations with RawResets:\n",
      "SeatResets        98.23%\n",
      "TotalPassengers   37.66%\n",
      "ShippedCount       2.39%\n",
      "ReceivedCount      2.04%\n",
      "InRepair           1.36%\n",
      "Name: RawResets, dtype: float64\n",
      "\n",
      "Top correlations with SeatResets:\n",
      "RawResets         98.23%\n",
      "TotalPassengers   35.53%\n",
      "ShippedCount       2.20%\n",
      "ReceivedCount      1.80%\n",
      "InRepair           1.06%\n",
      "Name: SeatResets, dtype: float64\n",
      "\n",
      "Top correlations with TotalPassengers:\n",
      "RawResets       37.66%\n",
      "SeatResets      35.53%\n",
      "InRepair         1.60%\n",
      "ShippedCount     0.67%\n",
      "ReceivedCount    0.66%\n",
      "Name: TotalPassengers, dtype: float64\n",
      "\n",
      "Data period coverage by field:\n",
      "ReceivedCount: 490822 values in sample from 2012-05-07 00:00:00 to 2025-03-31 00:00:00\n",
      "ShippedCount: 490608 values in sample from 2012-05-07 00:00:00 to 2025-03-31 00:00:00\n",
      "InRepair: 490492 values in sample from 2012-05-07 00:00:00 to 2025-03-31 00:00:00\n",
      "RawResets: 20150 values in sample from 2019-12-02 00:00:00 to 2026-04-06 00:00:00\n",
      "SeatResets: 22771 values in sample from 2018-06-11 00:00:00 to 2026-04-06 00:00:00\n",
      "TotalPassengers: 22844 values in sample from 2018-06-11 00:00:00 to 2026-04-06 00:00:00\n",
      "\n",
      "Variance by field:\n",
      "ReceivedCount variance (from sample): 30.92437171936035\n",
      "ShippedCount variance (from sample): 24.99958038330078\n",
      "InRepair variance (from sample): 338.732666015625\n",
      "RawResets variance calculation error: Cannot take a larger sample than population when 'replace=False'\n",
      "SeatResets variance calculation error: Cannot take a larger sample than population when 'replace=False'\n",
      "TotalPassengers variance calculation error: Cannot take a larger sample than population when 'replace=False'\n",
      "\n",
      "Data types by field:\n",
      "ReceivedCount: float32\n",
      "ShippedCount: float32\n",
      "InRepair: float32\n",
      "RawResets: float32\n",
      "SeatResets: float32\n",
      "TotalPassengers: float32\n",
      "Saving final sequence data...\n",
      "Saving data in 7 chunks...\n",
      "Saved chunk 1/7\n",
      "Saved chunk 2/7\n",
      "Saved chunk 3/7\n",
      "Saved chunk 4/7\n",
      "Saved chunk 5/7\n",
      "Saved chunk 6/7\n",
      "Saved chunk 7/7\n",
      "All chunks saved!\n",
      "Combined data saved to private/data/processed/weekly_sequence_5_final.parquet\n",
      "Removed temporary file: private/data/processed/weekly_sequence_5_final_temp_0.parquet\n",
      "Removed temporary file: private/data/processed/weekly_sequence_5_final_temp_1.parquet\n",
      "Removed temporary file: private/data/processed/weekly_sequence_5_final_temp_2.parquet\n",
      "Removed temporary file: private/data/processed/weekly_sequence_5_final_temp_3.parquet\n",
      "Removed temporary file: private/data/processed/weekly_sequence_5_final_temp_4.parquet\n",
      "Removed temporary file: private/data/processed/weekly_sequence_5_final_temp_5.parquet\n",
      "Removed temporary file: private/data/processed/weekly_sequence_5_final_temp_6.parquet\n",
      "Temporary files removed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "\n",
    "# Define directories\n",
    "processed_data_dir = os.path.join('private', 'data', 'processed')\n",
    "data_dir = os.path.join('private', 'data', 'transformed')\n",
    "\n",
    "print(\"Loading weekly sequence data...\")\n",
    "input_path = os.path.join(processed_data_dir, 'weekly_sequence_4_mtbf.parquet')\n",
    "\n",
    "needed_columns = [\n",
    "    'PartNumber', 'WeekStart', 'ContractualMTBF', 'EntryIntoServiceDate',\n",
    "    'RunningFleetTotal', 'ReceivedCount', 'ShippedCount', 'InRepair',\n",
    "    'RawResets', 'Failures', 'SeatResets', 'TotalPassengers', 'Tail',\n",
    "    'StandardTail', 'Airline', 'FlightDuration', 'BusinessClass', 'EconomyClass'\n",
    "]\n",
    "\n",
    "try:\n",
    "    mtbf_sequence_df = pd.read_parquet(input_path, columns=needed_columns)\n",
    "except Exception as e:\n",
    "    file_schema = pd.read_parquet(input_path, columns=None).head(0)\n",
    "    available_columns = file_schema.columns.tolist()    \n",
    "    existing_columns = [col for col in needed_columns if col in available_columns]\n",
    "    mtbf_sequence_df = pd.read_parquet(input_path, columns=existing_columns)\n",
    "\n",
    "print(\"Weekly sequence loaded\")\n",
    "\n",
    "# Create Year column\n",
    "mtbf_sequence_df['Year'] = mtbf_sequence_df['WeekStart'].dt.year\n",
    "\n",
    "for col in mtbf_sequence_df.select_dtypes(include=['float64']).columns:\n",
    "    mtbf_sequence_df[col] = pd.to_numeric(mtbf_sequence_df[col], downcast='float')\n",
    "for col in mtbf_sequence_df.select_dtypes(include=['int64']).columns:\n",
    "    mtbf_sequence_df[col] = pd.to_numeric(mtbf_sequence_df[col], downcast='integer')\n",
    "\n",
    "# Contractual MTBF should be updated at the existence of a record then carried forward\n",
    "if 'ContractualMTBF' in mtbf_sequence_df.columns:\n",
    "    unique_parts = mtbf_sequence_df['PartNumber'].unique()\n",
    "    chunk_size = 1000  \n",
    "    num_chunks = (len(unique_parts) + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, len(unique_parts))\n",
    "        chunk_parts = unique_parts[start_idx:end_idx]\n",
    "        \n",
    "        part_mask = mtbf_sequence_df['PartNumber'].isin(chunk_parts)\n",
    "        \n",
    "        # Fill this chunk of parts\n",
    "        mtbf_sequence_df.loc[part_mask, 'ContractualMTBF'] = (\n",
    "            mtbf_sequence_df.loc[part_mask]\n",
    "            .groupby('PartNumber')['ContractualMTBF']\n",
    "            .ffill()\n",
    "        )\n",
    "        \n",
    "        # Print progress\n",
    "        if (i+1) % 10 == 0 or i+1 == num_chunks:\n",
    "            print(f\"Completed {i+1}/{num_chunks} chunks ({(i+1)/num_chunks*100:.1f}%)\")\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "print(f\"Date range: {mtbf_sequence_df['WeekStart'].min()} to {mtbf_sequence_df['WeekStart'].max()}\")\n",
    "print(f\"Number of unique parts: {mtbf_sequence_df['PartNumber'].nunique()}\")\n",
    "\n",
    "# Define fields for analysis\n",
    "fields = [\n",
    "    'EntryIntoServiceDate',    # Important for equipment part age\n",
    "    'RunningFleetTotal',       # Critical for normalizing failure rates\n",
    "    'ReceivedCount',           # Number of parts received for repair\n",
    "    'ShippedCount',            # Number of parts shipped after repair\n",
    "    'InRepair',                # Number of parts currently in repair\n",
    "    'RawResets',               # System resets - indicator of issues\n",
    "    'Failures',                # Actual failures - primary prediction target\n",
    "    'ContractualMTBF',         # Expected time between failures - benchmark\n",
    "    'SeatResets',              # Passenger-facing issues\n",
    "    'TotalPassengers',         # For normalizing by usage\n",
    "]\n",
    "\n",
    "# Filter to only fields that exist in dataframe\n",
    "fields = [field for field in fields if field in mtbf_sequence_df.columns]\n",
    "print(\"Calculating yearly completeness...\")\n",
    "yearly_completeness = {}\n",
    "unique_years = mtbf_sequence_df['Year'].unique()\n",
    "unique_years.sort()\n",
    "\n",
    "for year in unique_years:\n",
    "    if year > 2026: \n",
    "        continue\n",
    "        \n",
    "    year_data = mtbf_sequence_df[mtbf_sequence_df['Year'] == year]\n",
    "    \n",
    "    field_completeness = {}\n",
    "    for field in fields:\n",
    "        field_completeness[field] = year_data[field].notna().mean()\n",
    "    \n",
    "    yearly_completeness[year] = field_completeness\n",
    "    \n",
    "    del year_data\n",
    "    gc.collect()\n",
    "\n",
    "yearly_completeness_df = pd.DataFrame.from_dict(yearly_completeness, orient='index')\n",
    "yearly_completeness_df = yearly_completeness_df.loc[yearly_completeness_df.index <= 2026]\n",
    "\n",
    "print(\"\\nYearly completeness by field:\")\n",
    "pd.set_option('display.float_format', '{:.2%}'.format)\n",
    "print(yearly_completeness_df)\n",
    "\n",
    "# Find years with good data for training/validation\n",
    "year_completeness = yearly_completeness_df.mean(axis=1) > 0.8  # Average across all fields\n",
    "complete_years = year_completeness[year_completeness].index.tolist()\n",
    "\n",
    "# Find continuous ranges of years\n",
    "year_ranges = []\n",
    "current_range = []\n",
    "\n",
    "for year in sorted(complete_years):\n",
    "    if len(current_range) == 0 or year == current_range[-1] + 1:\n",
    "        current_range.append(year)\n",
    "    else:\n",
    "        if current_range:  \n",
    "            year_ranges.append(current_range)  \n",
    "        current_range = [year]\n",
    "if current_range:  \n",
    "    year_ranges.append(current_range)\n",
    "\n",
    "# Find the longest range \n",
    "if year_ranges: \n",
    "    longest_range = max(year_ranges, key=len)\n",
    "    print(f\"Longest range: {longest_range}\")\n",
    "else:\n",
    "    longest_range = []\n",
    "    print(\"No year ranges found\")\n",
    "\n",
    "# Determine train/validation split with safety checks\n",
    "if longest_range and len(longest_range) > 3: \n",
    "    # Use most recent year with complete failure data for validation\n",
    "    failure_complete_years = [year for year, row in yearly_completeness_df.iterrows() \n",
    "                            if row.get('Failures', 0) > 0.8 and year in longest_range]\n",
    "    \n",
    "    if failure_complete_years:\n",
    "        # Find the most recent year with complete failure data\n",
    "        most_recent_complete = max(failure_complete_years)\n",
    "        validate_years = [most_recent_complete]\n",
    "        train_years = [year for year in longest_range if year < most_recent_complete]\n",
    "    else:\n",
    "        validate_years = [longest_range[-1]]\n",
    "        train_years = longest_range[:-1]\n",
    "elif longest_range:  # Some data but not enough for good split\n",
    "    print(\"Not enough continuous years of data for reliable validation\")\n",
    "    validate_years = [longest_range[-1]]\n",
    "    train_years = longest_range[:-1] if len(longest_range) > 1 else []\n",
    "else:\n",
    "    print(\"No complete years found, using default year range\")\n",
    "    validate_years = []\n",
    "    train_years = []\n",
    "\n",
    "print(f\"Training years: {train_years}\")\n",
    "print(f\"Validation years: {validate_years}\")\n",
    "\n",
    "# Define key fields for correlation analysis\n",
    "key_fields = [\n",
    "    'RunningFleetTotal',   \n",
    "    'ReceivedCount',         \n",
    "    'ShippedCount',          \n",
    "    'InRepair',              \n",
    "    'RawResets',            \n",
    "    'Failures',              \n",
    "    'ContractualMTBF',       \n",
    "    'SeatResets',            \n",
    "    'TotalPassengers'        \n",
    "]\n",
    "\n",
    "# Filter to only fields that exist in our dataframe\n",
    "key_fields = [field for field in key_fields if field in mtbf_sequence_df.columns]\n",
    "\n",
    "# Filter to recent years for correlation analysis\n",
    "print(\"Filtering data for correlation analysis...\")\n",
    "year_filter = (mtbf_sequence_df['Year'] >= 2019) & (mtbf_sequence_df['Year'] <= 2025)\n",
    "training_data_sample = mtbf_sequence_df[year_filter].sample(\n",
    "    min(100000, len(mtbf_sequence_df[year_filter])), \n",
    "    random_state=42\n",
    ")\n",
    "print(f\"Using {len(training_data_sample)} rows for correlation analysis\")\n",
    "\n",
    "# Clean data for correlation\n",
    "print(\"Preparing data for correlation...\")\n",
    "corr_df = training_data_sample[key_fields].copy()\n",
    "\n",
    "# Convert to numeric and handle NAs\n",
    "for col in key_fields:\n",
    "    # First check the column type and any potential issues\n",
    "    print(f\"Column {col}: dtype={corr_df[col].dtype}, na_count={corr_df[col].isna().sum()}\")\n",
    "    # Convert to numeric\n",
    "    corr_df[col] = pd.to_numeric(corr_df[col], errors='coerce')\n",
    "    # Replace NaN values with 0 for correlation calculation\n",
    "    corr_df[col] = corr_df[col].fillna(0)\n",
    "\n",
    "# Calculate correlation\n",
    "try:\n",
    "    key_field_corr = corr_df.corr()\n",
    "    print(\"\\nCorrelation matrix for key fields:\")\n",
    "    print(key_field_corr)\n",
    "    \n",
    "    for field in key_fields:\n",
    "        print(f\"\\nTop correlations with {field}:\")\n",
    "        field_corr = key_field_corr[field].sort_values(ascending=False)\n",
    "        print(field_corr.drop(field).head(5))\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating correlation: {e}\")    \n",
    "    # More aggressive cleaning approach\n",
    "    for col in key_fields:\n",
    "        # Keep only columns that can be converted to float\n",
    "        try:\n",
    "            corr_df[col] = corr_df[col].astype(float)\n",
    "        except:\n",
    "            print(f\"Dropping bad column: {col}\")\n",
    "            corr_df = corr_df.drop(columns=[col])\n",
    "    \n",
    "    # Try correlation again with remaining columns\n",
    "    if len(corr_df.columns) >= 2:\n",
    "        key_field_corr = corr_df.corr()\n",
    "        print(\"\\nCorrelation matrix after cleaning:\")\n",
    "        print(key_field_corr)\n",
    "    else:\n",
    "        print(\"Not enough valid columns for correlation analysis\")\n",
    "\n",
    "# Check for data period coverage by field\n",
    "print(\"\\nData period coverage by field:\")\n",
    "for field in key_fields:\n",
    "    if field in mtbf_sequence_df.columns:\n",
    "        # Sample data to avoid memory issues\n",
    "        sample_size = min(1000000, len(mtbf_sequence_df))\n",
    "        sampled_df = mtbf_sequence_df.sample(sample_size)\n",
    "        \n",
    "        # Get first and last non-null date for each field\n",
    "        field_data = sampled_df[sampled_df[field].notna()]\n",
    "        if len(field_data) > 0:\n",
    "            first_date = field_data['WeekStart'].min()\n",
    "            last_date = field_data['WeekStart'].max()\n",
    "            print(f\"{field}: {len(field_data)} values in sample from {first_date} to {last_date}\")\n",
    "        else:\n",
    "            print(f\"{field}: No valid data in sample\")\n",
    "        \n",
    "        # Clean up\n",
    "        del sampled_df\n",
    "        del field_data\n",
    "        gc.collect()\n",
    "\n",
    "# Check for variance in fields\n",
    "print(\"\\nVariance by field:\")\n",
    "for field in key_fields:\n",
    "    try:\n",
    "        # Sample data to avoid memory issues\n",
    "        sample_size = min(1000000, len(mtbf_sequence_df))\n",
    "        field_sample = mtbf_sequence_df[field].dropna().sample(sample_size)\n",
    "        \n",
    "        if len(field_sample) > 0:\n",
    "            field_sample = pd.to_numeric(field_sample, errors='coerce')\n",
    "            var = field_sample.var()\n",
    "            print(f\"{field} variance (from sample): {var}\")\n",
    "        else:\n",
    "            print(f\"{field}: No valid data for variance calculation\")\n",
    "    except Exception as e:\n",
    "        print(f\"{field} variance calculation error: {e}\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types by field:\")\n",
    "for field in key_fields:\n",
    "    if field in mtbf_sequence_df.columns:\n",
    "        dtype = mtbf_sequence_df[field].dtype\n",
    "        print(f\"{field}: {dtype}\")\n",
    "\n",
    "# Save the final sequence data\n",
    "print(\"Saving final sequence data...\")\n",
    "output_path = os.path.join(processed_data_dir, 'weekly_sequence_5_final.parquet')\n",
    "\n",
    "# Save in chunks if the data is large\n",
    "rows_per_chunk = 1000000\n",
    "num_save_chunks = (len(mtbf_sequence_df) + rows_per_chunk - 1) // rows_per_chunk\n",
    "\n",
    "if num_save_chunks <= 1:\n",
    "    # Small enough to save as one file\n",
    "    mtbf_sequence_df.to_parquet(output_path)\n",
    "    print(\"Data saved!\")\n",
    "else:\n",
    "    # Save in multiple chunks\n",
    "    print(f\"Saving data in {num_save_chunks} chunks...\")\n",
    "    temp_paths = []\n",
    "    \n",
    "    for i in range(num_save_chunks):\n",
    "        start_idx = i * rows_per_chunk\n",
    "        end_idx = min((i + 1) * rows_per_chunk, len(mtbf_sequence_df))\n",
    "        \n",
    "        chunk = mtbf_sequence_df.iloc[start_idx:end_idx]\n",
    "        temp_path = os.path.join(processed_data_dir, f'weekly_sequence_5_final_temp_{i}.parquet')\n",
    "        chunk.to_parquet(temp_path)\n",
    "        temp_paths.append(temp_path)\n",
    "        \n",
    "        del chunk\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Saved chunk {i+1}/{num_save_chunks}\")\n",
    "    \n",
    "    print(\"All chunks saved!\")\n",
    "\n",
    "    # Combine chunks into one file if needed\n",
    "    combined_df = pd.concat([pd.read_parquet(path) for path in temp_paths], ignore_index=True)\n",
    "    combined_df.to_parquet(output_path)\n",
    "    print(f\"Combined data saved to {output_path}\")\n",
    "    # Clean up temporary files\n",
    "    for path in temp_paths:\n",
    "        os.remove(path)\n",
    "        print(f\"Removed temporary file: {path}\")\n",
    "    print(\"Temporary files removed\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for RNN Training\n",
    "\n",
    "**Note:** All company-specific file names, column names, table references, and identifiers have been obfuscated or generalized to protect proprietary information while maintaining the analytical structure of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "data_dir = os.path.join('private', 'data', 'transformed')\n",
    "\n",
    "expected_files = [\n",
    "    'merged_inflight_parts.parquet',\n",
    "    'combined_rma.parquet',\n",
    "    'merged_flight_data.parquet',\n",
    "    'mtbf.parquet'\n",
    "]\n",
    "\n",
    "datasets = {}\n",
    "pd.reset_option('display.float_format')\n",
    "\n",
    "# Only load the files we need\n",
    "file_path = os.path.join(data_dir, 'merged_inflight_parts.parquet')\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"File {file_path} not found\")\n",
    "\n",
    "merged_inflight_parts_df = pd.read_parquet(file_path)\n",
    "\n",
    "# Convert to same dtypes\n",
    "for col in merged_inflight_parts_df.select_dtypes(include=['float64']).columns:\n",
    "    merged_inflight_parts_df[col] = pd.to_numeric(merged_inflight_parts_df[col], downcast='float')\n",
    "for col in merged_inflight_parts_df.select_dtypes(include=['int64']).columns:\n",
    "    merged_inflight_parts_df[col] = pd.to_numeric(merged_inflight_parts_df[col], downcast='integer')\n",
    "\n",
    "# Convert string columns to category\n",
    "categorical_candidate_cols = merged_inflight_parts_df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_candidate_cols:\n",
    "    nunique = merged_inflight_parts_df[col].nunique()\n",
    "    if nunique < len(merged_inflight_parts_df) * 0.5:  # If fewer than 50% of values are unique\n",
    "        merged_inflight_parts_df[col] = merged_inflight_parts_df[col].astype('category')\n",
    "\n",
    "print(\"Datasets loaded\")\n",
    "\n",
    "# Default decommision date is EIS + 10 years\n",
    "merged_inflight_parts_df['DecommissionDate'] = pd.NaT\n",
    "mask = merged_inflight_parts_df['DecommissionDate'].isna()\n",
    "merged_inflight_parts_df.loc[mask, 'DecommissionDate'] = merged_inflight_parts_df.loc[mask, 'EntryIntoServiceDate'] + pd.Timedelta(days=365.25 * 10)\n",
    "\n",
    "print(f\"\\nInfight parts dataset shape: {merged_inflight_parts_df.shape}\")\n",
    "print(f\"Number of unique parts: {merged_inflight_parts_df['PartNumber'].nunique()}\")\n",
    "print(f\"Number of unique aircraft: {merged_inflight_parts_df['Tail'].nunique()}\")\n",
    "\n",
    "# Concat the program_id and aircraft_fleet Name Fields to create a unique identifier for each aircraft_fleet\n",
    "merged_inflight_parts_df['ProgramFleetNumber'] = merged_inflight_parts_df['program_id'] + '-' + merged_inflight_parts_df['group_name']\n",
    "\n",
    "# Count unique fleets and tails\n",
    "print(f\"Number of unique fleets: {merged_inflight_parts_df['ProgramFleetNumber'].nunique()}\")\n",
    "print(f\"Number of unique tails: {merged_inflight_parts_df['Tail'].nunique()}\")\n",
    "\n",
    "# Count number of missing customer_id values\n",
    "missing_owner_company = merged_inflight_parts_df['customer_id'].isna().sum()\n",
    "print(f\"Number of missing customer_id values: {missing_owner_company} ({(missing_owner_company / merged_inflight_parts_df.shape[0]) * 100:.2f}%)\")\n",
    "\n",
    "# Calculate percentage of unique values in key columns\n",
    "key_columns = ['PartNumber', 'Tail', 'ProgramFleetNumber', 'customer_id']\n",
    "print(\"\\nUnique values in key columns\")\n",
    "for col in key_columns:\n",
    "    total_values = merged_inflight_parts_df[col].count()\n",
    "    unique_values = merged_inflight_parts_df[col].nunique()\n",
    "    if total_values > 0:\n",
    "        unique_percent = (unique_values / total_values) * 100\n",
    "    else:\n",
    "        unique_percent = 0\n",
    "    print(f\"{col}: {unique_values} unique values ({unique_percent:.2f}%) out of {total_values} non-null values\")\n",
    "\n",
    "# Remove redundant columns and columns with no unique values\n",
    "redundant_columns = ['part_number', 'productname', 'group_name', 'group_number', \n",
    "                     'equipment_entry_id', 'delivery_equipment_entry_id',\n",
    "                     'location_type', 'EntryType', 'group_type', \n",
    "                     'definitionlevel', 'useequipmentfamily']\n",
    "drop_cols = [col for col in redundant_columns if col in merged_inflight_parts_df.columns]\n",
    "merged_inflight_parts_df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# New identifier column\n",
    "merged_inflight_parts_df['UniquePartInstallationId'] = (\n",
    "    merged_inflight_parts_df['ProgramFleetNumber'] + '-' + \n",
    "    merged_inflight_parts_df['PartNumber'] + '-' + \n",
    "    merged_inflight_parts_df['InstallLocation'].fillna('Unknown')\n",
    ")\n",
    "\n",
    "sample_size = min(100000, len(merged_inflight_parts_df))\n",
    "sample_df = merged_inflight_parts_df.sample(sample_size)\n",
    "unique_counts = sample_df.groupby('UniquePartInstallationId').size().reset_index(name='count')\n",
    "duplicates = unique_counts[unique_counts['count'] > 1]\n",
    "print(f\"Number of duplicate UniquePartInstallationIds in sample: {len(duplicates)}\")\n",
    "\n",
    "if len(duplicates) > 0:\n",
    "    agg_dict = {\n",
    "        'power_mode': lambda x: ';'.join(sorted(x.dropna().astype(str).unique())),\n",
    "        'max_power_input': 'max',\n",
    "    }\n",
    "    \n",
    "    all_other_columns = [col for col in merged_inflight_parts_df.columns \n",
    "                         if col not in agg_dict.keys() and col != 'UniquePartInstallationId']\n",
    "    \n",
    "    for col in all_other_columns:\n",
    "        agg_dict[col] = 'first'\n",
    "    \n",
    "    # Group and aggregate in chunks\n",
    "    chunk_size = 100000\n",
    "    num_chunks = (len(merged_inflight_parts_df) + chunk_size - 1) // chunk_size\n",
    "    aggregated_chunks = []\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, len(merged_inflight_parts_df))\n",
    "        print(f\"Processing chunk {i+1}/{num_chunks} for aggregation...\")\n",
    "        \n",
    "        chunk = merged_inflight_parts_df.iloc[start_idx:end_idx]\n",
    "        unique_ids = chunk['UniquePartInstallationId']\n",
    "        agg_chunk = chunk.groupby('UniquePartInstallationId').agg(agg_dict)        \n",
    "        agg_chunk = agg_chunk.reset_index()\n",
    "        aggregated_chunks.append(agg_chunk)\n",
    "        \n",
    "        del chunk\n",
    "        del unique_ids\n",
    "        gc.collect()\n",
    "    \n",
    "    # Combine chunks\n",
    "    merged_inflight_parts_df = pd.concat(aggregated_chunks, ignore_index=True)\n",
    "    del aggregated_chunks\n",
    "    gc.collect()\n",
    "else:\n",
    "    print(\"No duplicates found in sample, skipping aggregation.\")\n",
    "\n",
    "## Turn into weekly time series\n",
    "# Get range of dates\n",
    "min_date = merged_inflight_parts_df['EntryIntoServiceDate'].min()\n",
    "max_date = merged_inflight_parts_df['DecommissionDate'].max()\n",
    "\n",
    "# Create weekly date range\n",
    "date_range = pd.date_range(min_date, max_date, freq='W-MON')\n",
    "print(f\"Date range: {date_range[0]} to {date_range[-1]}\")\n",
    "\n",
    "# Sort by install date for efficiency in the loop\n",
    "merged_inflight_parts_df = merged_inflight_parts_df.sort_values('EntryIntoServiceDate')\n",
    "\n",
    "# Dictionaries for running totals\n",
    "running_fleet_totals = {}\n",
    "running_asset_totals = {}\n",
    "running_location_totals = {}\n",
    "running_owner_totals = {}\n",
    "\n",
    "# Create a list of required columns to keep\n",
    "required_columns = ['UniquePartInstallationId', 'PartNumber', 'Tail', 'InstallLocation', \n",
    "                    'customer_id', 'DeliveryQuantity', 'EntryIntoServiceDate', \n",
    "                    'DecommissionDate', 'ProgramShipsetNumber']\n",
    "\n",
    "# Divide date range into chunks to process\n",
    "time_chunks = [date_range[i:i+52] for i in range(0, len(date_range), 52)]\n",
    "all_weekly_data = []\n",
    "\n",
    "for chunk_idx, time_chunk in enumerate(time_chunks):\n",
    "    print(f\"Processing time chunk {chunk_idx+1}/{len(time_chunks)} ({time_chunk[0]} to {time_chunk[-1]})...\")\n",
    "    \n",
    "    chunk_weekly_data = []\n",
    "    \n",
    "    for week_start in time_chunk:\n",
    "        week_end = week_start + timedelta(days=6)\n",
    "        \n",
    "        # Identify events for this week using boolean\n",
    "        install_mask = (\n",
    "            (merged_inflight_parts_df['EntryIntoServiceDate'] > week_start - timedelta(days=7)) &\n",
    "            (merged_inflight_parts_df['EntryIntoServiceDate'] <= week_end)\n",
    "        )\n",
    "        \n",
    "        decommision_mask = (\n",
    "            (merged_inflight_parts_df['DecommissionDate'] > week_start - timedelta(days=7)) &\n",
    "            (merged_inflight_parts_df['DecommissionDate'] <= week_end)\n",
    "        )\n",
    "        \n",
    "        # Process install events\n",
    "        for _, part in merged_inflight_parts_df[install_mask][required_columns].iterrows():\n",
    "            part_num = part['PartNumber']\n",
    "            quantity = part['DeliveryQuantity']\n",
    "            asset_id = part['AssetId']\n",
    "            location = part['InstallLocation']\n",
    "            company = part['customer_id']\n",
    "            \n",
    "            # Update running totals\n",
    "            running_fleet_totals[part_num] = running_fleet_totals.get(part_num, 0) + quantity\n",
    "            running_asset_totals[(part_num, asset_id)] = running_asset_totals.get((part_num, asset_id), 0) + quantity\n",
    "            running_location_totals[(part_num, location)] = running_location_totals.get((part_num, location), 0) + quantity\n",
    "            running_owner_totals[(part_num, company)] = running_owner_totals.get((part_num, company), 0) + quantity\n",
    "        \n",
    "        # Process decommision events\n",
    "        for _, part in merged_inflight_parts_df[decommision_mask][required_columns].iterrows():\n",
    "            part_num = part['PartNumber']\n",
    "            quantity = part['DeliveryQuantity']\n",
    "            asset_id = part['AssetId']\n",
    "            location = part['InstallLocation']\n",
    "            company = part['customer_id']\n",
    "            \n",
    "            # Update running totals\n",
    "            running_fleet_totals[part_num] = running_fleet_totals.get(part_num, 0) - quantity\n",
    "            running_asset_totals[(part_num, asset_id)] = running_asset_totals.get((part_num, asset_id), 0) - quantity\n",
    "            running_location_totals[(part_num, location)] = running_location_totals.get((part_num, location), 0) - quantity\n",
    "            running_owner_totals[(part_num, company)] = running_owner_totals.get((part_num, company), 0) - quantity\n",
    "        \n",
    "        # Find active parts for this week\n",
    "        active_mask = (\n",
    "            (merged_inflight_parts_df['EntryIntoServiceDate'] <= week_end) & \n",
    "            (merged_inflight_parts_df['DecommissionDate'] >= week_start)\n",
    "        )\n",
    "        \n",
    "        current_week_parts = merged_inflight_parts_df[active_mask][required_columns].copy()\n",
    "        # Add week start\n",
    "        current_week_parts['WeekStart'] = week_start\n",
    "        \n",
    "        # Add running totals\n",
    "        if min_date <= week_start <= max_date:\n",
    "            # Add running totals using vectorized operations where possible\n",
    "            current_week_parts['RunningFleetTotal'] = current_week_parts['PartNumber'].map(\n",
    "                lambda pn: max(0, running_fleet_totals.get(pn, 0))\n",
    "            )\n",
    "            \n",
    "            current_week_parts['RunningAssetTotal'] = current_week_parts.apply(\n",
    "                lambda row: max(0, running_asset_totals.get((row['PartNumber'], row['Tail']), 0)), axis=1\n",
    "            )\n",
    "            current_week_parts['RunningLocationTotal'] = current_week_parts.apply(\n",
    "                lambda row: max(0, running_location_totals.get((row['PartNumber'], row['InstallLocation']), 0)), axis=1\n",
    "            )\n",
    "            current_week_parts['RunningOwnerTotal'] = current_week_parts.apply(\n",
    "                lambda row: max(0, running_owner_totals.get((row['PartNumber'], row['customer_id']), 0)), axis=1\n",
    "            )\n",
    "        else:\n",
    "            # Set running totals to 0 outside the date range\n",
    "            current_week_parts['RunningFleetTotal'] = 0\n",
    "            current_week_parts['RunningAssetTotal'] = 0\n",
    "            current_week_parts['RunningLocationTotal'] = 0\n",
    "            current_week_parts['RunningOwnerTotal'] = 0\n",
    "        \n",
    "        chunk_weekly_data.append(current_week_parts)\n",
    "    \n",
    "    # Combine weeks in this chunk\n",
    "    if chunk_weekly_data:\n",
    "        time_chunk_df = pd.concat(chunk_weekly_data, ignore_index=True)\n",
    "        all_weekly_data.append(time_chunk_df)\n",
    "        \n",
    "        del chunk_weekly_data\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Time chunk {chunk_idx+1} completed with {len(time_chunk_df)} rows\")\n",
    "        del time_chunk_df\n",
    "        gc.collect()\n",
    "\n",
    "# Combine chunks\n",
    "weekly_sequence_df = pd.concat(all_weekly_data, ignore_index=True)\n",
    "\n",
    "del all_weekly_data\n",
    "del merged_inflight_parts_df\n",
    "gc.collect()\n",
    "\n",
    "print(\"Weekly sequences created!\")\n",
    "print(f\"Number of weeks covered: {weekly_sequence_df['WeekStart'].nunique()}\")\n",
    "print(f\"Number of unique parts: {weekly_sequence_df['PartNumber'].nunique()}\")\n",
    "print(f\"Date range: {weekly_sequence_df['WeekStart'].min()} to {weekly_sequence_df['WeekStart'].max()}\")\n",
    "\n",
    "# Save to parquet\n",
    "processed_data_dir = os.path.join('private', 'data', 'processed')\n",
    "if not os.path.exists(processed_data_dir):\n",
    "    os.makedirs(processed_data_dir)\n",
    "\n",
    "# Save in chunks\n",
    "rows_per_chunk = 1000000\n",
    "num_save_chunks = (len(weekly_sequence_df) + rows_per_chunk - 1) // rows_per_chunk\n",
    "\n",
    "if num_save_chunks <= 1:\n",
    "    output_path = os.path.join(processed_data_dir, 'weekly_sequence_1_parts.parquet')\n",
    "    weekly_sequence_df.to_parquet(output_path)\n",
    "    print(\"Data saved!\")\n",
    "else:\n",
    "    # Save in multiple chunks and then recombine\n",
    "    print(f\"Saving data in {num_save_chunks} chunks...\")\n",
    "    temp_file_paths = []\n",
    "    \n",
    "    for i in range(num_save_chunks):\n",
    "        start_idx = i * rows_per_chunk\n",
    "        end_idx = min((i + 1) * rows_per_chunk, len(weekly_sequence_df))\n",
    "        \n",
    "        chunk = weekly_sequence_df.iloc[start_idx:end_idx]\n",
    "        temp_path = os.path.join(processed_data_dir, f'weekly_sequence_1_parts_temp_{i}.parquet')\n",
    "        chunk.to_parquet(temp_path)\n",
    "        temp_file_paths.append(temp_path)\n",
    "        \n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    combined_df = pd.read_parquet(temp_file_paths[0])\n",
    "    \n",
    "    for i in range(1, len(temp_file_paths)):\n",
    "        next_chunk = pd.read_parquet(temp_file_paths[i])\n",
    "        combined_df = pd.concat([combined_df, next_chunk], ignore_index=True)\n",
    "        del next_chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Save the combined data\n",
    "    output_path = os.path.join(processed_data_dir, 'weekly_sequence_1_parts.parquet')\n",
    "    combined_df.to_parquet(output_path)\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    for temp_path in temp_file_paths:\n",
    "        os.remove(temp_path)\n",
    "    \n",
    "    print(\"Data saved and temporary files cleaned up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "\n",
    "# Define directories\n",
    "processed_data_dir = os.path.join('private', 'data', 'processed')\n",
    "data_dir = os.path.join('private', 'data', 'transformed')\n",
    "\n",
    "print(\"Loading weekly sequence data...\")\n",
    "# Load the weekly sequence data\n",
    "input_path = os.path.join(processed_data_dir, 'weekly_sequence_1_parts.parquet')\n",
    "essential_columns = ['PartNumber', 'WeekStart', 'UniquePartInstallationId', 'RunningFleetTotal', \n",
    "                    'RunningAssetTotal', 'RunningLocationTotal', 'RunningOwnerTotal', 'Tail']\n",
    "weekly_sequence_df = pd.read_parquet(input_path, columns=essential_columns)\n",
    "\n",
    "print(\"Weekly sequence loaded\")\n",
    "print(f\"Memory usage of weekly_sequence_df: {weekly_sequence_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"Loading RMA data...\")\n",
    "input_path = os.path.join(data_dir, 'combined_rma.parquet')\n",
    "\n",
    "# First all available columns to see what's actually in the file\n",
    "try:\n",
    "    temp_df = pd.read_parquet(input_path, columns=None)\n",
    "    available_columns = temp_df.columns.tolist()\n",
    "    print(f\"Available columns in RMA file: {available_columns}\")\n",
    "    del temp_df\n",
    "    gc.collect()\n",
    "    \n",
    "    rma_columns = ['rma_number', 'part_number', 'received_date', 'ship_date', 'repair_duration']\n",
    "    \n",
    "    # Add optional columns if they exist\n",
    "    if 'fault_category' in available_columns:\n",
    "        rma_columns.append('fault_category')\n",
    "    elif 'fault_code' in available_columns:\n",
    "        rma_columns.append('fault_code')  # Alternative column\n",
    "        \n",
    "    if 'repair_reason_category' in available_columns:\n",
    "        rma_columns.append('repair_reason_category')\n",
    "    elif 'return_reason_category' in available_columns:\n",
    "        rma_columns.append('return_reason_category')  # Alternative column\n",
    "        \n",
    "    if 'received_year' in available_columns:\n",
    "        rma_columns.append('received_year')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error reading all columns: {str(e)}\")\n",
    "    rma_columns = ['rma_number', 'part_number', 'received_date', 'ship_date', 'repair_duration']\n",
    "\n",
    "combined_rma_df = pd.read_parquet(input_path, columns=rma_columns)\n",
    "\n",
    "# Fix data types issues\n",
    "for col in combined_rma_df.select_dtypes(include=['float64']).columns:\n",
    "    combined_rma_df[col] = pd.to_numeric(combined_rma_df[col], downcast='float')\n",
    "for col in combined_rma_df.select_dtypes(include=['int64']).columns:\n",
    "    combined_rma_df[col] = pd.to_numeric(combined_rma_df[col], downcast='integer')\n",
    "\n",
    "# Use categories for string columns\n",
    "for col in combined_rma_df.select_dtypes(include=['object']).columns:\n",
    "    if combined_rma_df[col].nunique() < len(combined_rma_df) * 0.5:\n",
    "        combined_rma_df[col] = combined_rma_df[col].astype('category')\n",
    "\n",
    "# Check for missing values in critical columns\n",
    "missing_values = combined_rma_df['received_date'].isna().sum()\n",
    "print(f\"Missing received_date values: {missing_values} ({missing_values/len(combined_rma_df)*100:.2f}%)\")\n",
    "\n",
    "# Clean the RMA data\n",
    "print(\"Cleaning RMA data...\")\n",
    "combined_rma_df = combined_rma_df.dropna(subset=['received_date'])\n",
    "\n",
    "# Check for duplicates\n",
    "if 'received_year' not in combined_rma_df.columns:\n",
    "    combined_rma_df['received_year'] = combined_rma_df['received_date'].dt.year\n",
    "\n",
    "# Remove any date before 2012\n",
    "combined_rma_df = combined_rma_df[combined_rma_df['received_date'] >= '2012-01-01']\n",
    "if 'ship_date' in combined_rma_df.columns:\n",
    "    # Handle null ship dates\n",
    "    combined_rma_df = combined_rma_df[\n",
    "        combined_rma_df['ship_date'].isna() | \n",
    "        (combined_rma_df['ship_date'] >= '2012-01-01')\n",
    "    ]\n",
    "\n",
    "print(f\"Date range: {combined_rma_df['received_date'].min()} to {combined_rma_df['received_date'].max()}\")\n",
    "\n",
    "# Fix timezones\n",
    "for date_col in ['received_date', 'ship_date']:\n",
    "    if date_col in combined_rma_df.columns:\n",
    "        if hasattr(combined_rma_df[date_col].dtype, 'tz') and combined_rma_df[date_col].dtype.tz is not None:\n",
    "            combined_rma_df[date_col] = combined_rma_df[date_col].dt.tz_localize(None)\n",
    "\n",
    "# Rename part_number to match weekly_sequence_df\n",
    "combined_rma_df = combined_rma_df.rename(columns={'part_number': 'PartNumber'})\n",
    "\n",
    "# Process RMA data for weekly sequences\n",
    "print(\"Processing RMA data for weekly sequences...\")\n",
    "min_date = combined_rma_df['received_date'].min()\n",
    "max_date = combined_rma_df['received_date'].max()\n",
    "date_range = pd.date_range(min_date, max_date, freq='W-MON')\n",
    "print(f\"RMA date range: {date_range[0]} to {date_range[-1]}\")\n",
    "\n",
    "# Get unique part numbers in RMA data\n",
    "unique_part_numbers = combined_rma_df['PartNumber'].unique()\n",
    "print(f\"Number of unique part numbers in RMA data: {len(unique_part_numbers)}\")\n",
    "\n",
    "# Process in chunks\n",
    "chunk_size = 1000  # Process 1000 part numbers at a time\n",
    "num_chunks = (len(unique_part_numbers) + chunk_size - 1) // chunk_size\n",
    "all_rma_weekly_data = []\n",
    "\n",
    "print(f\"Processing {len(unique_part_numbers)} part numbers in {num_chunks} chunks...\")\n",
    "\n",
    "for chunk_idx in range(num_chunks):\n",
    "    start_idx = chunk_idx * chunk_size\n",
    "    end_idx = min((chunk_idx + 1) * chunk_size, len(unique_part_numbers))\n",
    "    chunk_part_numbers = unique_part_numbers[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Processing part number chunk {chunk_idx+1}/{num_chunks} ({start_idx} to {end_idx})...\")\n",
    "    \n",
    "    # Filter RMA data to only this chunk's part numbers\n",
    "    chunk_rma_df = combined_rma_df[combined_rma_df['PartNumber'].isin(chunk_part_numbers)]\n",
    "    \n",
    "    # Initialize tracking for this chunk\n",
    "    chunk_rma_weekly_data = []\n",
    "    chunk_running_rma_totals = {}\n",
    "    \n",
    "    # Process each week for these part numbers\n",
    "    for week_start in date_range:\n",
    "        week_end = week_start + timedelta(days=6)\n",
    "        \n",
    "        received_mask = (\n",
    "            (chunk_rma_df['received_date'] >= week_start) &\n",
    "            (chunk_rma_df['received_date'] <= week_end)\n",
    "        )\n",
    "        \n",
    "        shipped_mask = (\n",
    "            (chunk_rma_df['ship_date'].notna()) &\n",
    "            (chunk_rma_df['ship_date'] >= week_start) &\n",
    "            (chunk_rma_df['ship_date'] <= week_end)\n",
    "        )\n",
    "        \n",
    "        # Get counts for part numbers in this chunk for this week\n",
    "        received_parts = chunk_rma_df[received_mask]['PartNumber'].value_counts().to_dict()\n",
    "        shipped_parts = chunk_rma_df[shipped_mask]['PartNumber'].value_counts().to_dict()\n",
    "        \n",
    "        # Update data for each part number in this chunk\n",
    "        for part_num in chunk_part_numbers:\n",
    "            received_count = received_parts.get(part_num, 0)\n",
    "            shipped_count = shipped_parts.get(part_num, 0)\n",
    "            \n",
    "            # Only add to the results if there's activity\n",
    "            if received_count > 0 or shipped_count > 0 or chunk_running_rma_totals.get(part_num, 0) > 0:\n",
    "                in_repair = chunk_running_rma_totals.get(part_num, 0) + received_count - shipped_count\n",
    "                \n",
    "                chunk_rma_weekly_data.append({\n",
    "                    'PartNumber': part_num,\n",
    "                    'WeekStart': week_start,\n",
    "                    'ReceivedCount': received_count,\n",
    "                    'ShippedCount': shipped_count,\n",
    "                    'InRepair': in_repair\n",
    "                })\n",
    "                \n",
    "                # Update running total\n",
    "                chunk_running_rma_totals[part_num] = in_repair\n",
    "    \n",
    "    # Convert chunk results to DataFrame\n",
    "    if chunk_rma_weekly_data:\n",
    "        chunk_df = pd.DataFrame(chunk_rma_weekly_data)\n",
    "        all_rma_weekly_data.append(chunk_df)\n",
    "        \n",
    "        del chunk_rma_weekly_data\n",
    "        del chunk_running_rma_totals\n",
    "        del chunk_rma_df\n",
    "        gc.collect()\n",
    "    \n",
    "    # Report progress\n",
    "    print(f\"Completed part number chunk {chunk_idx+1}/{num_chunks}\")\n",
    "\n",
    "# Combine all chunks\n",
    "print(\"Combining all chunks...\")\n",
    "if all_rma_weekly_data:\n",
    "    rma_weekly_df = pd.concat(all_rma_weekly_data, ignore_index=True)\n",
    "    \n",
    "    del all_rma_weekly_data\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"RMA weekly data created with {len(rma_weekly_df)} rows\")\n",
    "    print(f\"Memory usage of rma_weekly_df: {rma_weekly_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"No RMA data to process.\")\n",
    "    rma_weekly_df = pd.DataFrame(columns=['PartNumber', 'WeekStart', 'ReceivedCount', 'ShippedCount', 'InRepair'])\n",
    "\n",
    "# Merge in chunks\n",
    "print(\"Merging RMA data into weekly sequence...\")\n",
    "\n",
    "# Get unique weeks in the weekly sequence for chunking\n",
    "unique_weeks = weekly_sequence_df['WeekStart'].unique()\n",
    "unique_weeks = np.sort(unique_weeks)\n",
    "\n",
    "# Process in chunks of weeks\n",
    "week_chunk_size = 52  # Process one year at a time\n",
    "num_week_chunks = (len(unique_weeks) + week_chunk_size - 1) // week_chunk_size\n",
    "all_merged_chunks = []\n",
    "\n",
    "for week_chunk_idx in range(num_week_chunks):\n",
    "    start_idx = week_chunk_idx * week_chunk_size\n",
    "    end_idx = min((week_chunk_idx + 1) * week_chunk_size, len(unique_weeks))\n",
    "    chunk_weeks = unique_weeks[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Processing week chunk {week_chunk_idx+1}/{num_week_chunks} ({start_idx} to {end_idx})...\")\n",
    "    \n",
    "    # Filter weekly sequence to only this chunk's weeks\n",
    "    sequence_chunk = weekly_sequence_df[weekly_sequence_df['WeekStart'].isin(chunk_weeks)].copy()\n",
    "    rma_chunk = rma_weekly_df[rma_weekly_df['WeekStart'].isin(chunk_weeks)]\n",
    "    \n",
    "    # Merge this chunk\n",
    "    if not rma_chunk.empty:\n",
    "        merged_chunk = sequence_chunk.merge(\n",
    "            rma_chunk,\n",
    "            how='left',\n",
    "            on=['WeekStart', 'PartNumber']\n",
    "        )\n",
    "        \n",
    "        # Fill missing values with 0\n",
    "        for col in ['ReceivedCount', 'ShippedCount', 'InRepair']:\n",
    "            if col in merged_chunk.columns:\n",
    "                # Only fill 0s for dates between min_date and max_date\n",
    "                date_mask = (merged_chunk['WeekStart'] >= min_date) & (merged_chunk['WeekStart'] <= max_date)\n",
    "                merged_chunk.loc[date_mask, col] = merged_chunk.loc[date_mask, col].fillna(0)\n",
    "    else:\n",
    "        merged_chunk = sequence_chunk.copy()\n",
    "        for col in ['ReceivedCount', 'ShippedCount', 'InRepair']:\n",
    "            merged_chunk[col] = np.nan\n",
    "            # Only fill 0s for dates between min_date and max_date\n",
    "            date_mask = (merged_chunk['WeekStart'] >= min_date) & (merged_chunk['WeekStart'] <= max_date)\n",
    "            merged_chunk.loc[date_mask, col] = 0\n",
    "    \n",
    "    all_merged_chunks.append(merged_chunk)\n",
    "    \n",
    "    del sequence_chunk\n",
    "    del rma_chunk\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Completed week chunk {week_chunk_idx+1}/{num_week_chunks}\")\n",
    "\n",
    "# Combine all merged chunks\n",
    "print(\"Combining all merged chunks...\")\n",
    "rma_sequence_df = pd.concat(all_merged_chunks, ignore_index=True)\n",
    "\n",
    "del all_merged_chunks\n",
    "del weekly_sequence_df\n",
    "del rma_weekly_df\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Final merged data created with {len(rma_sequence_df)} rows\")\n",
    "print(f\"Memory usage of final data: {rma_sequence_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Save in chunks\n",
    "rows_per_save_chunk = 1000000\n",
    "num_save_chunks = (len(rma_sequence_df) + rows_per_save_chunk - 1) // rows_per_save_chunk\n",
    "\n",
    "if num_save_chunks <= 1:\n",
    "    print(\"Saving merged data...\")\n",
    "    output_path = os.path.join(processed_data_dir, 'weekly_sequence_2_rma.parquet')\n",
    "    rma_sequence_df.to_parquet(output_path)\n",
    "    print(\"Data saved!\")\n",
    "else:\n",
    "    # Save in multiple chunks and then recombine\n",
    "    print(f\"Saving data in {num_save_chunks} chunks...\")\n",
    "    temp_file_paths = []\n",
    "    \n",
    "    for i in range(num_save_chunks):\n",
    "        start_idx = i * rows_per_save_chunk\n",
    "        end_idx = min((i + 1) * rows_per_save_chunk, len(rma_sequence_df))\n",
    "        \n",
    "        chunk = rma_sequence_df.iloc[start_idx:end_idx]\n",
    "        temp_path = os.path.join(processed_data_dir, f'weekly_sequence_2_rma_temp_{i}.parquet')\n",
    "        chunk.to_parquet(temp_path)\n",
    "        temp_file_paths.append(temp_path)\n",
    "        \n",
    "        del chunk\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Saved chunk {i+1}/{num_save_chunks}\")\n",
    "    \n",
    "    print(\"All chunks saved!\")\n",
    "    print(f\"Temporary files saved at: {', '.join(temp_file_paths)}\")\n",
    "\n",
    "# Combine the files\n",
    "print(\"Combining saved chunks...\")\n",
    "combined_df = pd.read_parquet(temp_file_paths[0])\n",
    "for i in range(1, len(temp_file_paths)):\n",
    "    next_chunk = pd.read_parquet(temp_file_paths[i])\n",
    "    combined_df = pd.concat([combined_df, next_chunk], ignore_index=True)\n",
    "    del next_chunk\n",
    "    gc.collect()\n",
    "# Save the combined data\n",
    "output_path = os.path.join(processed_data_dir, 'weekly_sequence_2_rma.parquet')\n",
    "combined_df.to_parquet(output_path)\n",
    "\n",
    "# Clean up temporary files  \n",
    "for temp_path in temp_file_paths:\n",
    "    os.remove(temp_path)\n",
    "print(\"Data saved and temporary files cleaned up!\")\n",
    "print(\"All processing completed!\")\n",
    "print(f\"Final data saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import re\n",
    "\n",
    "# Define directories\n",
    "processed_data_dir = os.path.join('private', 'data', 'processed')\n",
    "data_dir = os.path.join('private', 'data', 'transformed')\n",
    "\n",
    "print(\"Loading weekly sequence data...\")\n",
    "# Load the weekly sequence data efficiently\n",
    "input_path = os.path.join(processed_data_dir, 'weekly_sequence_2_rma.parquet')\n",
    "\n",
    "essential_columns = ['PartNumber', 'WeekStart', 'Tail', 'ReceivedCount', 'ShippedCount', 'InRepair']\n",
    "rma_sequence_df = pd.read_parquet(input_path, columns=essential_columns)\n",
    "\n",
    "print(\"Weekly sequence loaded\")\n",
    "\n",
    "# Standardize tail numbers function - do this early to avoid duplicate computation\n",
    "def standardize_tail(tail):\n",
    "    if not isinstance(tail, str):\n",
    "        return tail\n",
    "    # Remove all non-alphanumeric chars\n",
    "    tail = re.sub(r'[^A-Za-z0-9]', '', tail)\n",
    "    return tail.upper()\n",
    "\n",
    "# Apply standardization\n",
    "rma_sequence_df['StandardAsset'] = rma_sequence_df['Tail'].apply(\n",
    "    lambda x: standardize_tail(x) if pd.notna(x) else x\n",
    ")\n",
    "\n",
    "# Load the flight data with memory optimizations\n",
    "print(\"Loading flight data...\")\n",
    "input_path = os.path.join(data_dir, 'merged_flight_data.parquet')\n",
    "# Only load columns we need\n",
    "flight_columns = ['asset_id', 'FlightStartTime', 'FlightEndTime', 'FlightDuration',\n",
    "                  'RawResets', 'TotalPassengers', 'BusinessClass', 'EconomyClass',\n",
    "                  'SeatResets', 'Airline', 'AircraftType']\n",
    "merged_flight_data_df = pd.read_parquet(input_path, columns=flight_columns)\n",
    "\n",
    "# Optimize memory usage\n",
    "for col in merged_flight_data_df.select_dtypes(include=['float64']).columns:\n",
    "    merged_flight_data_df[col] = pd.to_numeric(merged_flight_data_df[col], downcast='float')\n",
    "for col in merged_flight_data_df.select_dtypes(include=['int64']).columns:\n",
    "    merged_flight_data_df[col] = pd.to_numeric(merged_flight_data_df[col], downcast='integer')\n",
    "\n",
    "# Use categories for string columns with low cardinality\n",
    "string_cols = ['asset_id', 'Airline', 'AircraftType']\n",
    "for col in string_cols:\n",
    "    if col in merged_flight_data_df.columns:\n",
    "        merged_flight_data_df[col] = merged_flight_data_df[col].astype('category')\n",
    "\n",
    "print(f\"Memory usage of merged_flight_data_df: {merged_flight_data_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(merged_flight_data_df.head())\n",
    "\n",
    "# Clean and filter the flight data\n",
    "print(\"Cleaning flight data...\")\n",
    "# Filter out future dates\n",
    "current_date = datetime.now()\n",
    "merged_flight_data_df = merged_flight_data_df[merged_flight_data_df['FlightStartTime'] <= current_date]\n",
    "\n",
    "# Remove flights with unrealistic durations\n",
    "merged_flight_data_df = merged_flight_data_df[\n",
    "    (merged_flight_data_df['FlightDuration'] >= timedelta(minutes=30)) &\n",
    "    (merged_flight_data_df['FlightDuration'] <= timedelta(hours=24))\n",
    "]\n",
    "\n",
    "print(f\"Date range: {merged_flight_data_df['FlightStartTime'].min()} to {merged_flight_data_df['FlightEndTime'].max()}\")\n",
    "print(f\"After filtering: {len(merged_flight_data_df)} rows\")\n",
    "\n",
    "# Standardize tail numbers in flight data\n",
    "merged_flight_data_df['StandardAsset'] = merged_flight_data_df['asset_id'].apply(\n",
    "    lambda x: standardize_tail(x) if pd.notna(x) else x\n",
    ")\n",
    "\n",
    "# Create WeekStart column\n",
    "merged_flight_data_df['WeekStart'] = pd.to_datetime(\n",
    "    merged_flight_data_df['FlightStartTime']\n",
    ").dt.to_period('W-MON').dt.start_time\n",
    "\n",
    "# Find overlapping tails\n",
    "flight_unique_tails = set(merged_flight_data_df['StandardAsset'].dropna().unique())\n",
    "sequence_unique_tails = set(rma_sequence_df['StandardAsset'].dropna().unique())\n",
    "overlap_tails = flight_unique_tails.intersection(sequence_unique_tails)\n",
    "\n",
    "print(f\"Unique tails in flight data: {len(flight_unique_tails)}\")\n",
    "print(f\"Unique tails in sequence data: {len(sequence_unique_tails)}\")\n",
    "print(f\"Overlapping unique tails: {len(overlap_tails)}\")\n",
    "if len(overlap_tails) > 0:\n",
    "    print(f\"Sample overlapping tails: {list(overlap_tails)[:5]}\")\n",
    "\n",
    "# Filter flight data to only overlapping tails before aggregation\n",
    "filtered_flight_data = merged_flight_data_df[merged_flight_data_df['StandardAsset'].isin(overlap_tails)]\n",
    "print(f\"Filtered flight data from {len(merged_flight_data_df)} to {len(filtered_flight_data)} rows\")\n",
    "\n",
    "del merged_flight_data_df\n",
    "gc.collect()\n",
    "\n",
    "# Process in chunks\n",
    "tail_list = list(overlap_tails)\n",
    "tail_chunk_size = 50  # Process 50 tails at a time\n",
    "num_tail_chunks = (len(tail_list) + tail_chunk_size - 1) // tail_chunk_size\n",
    "all_flight_weekly_data = []\n",
    "\n",
    "print(f\"Processing {len(tail_list)} tails in {num_tail_chunks} chunks...\")\n",
    "\n",
    "for chunk_idx in range(num_tail_chunks):\n",
    "    start_idx = chunk_idx * tail_chunk_size\n",
    "    end_idx = min((chunk_idx + 1) * tail_chunk_size, len(tail_list))\n",
    "    chunk_tails = tail_list[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Processing tail chunk {chunk_idx+1}/{num_tail_chunks} ({start_idx} to {end_idx})...\")\n",
    "    \n",
    "    # Filter data for this chunk of tails\n",
    "    chunk_data = filtered_flight_data[filtered_flight_data['StandardAsset'].isin(chunk_tails)]\n",
    "    \n",
    "    # Aggregate flight data by tail and week\n",
    "    chunk_weekly = chunk_data.groupby(['StandardAsset', 'WeekStart']).agg({\n",
    "        'RawResets': lambda x: x.sum() if not x.isna().all() else None,\n",
    "        'TotalPassengers': lambda x: x.fillna(0).sum(),\n",
    "        'BusinessClass': lambda x: x.fillna(0).sum(),\n",
    "        'EconomyClass': lambda x: x.fillna(0).sum(),\n",
    "        'FlightDuration': lambda x: x.dropna().dt.total_seconds().sum() / 3600,\n",
    "        'SeatResets': lambda x: x.fillna(0).sum(),\n",
    "        'Airline': lambda x: x.dropna().iloc[0] if not x.dropna().empty else None,\n",
    "        'AircraftType': lambda x: x.dropna().iloc[0] if not x.dropna().empty else None\n",
    "    }).reset_index()\n",
    "    \n",
    "    all_flight_weekly_data.append(chunk_weekly)\n",
    "    \n",
    "    # Clean up\n",
    "    del chunk_data\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Completed tail chunk {chunk_idx+1}/{num_tail_chunks}\")\n",
    "\n",
    "# Combine all chunks\n",
    "print(\"Combining all chunks...\")\n",
    "if all_flight_weekly_data:\n",
    "    flight_weekly_data = pd.concat(all_flight_weekly_data, ignore_index=True)\n",
    "    del all_flight_weekly_data\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Weekly flight data created:\")\n",
    "    print(flight_weekly_data.head())\n",
    "    \n",
    "    spot_check = flight_weekly_data[\n",
    "        (flight_weekly_data['WeekStart'] >= '2023-01-01') & \n",
    "        (flight_weekly_data['WeekStart'] <= '2023-01-07')\n",
    "    ]\n",
    "    if not spot_check.empty:\n",
    "        print(spot_check.head())\n",
    "else:\n",
    "    print(\"No flight data to process\")\n",
    "    flight_weekly_data = pd.DataFrame(columns=['StandardAsset', 'WeekStart', 'RawResets', \n",
    "                                              'TotalPassengers', 'BusinessClass', 'EconomyClass',\n",
    "                                              'FlightDuration', 'SeatResets', 'Airline', 'AircraftType'])\n",
    "\n",
    "flight_lookup = {}\n",
    "for _, row in flight_weekly_data.iterrows():\n",
    "    asset = row['StandardAsset']\n",
    "    week = row['WeekStart']\n",
    "    \n",
    "    if asset not in flight_lookup:\n",
    "        flight_lookup[asset] = {}\n",
    "    \n",
    "    flight_lookup[asset][week] = {\n",
    "        'RawResets': row['RawResets'],\n",
    "        'TotalPassengers': row['TotalPassengers'],\n",
    "        'BusinessClass': row['BusinessClass'],\n",
    "        'EconomyClass': row['EconomyClass'],\n",
    "        'FlightDuration': row['FlightDuration'],\n",
    "        'SeatResets': row['SeatResets'],\n",
    "        'Airline': row['Airline'],\n",
    "        'AircraftType': row['AircraftType']\n",
    "    }\n",
    "\n",
    "del flight_weekly_data\n",
    "gc.collect()\n",
    "\n",
    "# Optimized function to find closest flight week\n",
    "def find_closest_flight_week(row, max_weeks_diff=52):\n",
    "    asset = row['StandardAsset']\n",
    "    week = row['WeekStart']\n",
    "    \n",
    "    # Skip if tail is not in flight data or is null\n",
    "    if pd.isna(tail) or tail not in flight_lookup:\n",
    "        return [None] * 10  # Return list of None values for all expected columns\n",
    "    \n",
    "    # Get all weeks for this tail\n",
    "    weeks_data = flight_lookup[tail]\n",
    "    if not weeks_data:\n",
    "        return [None] * 10\n",
    "    \n",
    "    # Calculate time difference\n",
    "    week_diffs = []\n",
    "    for w in weeks_data:\n",
    "        days_diff = abs((w - week).days)\n",
    "        weeks_diff = days_diff / 7\n",
    "        if weeks_diff <= max_weeks_diff: \n",
    "            week_diffs.append((weeks_diff, w))\n",
    "    \n",
    "    if not week_diffs:\n",
    "        return [None] * 10\n",
    "    \n",
    "    # Find closest week\n",
    "    closest_diff, closest_week = min(week_diffs, key=lambda x: x[0])\n",
    "    \n",
    "    # Get the flight data for the closest week\n",
    "    flight_data = weeks_data[closest_week]\n",
    "    \n",
    "    return [\n",
    "        flight_data['RawResets'],\n",
    "        flight_data['TotalPassengers'],\n",
    "        flight_data['BusinessClass'],\n",
    "        flight_data['EconomyClass'],\n",
    "        flight_data['FlightDuration'],\n",
    "        flight_data['SeatResets'],\n",
    "        flight_data['Airline'],\n",
    "        flight_data['AircraftType'],\n",
    "        closest_diff,\n",
    "        closest_week\n",
    "    ]\n",
    "\n",
    "# Filter RMA sequence data to overlapping tails\n",
    "rma_filtered = rma_sequence_df[rma_sequence_df['StandardAsset'].isin(overlap_tails)].copy()\n",
    "print(f\"Filtered from {len(rma_sequence_df)} to {len(rma_filtered)} rows with overlapping tails\")\n",
    "\n",
    "# Process in smaller chunks\n",
    "chunk_size = 5000  # Smaller chunks to reduce memory pressure\n",
    "num_chunks = (len(rma_filtered) + chunk_size - 1) // chunk_size\n",
    "\n",
    "# Preallocate arrays for more efficient result storage\n",
    "result_columns = ['RawResets', 'TotalPassengers', 'BusinessClass', 'EconomyClass', \n",
    "                 'FlightDuration', 'SeatResets', 'Airline', 'AircraftType', \n",
    "                 'week_diff', 'flight_WeekStart']\n",
    "results = {col: np.empty(len(rma_filtered), dtype=object) for col in result_columns}\n",
    "\n",
    "print(f\"Processing {len(rma_filtered)} rows in {num_chunks} chunks...\")\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, len(rma_filtered))\n",
    "    chunk = rma_filtered.iloc[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Processing chunk {i+1}/{num_chunks} ({start_idx} to {end_idx})...\")\n",
    "    \n",
    "    # Process each row in the chunk\n",
    "    for j, (_, row) in enumerate(chunk.iterrows()):\n",
    "        result_values = find_closest_flight_week(row)\n",
    "        \n",
    "        for col_idx, col in enumerate(result_columns):\n",
    "            results[col][start_idx + j] = result_values[col_idx]\n",
    "    \n",
    "    # Print progress\n",
    "    if (i+1) % 5 == 0 or i+1 == num_chunks:\n",
    "        print(f\"Completed {i+1}/{num_chunks} chunks ({(i+1)/num_chunks*100:.1f}%)\")\n",
    "        gc.collect()\n",
    "\n",
    "# Add results to rma_filtered\n",
    "for col in result_columns:\n",
    "    rma_filtered[col] = results[col]\n",
    "\n",
    "del results\n",
    "gc.collect()\n",
    "\n",
    "# Prepare remaining data with correct data types\n",
    "print(\"Adding flight columns to remaining flight data...\")\n",
    "rma_remaining = rma_sequence_df[~rma_sequence_df['StandardAsset'].isin(overlap_tails)].copy()\n",
    "\n",
    "for col in result_columns:\n",
    "    if col == 'flight_WeekStart':\n",
    "        rma_remaining[col] = pd.Series(dtype='datetime64[ns]')\n",
    "    elif col == 'Airline' or col == 'AircraftType':\n",
    "        rma_remaining[col] = pd.Series(dtype='object')\n",
    "    else:\n",
    "        rma_remaining[col] = pd.Series(dtype='float64')\n",
    "\n",
    "print(\"Saving data in separate files...\")\n",
    "filtered_path = os.path.join(processed_data_dir, 'flight_sequence_filtered.parquet')\n",
    "remaining_path = os.path.join(processed_data_dir, 'flight_sequence_remaining.parquet')\n",
    "\n",
    "rma_filtered.to_parquet(filtered_path)\n",
    "rma_remaining.to_parquet(remaining_path)\n",
    "\n",
    "del rma_filtered\n",
    "del rma_remaining\n",
    "del rma_sequence_df\n",
    "gc.collect()\n",
    "\n",
    "# Final output path\n",
    "output_path = os.path.join(processed_data_dir, 'weekly_sequence_3_flight.parquet')\n",
    "\n",
    "# Define a function to divide files into smaller chunks\n",
    "def process_large_file_in_chunks(file_path, chunk_size=100000):\n",
    "    try:\n",
    "        file_info = pd.read_parquet(file_path, columns=[])\n",
    "        total_rows = len(file_info)\n",
    "        del file_info\n",
    "        gc.collect()\n",
    "        \n",
    "        for i in range(0, total_rows, chunk_size):\n",
    "            end_row = min(i + chunk_size, total_rows)\n",
    "            print(f\"  Reading rows {i} to {end_row} of {total_rows}...\")\n",
    "            chunk = pd.read_parquet(file_path, rows=slice(i, end_row))\n",
    "            yield chunk\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Chunking failed, reading entire file: {str(e)}\")\n",
    "        yield pd.read_parquet(file_path)\n",
    "\n",
    "try:\n",
    "    print(f\"Processing filtered file: {filtered_path}\")\n",
    "    first_chunk = True\n",
    "    for chunk in process_large_file_in_chunks(filtered_path):\n",
    "        if first_chunk:\n",
    "            print(f\"  Writing first chunk to output...\")\n",
    "            chunk.to_parquet(output_path)\n",
    "            first_chunk = False\n",
    "        else:\n",
    "            print(f\"  Appending chunk to output...\")\n",
    "            chunk.to_parquet(output_path, append=True)\n",
    "        \n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    # Append the remaining data\n",
    "    print(f\"Processing remaining file: {remaining_path}\")\n",
    "    for chunk in process_large_file_in_chunks(remaining_path):\n",
    "        print(f\"  Appending chunk to output...\")\n",
    "        # Ensure column data types match\n",
    "        for col in chunk.columns:\n",
    "            if col in result_columns:\n",
    "                try:\n",
    "                    sample = pd.read_parquet(output_path, rows=1)\n",
    "                    if col in sample.columns:\n",
    "                        chunk[col] = chunk[col].astype(sample[col].dtype)\n",
    "                    del sample\n",
    "                except:\n",
    "                    pass \n",
    "                \n",
    "        chunk.to_parquet(output_path, append=True)\n",
    "        \n",
    "        del chunk\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"All files combined successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in pandas approach: {str(e)}\")\n",
    "    print(\"Falling back to simplest approach...\")\n",
    "    \n",
    "    try:\n",
    "        filtered_data = pd.read_parquet(filtered_path)\n",
    "        remaining_data = pd.read_parquet(remaining_path)\n",
    "        combined_data = pd.concat([filtered_data, remaining_data], ignore_index=True)\n",
    "        combined_data.to_parquet(output_path)\n",
    "        \n",
    "        del filtered_data\n",
    "        del remaining_data\n",
    "        del combined_data\n",
    "        gc.collect()\n",
    "                \n",
    "    except Exception as e:        \n",
    "        print(f\"Files saved separately at {filtered_path} and {remaining_path}\")        \n",
    "\n",
    "try:\n",
    "    os.remove(filtered_path)\n",
    "    os.remove(remaining_path)\n",
    "    print(\"Temporary files removed\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not remove temporary files: {str(e)}\")\n",
    "\n",
    "# Validate final output\n",
    "try:\n",
    "    sample = pd.read_parquet(output_path, rows=5)\n",
    "    print(\"\\nSample of combined data:\")\n",
    "    print(sample[['StandardAsset', 'WeekStart', 'ReceivedCount', 'ShippedCount', 'InRepair']].head())\n",
    "    \n",
    "    # Count rows with flight data\n",
    "    stats = pd.read_parquet(output_path, columns=['RawResets'])\n",
    "    matched_rows = stats['RawResets'].notna().sum()\n",
    "    total_rows = len(stats)\n",
    "    print(f\"\\nRows with flight data: {matched_rows} out of {total_rows} ({matched_rows/total_rows*100:.2f}%)\")\n",
    "    \n",
    "    del sample\n",
    "    del stats\n",
    "    gc.collect()\n",
    "except Exception as e:\n",
    "    print(f\"Validation failed: {str(e)}\")\n",
    "\n",
    "print(\"Data saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define paths\n",
    "processed_data_dir = os.path.join('private', 'data', 'processed')\n",
    "\n",
    "print(\"Loading data...\")\n",
    "input_path = os.path.join(processed_data_dir, 'weekly_sequence_3_flight.parquet')\n",
    "flight_sequence_df = pd.read_parquet(input_path)\n",
    "print(\"Weekly sequence loaded\")\n",
    "\n",
    "# Make sure data_dir is defined correctly\n",
    "data_dir = os.path.join('private', 'data', 'transformed') \n",
    "input_path = os.path.join(data_dir, 'reliability_data.parquet')\n",
    "mtbf_df = pd.read_parquet(input_path)\n",
    "print(\"reliability_data data loaded\")\n",
    "\n",
    "for df in [flight_sequence_df, mtbf_df]:\n",
    "    for col in df.select_dtypes(include=['float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    for col in df.select_dtypes(include=['int64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "\n",
    "print(mtbf_df.head())\n",
    "duplicates = mtbf_df[mtbf_df.duplicated(subset=['PartNumber', 'Month', 'Airline'], keep=False)]\n",
    "print(f\"Number of duplicate partnumbers: {len(duplicates)}\")\n",
    "\n",
    "# date range\n",
    "min_date = mtbf_df['Month'].min()\n",
    "max_date = mtbf_df['Month'].max()\n",
    "print(f\"Date range: {min_date} to {max_date}\")\n",
    "\n",
    "# Create a weekly date range\n",
    "mtbf_df['WeekStart'] = pd.to_datetime(mtbf_df['Month'].dt.to_period('W-MON').dt.start_time)\n",
    "\n",
    "# Unique values summary\n",
    "print(\"\\nUnique values in essential columns\")\n",
    "for col in ['PartNumber', 'Month', 'Airline', 'target_reliability', 'Failures', 'FlightHours']:\n",
    "    total_values = mtbf_df[col].count()  # Count non-NaN values\n",
    "    unique_values = mtbf_df[col].nunique()\n",
    "    if total_values > 0:\n",
    "        unique_percent = (unique_values / total_values) * 100\n",
    "    else:\n",
    "        unique_percent = 0\n",
    "    print(f\"{col}: {unique_values} unique values ({unique_percent:.2f}%) out of {total_values} non-null values\")\n",
    "\n",
    "# Find overlapping partnumbers\n",
    "flight_unique_parts = set(flight_sequence_df['PartNumber'].dropna().unique())\n",
    "mtbf_unique_parts = set(mtbf_df['PartNumber'].dropna().unique())\n",
    "overlap_parts = flight_unique_parts.intersection(mtbf_unique_parts)\n",
    "\n",
    "print(f\"Unique parts in flight data: {len(flight_unique_parts)}\")\n",
    "print(f\"Unique parts in reliability_data data: {len(mtbf_unique_parts)}\")\n",
    "print(f\"Overlapping unique parts: {len(overlap_parts)}\")\n",
    "\n",
    "mtbf_df_filtered = mtbf_df[mtbf_df['PartNumber'].isin(overlap_parts)].copy()\n",
    "print(f\"Filtered reliability_data data from {len(mtbf_df)} to {len(mtbf_df_filtered)} rows\")\n",
    "\n",
    "# Add the WeekStart column to flight_sequence_df to avoid doing it multiple times\n",
    "if 'WeekStart' not in flight_sequence_df.columns:\n",
    "    flight_sequence_df['WeekStart'] = pd.to_datetime(flight_sequence_df['Date'].dt.to_period('W-MON').dt.start_time)\n",
    "\n",
    "flight_filtered = flight_sequence_df[flight_sequence_df['PartNumber'].isin(overlap_parts)].copy()\n",
    "print(f\"Filtered flight data from {len(flight_sequence_df)} to {len(flight_filtered)} rows\")\n",
    "\n",
    "# Aggregate reliability_data data by PartNumber and WeekStart\n",
    "print(\"Aggregating reliability_data data by PartNumber and WeekStart...\")\n",
    "mtbf_weekly_data = mtbf_df_filtered.groupby(['PartNumber', 'WeekStart']).agg({\n",
    "    'target_reliability': 'first',  # Contractual reliability_data per week\n",
    "    'Failures': 'sum',  # Total failures per week\n",
    "    'Airline': 'first',  # Airline\n",
    "}).reset_index()\n",
    "\n",
    "print(\"Weekly reliability_data data created\")\n",
    "print(mtbf_weekly_data.head())\n",
    "\n",
    "del mtbf_df\n",
    "del mtbf_df_filtered\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "mtbf_lookup = {}\n",
    "for _, row in mtbf_weekly_data.iterrows():\n",
    "    part = row['PartNumber']\n",
    "    week = row['WeekStart']\n",
    "    \n",
    "    if part not in mtbf_lookup:\n",
    "        mtbf_lookup[part] = {}\n",
    "    \n",
    "    mtbf_lookup[part][week] = {\n",
    "        'target_reliability': row['target_reliability'],\n",
    "        'Failures': row['Failures'],\n",
    "        'Airline': row['Airline']\n",
    "    }\n",
    "\n",
    "del mtbf_weekly_data\n",
    "gc.collect()\n",
    "\n",
    "def find_closest_mtbf_week(row, max_weeks_diff=52):\n",
    "    part = row['PartNumber']\n",
    "    week = row['WeekStart']\n",
    "    airline = row.get('Airline')\n",
    "    \n",
    "    # Skip if part is not in reliability_data data\n",
    "    if pd.isna(part) or part not in mtbf_lookup:\n",
    "        return pd.Series([None, None, None, None], \n",
    "                         index=['mtbf_ContractualMTBF', 'mtbf_Failures', 'mtbf_week_diff', 'mtbf_WeekStart'])\n",
    "    \n",
    "    # Find all weeks for this part\n",
    "    weeks_data = mtbf_lookup[part]\n",
    "    if not weeks_data:\n",
    "        return pd.Series([None, None, None, None], \n",
    "                         index=['mtbf_ContractualMTBF', 'mtbf_Failures', 'mtbf_week_diff', 'mtbf_WeekStart'])\n",
    "    \n",
    "    # Filter by airline\n",
    "    if not pd.isna(airline):\n",
    "        matching_weeks = {w: data for w, data in weeks_data.items() \n",
    "                         if data['Airline'] == airline}\n",
    "        if matching_weeks:\n",
    "            weeks_data = matching_weeks\n",
    "    \n",
    "    # Calculate time differences\n",
    "    week_diffs = []\n",
    "    for w in weeks_data:\n",
    "        days_diff = abs((w - week).days)\n",
    "        weeks_diff = days_diff / 7\n",
    "        if weeks_diff <= max_weeks_diff:\n",
    "            week_diffs.append((weeks_diff, w))\n",
    "    \n",
    "    if not week_diffs:\n",
    "        return pd.Series([None, None, None, None], \n",
    "                         index=['mtbf_ContractualMTBF', 'mtbf_Failures', 'mtbf_week_diff', 'mtbf_WeekStart'])\n",
    "    \n",
    "    # Find closest week\n",
    "    closest_diff, closest_week = min(week_diffs, key=lambda x: x[0])\n",
    "    mtbf_data = weeks_data[closest_week]\n",
    "    \n",
    "    return pd.Series([\n",
    "        mtbf_data['target_reliability'],\n",
    "        mtbf_data['Failures'],\n",
    "        closest_diff,\n",
    "        closest_week\n",
    "    ], index=['mtbf_ContractualMTBF', 'mtbf_Failures', 'mtbf_week_diff', 'mtbf_WeekStart'])\n",
    "\n",
    "# Process in chunks\n",
    "chunk_size = 5000 \n",
    "num_chunks = (len(flight_filtered) + chunk_size - 1) // chunk_size\n",
    "\n",
    "mtbf_ContractualMTBF = np.empty(len(flight_filtered), dtype=object)\n",
    "mtbf_Failures = np.empty(len(flight_filtered), dtype=object)\n",
    "mtbf_week_diff = np.empty(len(flight_filtered), dtype=object)\n",
    "mtbf_WeekStart = np.empty(len(flight_filtered), dtype=object)\n",
    "\n",
    "print(f\"Processing {len(flight_filtered)} rows in {num_chunks} chunks...\")\n",
    "for i in range(num_chunks):\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min((i + 1) * chunk_size, len(flight_filtered))\n",
    "    chunk = flight_filtered.iloc[start_idx:end_idx]\n",
    "    \n",
    "    print(f\"Processing chunk {i+1}/{num_chunks} ({start_idx} to {end_idx})...\")\n",
    "    \n",
    "    # Process each row in the chunk\n",
    "    for j, (idx, row) in enumerate(chunk.iterrows()):\n",
    "        result = find_closest_mtbf_week(row)\n",
    "        mtbf_ContractualMTBF[start_idx + j] = result['mtbf_ContractualMTBF']\n",
    "        mtbf_Failures[start_idx + j] = result['mtbf_Failures']\n",
    "        mtbf_week_diff[start_idx + j] = result['mtbf_week_diff']\n",
    "        mtbf_WeekStart[start_idx + j] = result['mtbf_WeekStart']\n",
    "    \n",
    "    # Print progress update and clear memory\n",
    "    if (i+1) % 5 == 0 or i+1 == num_chunks:\n",
    "        print(f\"Completed {i+1}/{num_chunks} chunks ({(i+1)/num_chunks*100:.1f}%)\")\n",
    "        gc.collect()  # Force garbage collection\n",
    "\n",
    "# Add results to flight_filtered\n",
    "flight_filtered['mtbf_ContractualMTBF'] = mtbf_ContractualMTBF\n",
    "flight_filtered['mtbf_Failures'] = mtbf_Failures\n",
    "flight_filtered['mtbf_week_diff'] = mtbf_week_diff\n",
    "flight_filtered['mtbf_WeekStart'] = mtbf_WeekStart\n",
    "\n",
    "# Add columns to flight_remaining\n",
    "print(\"Adding reliability_data columns to remaining flight data...\")\n",
    "flight_remaining = flight_sequence_df[~flight_sequence_df['PartNumber'].isin(overlap_parts)]\n",
    "for col in ['mtbf_ContractualMTBF', 'mtbf_Failures', 'mtbf_week_diff', 'mtbf_WeekStart']:\n",
    "    flight_remaining[col] = None\n",
    "\n",
    "# Combine the data efficiently\n",
    "print(\"Combining datasets...\")\n",
    "indexes_to_update = flight_filtered.index\n",
    "mtbf_sequence_df = flight_sequence_df.copy()\n",
    "\n",
    "# Update only the necessary rows with reliability_data data\n",
    "for col in ['mtbf_ContractualMTBF', 'mtbf_Failures', 'mtbf_week_diff', 'mtbf_WeekStart']:\n",
    "    mtbf_sequence_df.loc[indexes_to_update, col] = flight_filtered[col].values\n",
    "\n",
    "matched_rows = mtbf_sequence_df['mtbf_ContractualMTBF'].notna().sum()\n",
    "print(f\"Rows with reliability_data data after matching: {matched_rows} out of {len(mtbf_sequence_df)} ({matched_rows/len(mtbf_sequence_df)*100:.2f}%)\")\n",
    "\n",
    "# Show distribution of week differences\n",
    "if 'mtbf_week_diff' in mtbf_sequence_df.columns and mtbf_sequence_df['mtbf_week_diff'].notna().any():\n",
    "    week_diff_stats = mtbf_sequence_df['mtbf_week_diff'].describe()\n",
    "    print(\"\\nWeek difference statistics:\")\n",
    "    print(week_diff_stats)\n",
    "    \n",
    "    print(\"\\nDistribution of week differences:\")\n",
    "    week_ranges = [(0, 4), (4, 12), (12, 26), (26, 52)]\n",
    "    for start, end in week_ranges:\n",
    "        count = ((mtbf_sequence_df['mtbf_week_diff'] >= start) & (mtbf_sequence_df['mtbf_week_diff'] < end)).sum()\n",
    "        pct = count / matched_rows * 100 if matched_rows > 0 else 0\n",
    "        print(f\"{start}-{end} weeks: {count} ({pct:.1f}%)\")\n",
    "    \n",
    "    count = (mtbf_sequence_df['mtbf_week_diff'] >= 52).sum()\n",
    "    pct = count / matched_rows * 100 if matched_rows > 0 else 0\n",
    "    print(f\"52+ weeks: {count} ({pct:.1f}%)\")\n",
    "\n",
    "if matched_rows > 0:\n",
    "    sample_matched = mtbf_sequence_df[mtbf_sequence_df['mtbf_ContractualMTBF'].notna()].sample(min(5, matched_rows))\n",
    "    print(sample_matched[['PartNumber', 'WeekStart', 'mtbf_WeekStart', 'mtbf_week_diff', 'mtbf_ContractualMTBF', 'mtbf_Failures']])\n",
    "\n",
    "# Save the results\n",
    "print(\"Saving merged data...\")\n",
    "output_path = os.path.join(processed_data_dir, 'weekly_sequence_4_mtbf.parquet')\n",
    "mtbf_sequence_df.to_parquet(output_path)\n",
    "print(\"Data saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "\n",
    "# Define directories\n",
    "processed_data_dir = os.path.join('private', 'data', 'processed')\n",
    "data_dir = os.path.join('private', 'data', 'transformed')\n",
    "\n",
    "print(\"Loading weekly sequence data...\")\n",
    "input_path = os.path.join(processed_data_dir, 'weekly_sequence_4_mtbf.parquet')\n",
    "\n",
    "needed_columns = [\n",
    "    'PartNumber', 'WeekStart', 'target_reliability', 'EntryIntoServiceDate',\n",
    "    'RunningFleetTotal', 'ReceivedCount', 'ShippedCount', 'InRepair',\n",
    "    'RawResets', 'Failures', 'SeatResets', 'TotalPassengers', 'Tail',\n",
    "    'StandardAsset', 'Airline', 'FlightDuration', 'BusinessClass', 'EconomyClass'\n",
    "]\n",
    "\n",
    "try:\n",
    "    mtbf_sequence_df = pd.read_parquet(input_path, columns=needed_columns)\n",
    "except Exception as e:\n",
    "    file_schema = pd.read_parquet(input_path, columns=None).head(0)\n",
    "    available_columns = file_schema.columns.tolist()    \n",
    "    existing_columns = [col for col in needed_columns if col in available_columns]\n",
    "    mtbf_sequence_df = pd.read_parquet(input_path, columns=existing_columns)\n",
    "\n",
    "print(\"Weekly sequence loaded\")\n",
    "\n",
    "# Create Year column\n",
    "mtbf_sequence_df['Year'] = mtbf_sequence_df['WeekStart'].dt.year\n",
    "\n",
    "for col in mtbf_sequence_df.select_dtypes(include=['float64']).columns:\n",
    "    mtbf_sequence_df[col] = pd.to_numeric(mtbf_sequence_df[col], downcast='float')\n",
    "for col in mtbf_sequence_df.select_dtypes(include=['int64']).columns:\n",
    "    mtbf_sequence_df[col] = pd.to_numeric(mtbf_sequence_df[col], downcast='integer')\n",
    "\n",
    "# Contractual reliability_data should be updated at the existence of a record then carried forward\n",
    "if 'target_reliability' in mtbf_sequence_df.columns:\n",
    "    unique_parts = mtbf_sequence_df['PartNumber'].unique()\n",
    "    chunk_size = 1000  \n",
    "    num_chunks = (len(unique_parts) + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, len(unique_parts))\n",
    "        chunk_parts = unique_parts[start_idx:end_idx]\n",
    "        \n",
    "        part_mask = mtbf_sequence_df['PartNumber'].isin(chunk_parts)\n",
    "        \n",
    "        # Fill this chunk of parts\n",
    "        mtbf_sequence_df.loc[part_mask, 'target_reliability'] = (\n",
    "            mtbf_sequence_df.loc[part_mask]\n",
    "            .groupby('PartNumber')['target_reliability']\n",
    "            .ffill()\n",
    "        )\n",
    "        \n",
    "        # Print progress\n",
    "        if (i+1) % 10 == 0 or i+1 == num_chunks:\n",
    "            print(f\"Completed {i+1}/{num_chunks} chunks ({(i+1)/num_chunks*100:.1f}%)\")\n",
    "        \n",
    "        gc.collect()\n",
    "\n",
    "print(f\"Date range: {mtbf_sequence_df['WeekStart'].min()} to {mtbf_sequence_df['WeekStart'].max()}\")\n",
    "print(f\"Number of unique parts: {mtbf_sequence_df['PartNumber'].nunique()}\")\n",
    "\n",
    "# Define fields for analysis\n",
    "fields = [\n",
    "    'EntryIntoServiceDate',    # Important for equipment part age\n",
    "    'RunningFleetTotal',       # Critical for normalizing failure rates\n",
    "    'ReceivedCount',           # Number of parts received for repair\n",
    "    'ShippedCount',            # Number of parts shipped after repair\n",
    "    'InRepair',                # Number of parts currently in repair\n",
    "    'RawResets',               # System resets - indicator of issues\n",
    "    'Failures',                # Actual failures - primary prediction target\n",
    "    'target_reliability',         # Expected time between failures - benchmark\n",
    "    'SeatResets',              # Passenger-facing issues\n",
    "    'TotalPassengers',         # For normalizing by usage\n",
    "]\n",
    "\n",
    "# Filter to only fields that exist in dataframe\n",
    "fields = [field for field in fields if field in mtbf_sequence_df.columns]\n",
    "print(\"Calculating yearly completeness...\")\n",
    "yearly_completeness = {}\n",
    "unique_years = mtbf_sequence_df['Year'].unique()\n",
    "unique_years.sort()\n",
    "\n",
    "for year in unique_years:\n",
    "    if year > 2026: \n",
    "        continue\n",
    "        \n",
    "    year_data = mtbf_sequence_df[mtbf_sequence_df['Year'] == year]\n",
    "    \n",
    "    field_completeness = {}\n",
    "    for field in fields:\n",
    "        field_completeness[field] = year_data[field].notna().mean()\n",
    "    \n",
    "    yearly_completeness[year] = field_completeness\n",
    "    \n",
    "    del year_data\n",
    "    gc.collect()\n",
    "\n",
    "yearly_completeness_df = pd.DataFrame.from_dict(yearly_completeness, orient='index')\n",
    "yearly_completeness_df = yearly_completeness_df.loc[yearly_completeness_df.index <= 2026]\n",
    "\n",
    "print(\"\\nYearly completeness by field:\")\n",
    "pd.set_option('display.float_format', '{:.2%}'.format)\n",
    "print(yearly_completeness_df)\n",
    "\n",
    "# Find years with good data for training/validation\n",
    "year_completeness = yearly_completeness_df.mean(axis=1) > 0.8  # Average across all fields\n",
    "complete_years = year_completeness[year_completeness].index.tolist()\n",
    "\n",
    "# Find continuous ranges of years\n",
    "year_ranges = []\n",
    "current_range = []\n",
    "\n",
    "for year in sorted(complete_years):\n",
    "    if len(current_range) == 0 or year == current_range[-1] + 1:\n",
    "        current_range.append(year)\n",
    "    else:\n",
    "        if current_range:  \n",
    "            year_ranges.append(current_range)  \n",
    "        current_range = [year]\n",
    "if current_range:  \n",
    "    year_ranges.append(current_range)\n",
    "\n",
    "# Find the longest range \n",
    "if year_ranges: \n",
    "    longest_range = max(year_ranges, key=len)\n",
    "    print(f\"Longest range: {longest_range}\")\n",
    "else:\n",
    "    longest_range = []\n",
    "    print(\"No year ranges found\")\n",
    "\n",
    "# Determine train/validation split with safety checks\n",
    "if longest_range and len(longest_range) > 3: \n",
    "    # Use most recent year with complete failure data for validation\n",
    "    failure_complete_years = [year for year, row in yearly_completeness_df.iterrows() \n",
    "                            if row.get('Failures', 0) > 0.8 and year in longest_range]\n",
    "    \n",
    "    if failure_complete_years:\n",
    "        # Find the most recent year with complete failure data\n",
    "        most_recent_complete = max(failure_complete_years)\n",
    "        validate_years = [most_recent_complete]\n",
    "        train_years = [year for year in longest_range if year < most_recent_complete]\n",
    "    else:\n",
    "        validate_years = [longest_range[-1]]\n",
    "        train_years = longest_range[:-1]\n",
    "elif longest_range:  # Some data but not enough for good split\n",
    "    print(\"Not enough continuous years of data for reliable validation\")\n",
    "    validate_years = [longest_range[-1]]\n",
    "    train_years = longest_range[:-1] if len(longest_range) > 1 else []\n",
    "else:\n",
    "    print(\"No complete years found, using default year range\")\n",
    "    validate_years = []\n",
    "    train_years = []\n",
    "\n",
    "print(f\"Training years: {train_years}\")\n",
    "print(f\"Validation years: {validate_years}\")\n",
    "\n",
    "# Define key fields for correlation analysis\n",
    "key_fields = [\n",
    "    'RunningFleetTotal',   \n",
    "    'ReceivedCount',         \n",
    "    'ShippedCount',          \n",
    "    'InRepair',              \n",
    "    'RawResets',            \n",
    "    'Failures',              \n",
    "    'target_reliability',       \n",
    "    'SeatResets',            \n",
    "    'TotalPassengers'        \n",
    "]\n",
    "\n",
    "# Filter to only fields that exist in our dataframe\n",
    "key_fields = [field for field in key_fields if field in mtbf_sequence_df.columns]\n",
    "\n",
    "# Filter to recent years for correlation analysis\n",
    "print(\"Filtering data for correlation analysis...\")\n",
    "year_filter = (mtbf_sequence_df['Year'] >= 2019) & (mtbf_sequence_df['Year'] <= 2025)\n",
    "training_data_sample = mtbf_sequence_df[year_filter].sample(\n",
    "    min(100000, len(mtbf_sequence_df[year_filter])), \n",
    "    random_state=42\n",
    ")\n",
    "print(f\"Using {len(training_data_sample)} rows for correlation analysis\")\n",
    "\n",
    "# Clean data for correlation\n",
    "print(\"Preparing data for correlation...\")\n",
    "corr_df = training_data_sample[key_fields].copy()\n",
    "\n",
    "# Convert to numeric and handle NAs\n",
    "for col in key_fields:\n",
    "    # First check the column type and any potential issues\n",
    "    print(f\"Column {col}: dtype={corr_df[col].dtype}, na_count={corr_df[col].isna().sum()}\")\n",
    "    # Convert to numeric\n",
    "    corr_df[col] = pd.to_numeric(corr_df[col], errors='coerce')\n",
    "    # Replace NaN values with 0 for correlation calculation\n",
    "    corr_df[col] = corr_df[col].fillna(0)\n",
    "\n",
    "# Calculate correlation\n",
    "try:\n",
    "    key_field_corr = corr_df.corr()\n",
    "    print(\"\\nCorrelation matrix for key fields:\")\n",
    "    print(key_field_corr)\n",
    "    \n",
    "    for field in key_fields:\n",
    "        print(f\"\\nTop correlations with {field}:\")\n",
    "        field_corr = key_field_corr[field].sort_values(ascending=False)\n",
    "        print(field_corr.drop(field).head(5))\n",
    "except Exception as e:\n",
    "    print(f\"Error calculating correlation: {e}\")    \n",
    "    # More aggressive cleaning approach\n",
    "    for col in key_fields:\n",
    "        # Keep only columns that can be converted to float\n",
    "        try:\n",
    "            corr_df[col] = corr_df[col].astype(float)\n",
    "        except:\n",
    "            print(f\"Dropping bad column: {col}\")\n",
    "            corr_df = corr_df.drop(columns=[col])\n",
    "    \n",
    "    # Try correlation again with remaining columns\n",
    "    if len(corr_df.columns) >= 2:\n",
    "        key_field_corr = corr_df.corr()\n",
    "        print(\"\\nCorrelation matrix after cleaning:\")\n",
    "        print(key_field_corr)\n",
    "    else:\n",
    "        print(\"Not enough valid columns for correlation analysis\")\n",
    "\n",
    "# Check for data period coverage by field\n",
    "print(\"\\nData period coverage by field:\")\n",
    "for field in key_fields:\n",
    "    if field in mtbf_sequence_df.columns:\n",
    "        # Sample data to avoid memory issues\n",
    "        sample_size = min(1000000, len(mtbf_sequence_df))\n",
    "        sampled_df = mtbf_sequence_df.sample(sample_size)\n",
    "        \n",
    "        # Get first and last non-null date for each field\n",
    "        field_data = sampled_df[sampled_df[field].notna()]\n",
    "        if len(field_data) > 0:\n",
    "            first_date = field_data['WeekStart'].min()\n",
    "            last_date = field_data['WeekStart'].max()\n",
    "            print(f\"{field}: {len(field_data)} values in sample from {first_date} to {last_date}\")\n",
    "        else:\n",
    "            print(f\"{field}: No valid data in sample\")\n",
    "        \n",
    "        # Clean up\n",
    "        del sampled_df\n",
    "        del field_data\n",
    "        gc.collect()\n",
    "\n",
    "# Check for variance in fields\n",
    "print(\"\\nVariance by field:\")\n",
    "for field in key_fields:\n",
    "    try:\n",
    "        # Sample data to avoid memory issues\n",
    "        sample_size = min(1000000, len(mtbf_sequence_df))\n",
    "        field_sample = mtbf_sequence_df[field].dropna().sample(sample_size)\n",
    "        \n",
    "        if len(field_sample) > 0:\n",
    "            field_sample = pd.to_numeric(field_sample, errors='coerce')\n",
    "            var = field_sample.var()\n",
    "            print(f\"{field} variance (from sample): {var}\")\n",
    "        else:\n",
    "            print(f\"{field}: No valid data for variance calculation\")\n",
    "    except Exception as e:\n",
    "        print(f\"{field} variance calculation error: {e}\")\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types by field:\")\n",
    "for field in key_fields:\n",
    "    if field in mtbf_sequence_df.columns:\n",
    "        dtype = mtbf_sequence_df[field].dtype\n",
    "        print(f\"{field}: {dtype}\")\n",
    "\n",
    "# Save the final sequence data\n",
    "print(\"Saving final sequence data...\")\n",
    "output_path = os.path.join(processed_data_dir, 'weekly_sequence_5_final.parquet')\n",
    "\n",
    "# Save in chunks if the data is large\n",
    "rows_per_chunk = 1000000\n",
    "num_save_chunks = (len(mtbf_sequence_df) + rows_per_chunk - 1) // rows_per_chunk\n",
    "\n",
    "if num_save_chunks <= 1:\n",
    "    # Small enough to save as one file\n",
    "    mtbf_sequence_df.to_parquet(output_path)\n",
    "    print(\"Data saved!\")\n",
    "else:\n",
    "    # Save in multiple chunks\n",
    "    print(f\"Saving data in {num_save_chunks} chunks...\")\n",
    "    temp_paths = []\n",
    "    \n",
    "    for i in range(num_save_chunks):\n",
    "        start_idx = i * rows_per_chunk\n",
    "        end_idx = min((i + 1) * rows_per_chunk, len(mtbf_sequence_df))\n",
    "        \n",
    "        chunk = mtbf_sequence_df.iloc[start_idx:end_idx]\n",
    "        temp_path = os.path.join(processed_data_dir, f'weekly_sequence_5_final_temp_{i}.parquet')\n",
    "        chunk.to_parquet(temp_path)\n",
    "        temp_paths.append(temp_path)\n",
    "        \n",
    "        del chunk\n",
    "        gc.collect()\n",
    "        \n",
    "        print(f\"Saved chunk {i+1}/{num_save_chunks}\")\n",
    "    \n",
    "    print(\"All chunks saved!\")\n",
    "\n",
    "    # Combine chunks into one file if needed\n",
    "    combined_df = pd.concat([pd.read_parquet(path) for path in temp_paths], ignore_index=True)\n",
    "    combined_df.to_parquet(output_path)\n",
    "    print(f\"Combined data saved to {output_path}\")\n",
    "    # Clean up temporary files\n",
    "    for path in temp_paths:\n",
    "        os.remove(path)\n",
    "        print(f\"Removed temporary file: {path}\")\n",
    "    print(\"Temporary files removed\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "title": "Data Extraction Pipeline"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the data for use in RNN\n",
    "\n",
    "**Note:** All company-specific file names, column names, and identifiers have been obfuscated or generalized to protect proprietary information while maintaining the analytical structure of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "# Load each CSV into a DataFrame\n",
    "raw_data_dir = os.path.join('private', 'data', 'raw')\n",
    "\n",
    "# List of file names\n",
    "file_names = [\n",
    "    'partslist.csv',\n",
    "    'merged_rmaorders.csv',\n",
    "    'hist_repair_rma.csv',\n",
    "    'flights.csv',\n",
    "    'flightresets.csv',\n",
    "    'passenger_count.csv',\n",
    "    'mtbf.csv',\n",
    "    'productinfo.csv',\n",
    "]\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "pd.reset_option('display.float_format')\n",
    "\n",
    "# Load each CSV into a DataFrame and store in the dictionary\n",
    "for file_name in file_names:\n",
    "    df_name = file_name.split('.')[0] + '_df'\n",
    "    dataframes[df_name] = pd.read_csv(\n",
    "        os.path.join(raw_data_dir, file_name),\n",
    "        parse_dates=True,  # Try to parse date columns\n",
    "        infer_datetime_format=True,  # Use format inference for dates\n",
    "        low_memory=False  # Avoid mixed type inference warnings\n",
    "    )\n",
    "\n",
    "    # Temp store current dataframe\n",
    "    df = dataframes[df_name]\n",
    "\n",
    "    # Change objects to columns\n",
    "    object_columns = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    if len(object_columns) > 0:\n",
    "        print(f\"\\nConverting object columns to datetimes or strings for {df_name}:\")\n",
    "        for col in object_columns:\n",
    "            # Try converting to datetime\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], errors='raise')\n",
    "            except:\n",
    "                # Convert to string dtype\n",
    "                df[col] = df[col].astype(\"string\")\n",
    "\n",
    "    print(f\"\\nDataframe: {df_name}\")\n",
    "    print(\"\\nColumn Types:\")\n",
    "    type_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in type_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "\n",
    "    datetime_columns = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "    if datetime_columns:\n",
    "        print(\"\\nDatetime Columns:\")\n",
    "        for col in datetime_columns:\n",
    "            print(f\"  - {col}\")\n",
    "\n",
    "# DataFrames\n",
    "inflight_parts_df = dataframes['partslist_df']\n",
    "rma_df = dataframes['merged_rmaorders_df']\n",
    "hist_rma_df = dataframes['hist_repair_rma_df']\n",
    "flight_data_df = dataframes['flights_df']\n",
    "flightresets_df = dataframes['flightresets_df']\n",
    "flightpassengers_df = dataframes['passenger_count_df']\n",
    "mtbf_df = dataframes['mtbf_df']\n",
    "productinfo_df = dataframes['productinfo_df']\n",
    "\n",
    "# Label encode line manager to preserve privacy\n",
    "productinfo_df['product_manager'] = productinfo_df['product_manager'].astype('category').cat.codes\n",
    "\n",
    "## Fix Issues as they are discovered\n",
    "# Determine validity of missing fields \n",
    "actual_missing_tails = inflight_parts_df[\n",
    "    (inflight_parts_df['Tail'].isnull()) & \n",
    "    (inflight_parts_df['status'] != 'Not in Service Yet')\n",
    "]\n",
    "print(f\"inflight_parts_df: Actual missing tails: {len(actual_missing_tails)}\")\n",
    "actual_missing_hist_ship_dates = hist_rma_df[\n",
    "    (hist_rma_df['ReceivedDate'].isnull()) & \n",
    "    (hist_rma_df['ReceivedAtPartner'].isnull())\n",
    "]\n",
    "print(f\"\\nhist_rma_df: Actual missing hist_ship_dates: {len(actual_missing_hist_ship_dates)}\")\n",
    "print(f\" actual hist_ship_dates: {len(actual_missing_hist_ship_dates)} missing ({len(actual_missing_hist_ship_dates)/len(hist_rma_df)*100:.1f}%)\")\n",
    "\n",
    "# Drop 'unused' fields\n",
    "rma_df.drop(['Message'], axis=1, inplace=True)\n",
    "rma_df.drop(['Subject'], axis=1, inplace=True)\n",
    "rma_df.drop(['Progress'], axis=1, inplace=True)\n",
    "rma_df.drop(['Solution'], axis=1, inplace=True)\n",
    "rma_df.drop(['AircraftType'], axis=1, inplace=True)\n",
    "rma_df.drop(['FinalDocRevision'], axis=1, inplace=True)\n",
    "rma_df.drop(['AircraftTailSerialNumber'], axis=1, inplace=True)\n",
    "rma_df.drop(['AircraftTailNumber'], axis=1, inplace=True)\n",
    "# Drop sensitive customer/contact information columns\n",
    "sensitive_columns = ['ContactPersonPhone', 'ContactPersonEmail', 'CustAccount', 'Personnel_Number', 'SvcCallInitiator', 'CustAccountUser', 'CustAccountOwner']\n",
    "for col in sensitive_columns:\n",
    "    if col in rma_df.columns:\n",
    "        rma_df.drop([col], axis=1, inplace=True)\n",
    "rma_missing = rma_df.isnull().sum()\n",
    "if rma_missing.any():\n",
    "    print(\"\\nNew RMA Columns with missing values:\")\n",
    "    for col, count in rma_missing[rma_missing > 0].items():\n",
    "        print(f\"  {col}: {count} missing ({count/len(rma_df)*100:.1f}%)\")\n",
    "\n",
    "hist_rma_df.drop(['ServiceBulletinInfo'], axis=1, inplace=True)\n",
    "hist_rma_df.drop(['ServiceBulletinNumber'], axis=1, inplace=True)\n",
    "hist_rma_df.drop(['ServiceBulletin'], axis=1, inplace=True)\n",
    "hist_rma_df.drop(['AlertCategoryCode'], axis=1, inplace=True)\n",
    "hist_rma_missing = hist_rma_df.isnull().sum()\n",
    "if hist_rma_missing.any():\n",
    "    print(\"\\nNew Hist-RMA Columns with missing values:\")\n",
    "    for col, count in hist_rma_missing[hist_rma_missing > 0].items():\n",
    "        print(f\"  {col}: {count} missing ({count/len(hist_rma_df)*100:.1f}%)\")\n",
    "productinfo_df.drop(['productgroup'], axis=1, inplace=True)\n",
    "productinfo_df.drop(['actualdate'], axis=1, inplace=True)\n",
    "productinfo_df.drop(['notes'], axis=1, inplace=True)\n",
    "productinfo_df.drop(['milestonedate'], axis=1, inplace=True)\n",
    "productinfo_df.drop(['milestonestatus'], axis=1, inplace=True)\n",
    "productinfo_df.drop(['milestone'], axis=1, inplace=True)\n",
    "productinfo_df.drop(['functionalspec'], axis=1, inplace=True) # Generic spec field\n",
    "productinfo_df.drop(['conformitydescription'], axis=1, inplace=True)\n",
    "\n",
    "# Convert missed Datetime fields\n",
    "print(\"Converting Missed Datetime fields\")\n",
    "flight_data_df['FlightStartTime'] = pd.to_datetime(flight_data_df['FlightStartTime'], format='ISO8601', errors='coerce')\n",
    "flight_data_df['FlightEndTime'] = pd.to_datetime(flight_data_df['FlightEndTime'], format='ISO8601', errors='coerce')\n",
    "flightresets_df['FlightStartTime'] = pd.to_datetime(flightresets_df['FlightStartTime'], format='ISO8601',  errors='coerce')\n",
    "flightresets_df['FlightEndTime'] = pd.to_datetime(flightresets_df['FlightEndTime'], format='ISO8601',  errors='coerce')\n",
    "hist_rma_df['ReceivedDate'] = pd.to_datetime(hist_rma_df['ReceivedDate'], errors='coerce')\n",
    "hist_rma_df['ShipDate'] = pd.to_datetime(hist_rma_df['ShipDate'], errors='coerce')\n",
    "# Get rid of insert/update dates\n",
    "hist_rma_df.drop(['InsertDate'], axis=1, inplace=True)\n",
    "flight_data_df.drop(['FileCreatedTime'], axis=1, inplace=True)\n",
    "flight_data_df.drop(['InsertDate'], axis=1, inplace=True)\n",
    "flightpassengers_df.drop(['InsertDate'], axis=1, inplace=True)\n",
    "mtbf_df.drop(['InsertDate'], axis=1, inplace=True)\n",
    "mtbf_df.drop(['UpdateDate'], axis=1, inplace=True)\n",
    "\n",
    "# I want to see earliest dates and latest dates in all the dfs\n",
    "# I dont want placeholder dates, replace will null\n",
    "print(\"Earliest and Latest Dates\")\n",
    "for df_name, df in dataframes.items():\n",
    "    min_valid_date = pd.Timestamp('2012-01-01')\n",
    "    print(f\"\\n{df_name}\")\n",
    "    \n",
    "    # Get all datetime columns regardless of timezone\n",
    "    datetime_cols = [col for col in df.columns if pd.api.types.is_datetime64_dtype(df[col])]\n",
    "    \n",
    "    for col in datetime_cols:\n",
    "        print(f\"  {col} (before): {df[col].min()} - {df[col].max()}\")\n",
    "        # Convert timezone-aware columns to regular\n",
    "        if hasattr(df[col].dtype, 'tz') and df[col].dtype.tz is not None:\n",
    "            df[col] = df[col].dt.tz_localize(None)\n",
    "            print(f\"Removed timezone from {col}\")\n",
    "        # Replace dates before min_valid_date with NaT\n",
    "        invalid_count = (df[col] < min_valid_date).sum()\n",
    "        df.loc[df[col] < min_valid_date, col] = pd.NaT\n",
    "        print(f\"  {col} (after): {df[col].min()} - {df[col].max()} ({invalid_count} values replaced with NaT)\")\n",
    "\n",
    "# I need to find out how to merge data\n",
    "# inflight_parts_df gets Left joined to productinfo_df on PartNumber\n",
    "merged_inflight_parts_df = inflight_parts_df.merge(productinfo_df, how='left', left_on='PartNumber', right_on='part_number')\n",
    "\n",
    "# Determine new column names\n",
    "rma_standardized = rma_df.copy()\n",
    "rma_standardized['source'] = 'current_rma'\n",
    "rma_standardized['rma_number'] = rma_standardized['ServiceRequestId']\n",
    "rma_standardized['part_number'] = rma_standardized['ItemId']\n",
    "rma_standardized['serial_number'] = rma_standardized['SerialId']\n",
    "rma_standardized['status'] = rma_standardized['RepairStatus']\n",
    "rma_standardized['received_date'] = rma_standardized['UnitReceivedDate']  \n",
    "rma_standardized['ship_date'] = rma_standardized['ActualShipDate'] \n",
    "rma_standardized['warranty_end_date'] = rma_standardized['WarrantyEndDate']  \n",
    "rma_standardized['customer'] = 'ANONYMIZED_CUSTOMER'  # Customer info anonymized\n",
    "rma_standardized['part_description'] = rma_standardized['Description']\n",
    "rma_standardized['fault_code'] = rma_standardized['ComplaintId']\n",
    "rma_standardized['lru_name'] = None  # No direct match in current RMA\n",
    "\n",
    "hist_standardized = hist_rma_df.copy()\n",
    "hist_standardized['source'] = 'historical_rma'\n",
    "hist_standardized['rma_number'] = hist_standardized['RMA']\n",
    "hist_standardized['part_number'] = hist_standardized['PN']\n",
    "hist_standardized['serial_number'] = hist_standardized['SN']\n",
    "hist_standardized['status'] = hist_standardized['StatusDescription']\n",
    "hist_standardized['received_date'] = hist_standardized['ReceivedDate']  \n",
    "hist_standardized['ship_date'] = hist_standardized['ShipDate']  \n",
    "hist_standardized['customer'] = 'ANONYMIZED_CUSTOMER'  # Customer info anonymized\n",
    "hist_standardized['part_description'] = hist_standardized['PartDescription']\n",
    "hist_standardized['fault_code'] = hist_standardized['FaultCode']\n",
    "hist_standardized['lru_name'] = hist_standardized['LRUName']\n",
    "\n",
    "hist_standardized['workshop_location'] = None  # No equivalent in historical data\n",
    "hist_standardized['flight_hours'] = None  # No equivalent in historical data\n",
    "hist_standardized['return_reason'] = hist_standardized['FaultCode'] \n",
    "hist_standardized['warranty_end_date'] = None  # No equivalent in historical data\n",
    "\n",
    "rma_standardized['workshop_location'] = rma_standardized['RepairLocation']\n",
    "rma_standardized['flight_hours'] = rma_standardized['FlightHours']\n",
    "rma_standardized['return_reason'] = rma_standardized['ReturnReason']\n",
    "rma_standardized['warranty_end_date'] =  rma_standardized['WarrantyEndDate']\n",
    "\n",
    "final_columns = [\n",
    "    'source', 'rma_number', 'part_number', 'serial_number', 'customer',\n",
    "    'status', 'received_date', 'ship_date', 'part_description', \n",
    "    'fault_code', 'lru_name', 'workshop_location', 'flight_hours',\n",
    "    'return_reason', 'warranty_end_date'\n",
    "]\n",
    "\n",
    "rma_for_concat = rma_standardized[final_columns]\n",
    "hist_for_concat = hist_standardized[final_columns]\n",
    "\n",
    "combined_rma_df = pd.concat([rma_for_concat, hist_for_concat], ignore_index=True)\n",
    "\n",
    "print(\"flight_data_df columns:\", flight_data_df.columns.tolist())\n",
    "print(\"flightresets_df columns:\", flightresets_df.columns.tolist())\n",
    "print('flightresets_df', flightresets_df[flightresets_df['FlightStartTime'] > '2023-01-01'].head())\n",
    "\n",
    "merged_flight_data_df = flight_data_df.merge(flightresets_df, how='left', on='FlightID')\n",
    "\n",
    "# Fix float fields that should be integers\n",
    "merged_flight_data_df['RawResets'] = pd.array(\n",
    "    merged_flight_data_df['RawResets'].to_numpy(), \n",
    "    dtype=pd.Int64Dtype()\n",
    ")\n",
    "\n",
    "print(\"merged_flight_data_df columns:\", merged_flight_data_df.columns.tolist())\n",
    "\n",
    "print(\"Unique RawResets values in flightresets_df:\")\n",
    "print(flightresets_df['RawResets'].unique())\n",
    "# Print all unique vvalues of RawResets in merged_flight_data_df\n",
    "print(\"Unique RawResets values in merged_flight_data_df:\")\n",
    "print(merged_flight_data_df['RawResets'].unique())\n",
    "\n",
    "# It appears that this merge is not working, \n",
    "# check for common flight ids between the two dataframes\n",
    "common_flight_ids = set(flight_data_df['FlightID']).intersection(set(flightresets_df['FlightID']))\n",
    "print(f\"Common Flight IDs: {len(common_flight_ids)} of {len(flight_data_df)} and {len(flightresets_df)}\")\n",
    "\n",
    "merged_flight_data_df = merged_flight_data_df.merge(flightpassengers_df, how='left', on='FlightID'  )\n",
    "\n",
    "# Drop all duplicate columns from the merged_flight_data_df\n",
    "columns_to_drop = [\n",
    "    # Duplicates from flightresets_df merge\n",
    "    'Airline_y', \n",
    "    'DepartureCode_y', \n",
    "    'ArrivalCode_y', \n",
    "    'FlightNumber_y', \n",
    "    'TailNumber_y', \n",
    "    'AircraftType_y',\n",
    "    \n",
    "    # Duplicates from flightpassengers_df merge (keeping the passenger data)\n",
    "    'asset_id',  # Keep the original 'TailNumber_x'\n",
    "    'FlightNumber',  # Keep the original 'FlightNumber_x'\n",
    "    'DepartureCode',  # Keep the original 'DepartureCode_x'\n",
    "    'ArrivalCode',  # Keep the original 'ArrivalCode_x'\n",
    "    'FlightStartTime',  # Keep the original 'FlightStartTime_x'\n",
    "    'FlightEndTime'  # Keep the original 'FlightEndTime_x'\n",
    "]\n",
    "\n",
    "# Drop these columns\n",
    "merged_flight_data_df = merged_flight_data_df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# Rename the remaining columns to remove the _x suffix\n",
    "columns_to_rename = {\n",
    "    'FlightID_x ': 'FlightID',\n",
    "    'Airline_x': 'Airline',\n",
    "    'DepartureCode_x': 'DepartureCode',\n",
    "    'ArrivalCode_x': 'ArrivalCode',\n",
    "    'FlightStartTime_x': 'FlightStartTime',\n",
    "    'FlightEndTime_x': 'FlightEndTime',\n",
    "    'TailNumber_x': 'asset_id',\n",
    "    'FlightNumber_x': 'FlightNumber',\n",
    "    'AircraftType_x': 'AircraftType'\n",
    "}\n",
    "\n",
    "# Between flightstarttime_y and flightstarttime keep the one that isnt null\n",
    "# Between flightendtime_y and flightendtime keep the one that isnt null\n",
    "merged_flight_data_df = merged_flight_data_df.rename(columns=columns_to_rename)\n",
    "print(\"Merged_flight_data_df columns:\", merged_flight_data_df.columns.tolist())\n",
    "\n",
    "merged_flight_data_df['FlightStartTime'] = merged_flight_data_df['FlightStartTime_y'].combine_first(merged_flight_data_df['FlightStartTime'])\n",
    "merged_flight_data_df['FlightEndTime'] = merged_flight_data_df['FlightEndTime_y'].combine_first(merged_flight_data_df['FlightEndTime'])\n",
    "# Now I have 4 dataframes, merged_inflight_parts_df, combined_rma_df, merged_flight_data_df, mtbf_df\n",
    "\n",
    "# Print date range\n",
    "print(\"\\nDate Range for Dataframes:\")\n",
    "print(merged_flight_data_df['FlightStartTime'].min(), \"to\", merged_flight_data_df['FlightEndTime'].max())\n",
    "\n",
    "# Calculate flight duration\n",
    "merged_flight_data_df['FlightDuration'] = merged_flight_data_df['FlightEndTime'] - merged_flight_data_df['FlightStartTime']\n",
    "negative_duration = (merged_flight_data_df['FlightDuration'] < pd.Timedelta(0))\n",
    "merged_flight_data_df.loc[negative_duration, 'FlightDuration'] = pd.NaT\n",
    "\n",
    "print(\"Merged Flight Data Columns:\", merged_flight_data_df.columns.tolist())\n",
    "print(merged_flight_data_df.head())\n",
    "# print head starting in jan 1 2023\n",
    "print(merged_flight_data_df[merged_flight_data_df['FlightStartTime'] > '2023-01-01'].head())\n",
    "\n",
    "\n",
    "# Analyze RMA reasonings\n",
    "# Get all unique return reasons and their counts\n",
    "unique_return_reasons = combined_rma_df['return_reason'].unique()\n",
    "print(f\"\\nUnique Return Reasons ({len(unique_return_reasons)}):\")\n",
    "return_reason_counts = combined_rma_df['return_reason'].value_counts()\n",
    "print(return_reason_counts)\n",
    "\n",
    "def categorize_aerospace_return_reason(reason):\n",
    "    if pd.isna(reason):\n",
    "        return 'Unknown'\n",
    "        \n",
    "    reason_str = str(reason).upper().strip()\n",
    "    \n",
    "    # V-Code classification (common aerospace fault codes)\n",
    "    if re.match(r'^V\\d+$', reason_str) or reason_str.startswith('|V') or re.search(r'V\\d+,\\s*V\\d+', reason_str):\n",
    "        return 'V_Code'\n",
    "        \n",
    "    # E-Code classification\n",
    "    if re.match(r'^E\\d+$', reason_str) or reason_str.startswith('|E') or 'E17-MB' in reason_str:\n",
    "        return 'E_Code'\n",
    "    \n",
    "    # No Fault Found / Could Not Confirm\n",
    "    if reason_str in ['NFF', '|NFF|'] or 'NO FAULT FOUND' in reason_str:\n",
    "        return 'No_Fault_Found'\n",
    "        \n",
    "    if reason_str in ['CNC', '|CNC|'] or 'COULD NOT CONFIRM' in reason_str or 'CANNOT CONFIRM' in reason_str:\n",
    "        return 'Could_Not_Confirm'\n",
    "    \n",
    "    # Hardware issues\n",
    "    if 'PHYSICAL DAMAGE' in reason_str or 'BROKEN' in reason_str or 'CRACK' in reason_str or 'SCRATCH' in reason_str:\n",
    "        return 'Physical_Damage'\n",
    "        \n",
    "    if 'DISPLAY' in reason_str or 'SCREEN' in reason_str or 'BLANK' in reason_str or 'BLACK SCREEN' in reason_str:\n",
    "        return 'Display_Issue'\n",
    "        \n",
    "    if 'TOUCH' in reason_str or 'PHANTOM TOUCH' in reason_str:\n",
    "        return 'Touch_Screen_Issue'\n",
    "        \n",
    "    if 'POWER' in reason_str or 'NO POWER' in reason_str or 'NOT POWER' in reason_str or 'WILL NOT TURN ON' in reason_str:\n",
    "        return 'Power_Issue'\n",
    "        \n",
    "    if 'BOOT' in reason_str or 'NO START' in reason_str or 'WILL NOT BOOT' in reason_str:\n",
    "        return 'Boot_Issue'\n",
    "        \n",
    "    if 'AUDIO' in reason_str or 'JACK' in reason_str:\n",
    "        return 'Audio_Issue'\n",
    "        \n",
    "    if 'USB' in reason_str or 'GENERIC USB' in reason_str:\n",
    "        return 'USB_Issue'\n",
    "    \n",
    "    if 'ETHERNET' in reason_str or 'NETWORK' in reason_str or 'CONNECT' in reason_str:\n",
    "        return 'Network_Issue'\n",
    "        \n",
    "    # Software issues\n",
    "    if 'SOFTWARE' in reason_str or 'CORRUPT' in reason_str or 'SW' in reason_str or 'FIRMWARE' in reason_str:\n",
    "        return 'Software_Issue'\n",
    "        \n",
    "    # Common specific failures\n",
    "    if 'MEZZ' in reason_str or 'BOARD' in reason_str or 'PCB' in reason_str or 'MAIN BOARD' in reason_str:\n",
    "        return 'Board_Failure'\n",
    "        \n",
    "    if 'BATTERY' in reason_str:\n",
    "        return 'Battery_Issue'\n",
    "        \n",
    "    if 'FAN' in reason_str:\n",
    "        return 'Fan_Issue'\n",
    "        \n",
    "    if 'CRYPTO' in reason_str or 'SECURITY' in reason_str:\n",
    "        return 'Security_Component_Issue'\n",
    "        \n",
    "    if 'DEMO' in reason_str or 'CERTIFICATION' in reason_str or 'RECERTIF' in reason_str:\n",
    "        return 'Certification_Test'\n",
    "        \n",
    "    # Catch-all categories\n",
    "    if 'INOP' in reason_str or 'FAIL' in reason_str or 'FAULT' in reason_str:\n",
    "        return 'General_Inoperative'\n",
    "        \n",
    "    if reason_str == 'OTHER' or reason_str == 'V10':\n",
    "        return 'Other'\n",
    "        \n",
    "    # Numeric codes and special codes\n",
    "    if re.match(r'^\\d+-\\d+-\\d+-\\d+$', reason_str) or re.match(r'^SPEC-', reason_str):\n",
    "        return 'Special_Code'\n",
    "        \n",
    "    # Anything else\n",
    "    return 'Miscellaneous'\n",
    "\n",
    "# Apply the categorization to create a new column\n",
    "combined_rma_df['return_reason_category'] = combined_rma_df['return_reason'].apply(categorize_aerospace_return_reason)\n",
    "\n",
    "# Standardize part numbers\n",
    "merged_inflight_parts_df['PartNumber'] = merged_inflight_parts_df['PartNumber'].str.upper()\n",
    "combined_rma_df['part_number'] = combined_rma_df['part_number'].str.upper()\n",
    "mtbf_df['PartNumber'] = mtbf_df['PartNumber'].str.upper()\n",
    "\n",
    "# Calculate Repairs duration\n",
    "combined_rma_df['repair_duration'] = combined_rma_df['ship_date'] - combined_rma_df['received_date']\n",
    "# Remove negative repair durations\n",
    "negative_duration = (combined_rma_df['repair_duration'] < pd.Timedelta(0))\n",
    "combined_rma_df.loc[negative_duration, 'repair_duration'] = pd.NaT\n",
    "\n",
    "# Handle Outlier data for all of the dataframes\n",
    "# Calculate the z-score for the duration columns\n",
    "def remove_outliers_zscore(df, column, threshold=3):\n",
    "    # Remove outliers using z-score method\n",
    "    if df[column].dtype == 'timedelta64[ns]':\n",
    "        # Convert timedelta to seconds for z-score calculation\n",
    "        seconds = df[column].dt.total_seconds()\n",
    "        z_scores = zscore(seconds, nan_policy='omit')\n",
    "    else:\n",
    "        z_scores = zscore(df[column], nan_policy='omit')\n",
    "    \n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    filtered_entries = (abs_z_scores < threshold)\n",
    "    return df[filtered_entries]\n",
    "\n",
    "# Check data types and fix repair_duration if needed\n",
    "print(f\"repair_duration dtype: {combined_rma_df['repair_duration'].dtype}\")\n",
    "\n",
    "# Convert repair_duration to timedelta\n",
    "if not pd.api.types.is_timedelta64_dtype(combined_rma_df['repair_duration']):\n",
    "    try:\n",
    "        if combined_rma_df['repair_duration'].dtype == 'object' or pd.api.types.is_string_dtype(combined_rma_df['repair_duration']):\n",
    "            combined_rma_df['repair_duration'] = pd.to_timedelta(combined_rma_df['repair_duration'])\n",
    "        else:\n",
    "            if combined_rma_df['repair_duration'].median() < 1000:\n",
    "                combined_rma_df['repair_duration'] = pd.to_timedelta(combined_rma_df['repair_duration'], unit='D')\n",
    "                print(\"Converted numeric repair_duration to timedelta (assuming days)\")\n",
    "            else:\n",
    "                combined_rma_df['repair_duration'] = pd.to_timedelta(combined_rma_df['repair_duration'], unit='s')\n",
    "                print(\"Converted numeric repair_duration to timedelta (assuming seconds)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting repair_duration: {e}\")\n",
    "        combined_rma_df['repair_duration_days'] = np.nan\n",
    "        \n",
    "        # Try to calculate repair duration in days from received_date and ship_date\n",
    "        mask = ~combined_rma_df['received_date'].isna() & ~combined_rma_df['ship_date'].isna()\n",
    "        if mask.any():\n",
    "            duration = (combined_rma_df.loc[mask, 'ship_date'] - combined_rma_df.loc[mask, 'received_date']).dt.total_seconds() / (24*60*60)\n",
    "            combined_rma_df.loc[mask, 'repair_duration_days'] = duration\n",
    "\n",
    "## Handle Outliers in repair duration\n",
    "print(\"\\nHandling outliers in repair duration...\")\n",
    "# Determine which repair duration column to use\n",
    "if pd.api.types.is_timedelta64_dtype(combined_rma_df['repair_duration']):\n",
    "    repair_col = 'repair_duration'\n",
    "    repair_duration_df = combined_rma_df.dropna(subset=[repair_col])\n",
    "    repair_duration_days = repair_duration_df[repair_col].dt.total_seconds() / (24 * 60 * 60)\n",
    "    print(\"Using repair_duration column (timedelta)\")\n",
    "elif 'repair_duration_days' in combined_rma_df.columns:\n",
    "    repair_col = 'repair_duration_days'\n",
    "    repair_duration_df = combined_rma_df.dropna(subset=[repair_col])\n",
    "    repair_duration_days = repair_duration_df[repair_col]\n",
    "    print(\"Using repair_duration_days column (float)\")\n",
    "else:\n",
    "    print(\"No valid repair duration column available\")\n",
    "    repair_duration_days = None\n",
    "\n",
    "if repair_duration_days is not None:\n",
    "    # Calculate statistics before handling outliers\n",
    "    repair_mean_before = repair_duration_days.mean()\n",
    "    repair_median_before = repair_duration_days.median()\n",
    "    repair_max_before = repair_duration_days.max()\n",
    "    repair_min_before = repair_duration_days.min()\n",
    "\n",
    "    print(f\"Repair duration before (in days): mean={repair_mean_before:.2f}, median={repair_median_before:.2f}, min={repair_min_before:.2f}, max={repair_max_before:.2f}\")\n",
    "\n",
    "    # Apply outlier handling using the z-score method on days\n",
    "    z_scores = zscore(repair_duration_days, nan_policy='omit')\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    threshold = 3\n",
    "\n",
    "    # Identify outliers\n",
    "    outliers = abs_z_scores > threshold\n",
    "    outlier_indices = repair_duration_df.index[outliers]\n",
    "\n",
    "    if len(outlier_indices) > 0:\n",
    "        # Calculate the threshold in days\n",
    "        mean_val = repair_duration_days.mean()\n",
    "        std_val = repair_duration_days.std()\n",
    "        upper_limit_days = mean_val + threshold * std_val\n",
    "        lower_limit_days = max(0, mean_val - threshold * std_val)  # Ensure non-negative\n",
    "\n",
    "        print(f\"Identified {len(outlier_indices)} outliers\")\n",
    "        print(f\"Upper limit: {upper_limit_days:.2f} days, Lower limit: {lower_limit_days:.2f} days\")\n",
    "\n",
    "        if repair_col == 'repair_duration':\n",
    "            # Convert back to timedelta\n",
    "            upper_limit_td = pd.Timedelta(days=upper_limit_days)\n",
    "            lower_limit_td = pd.Timedelta(days=lower_limit_days)\n",
    "            \n",
    "            # Apply caps to original dataframe\n",
    "            combined_rma_df.loc[combined_rma_df.index.isin(outlier_indices) & (combined_rma_df[repair_col] > upper_limit_td), repair_col] = upper_limit_td\n",
    "            combined_rma_df.loc[combined_rma_df.index.isin(outlier_indices) & (combined_rma_df[repair_col] < lower_limit_td), repair_col] = lower_limit_td\n",
    "        else:\n",
    "            # Apply caps to days column\n",
    "            combined_rma_df.loc[combined_rma_df.index.isin(outlier_indices) & (combined_rma_df[repair_col] > upper_limit_days), repair_col] = upper_limit_days\n",
    "            combined_rma_df.loc[combined_rma_df.index.isin(outlier_indices) & (combined_rma_df[repair_col] < lower_limit_days), repair_col] = lower_limit_days\n",
    "\n",
    "        # Calculate statistics after handling outliers\n",
    "        if repair_col == 'repair_duration':\n",
    "            repair_duration_df = combined_rma_df.dropna(subset=[repair_col])\n",
    "            repair_duration_days_after = repair_duration_df[repair_col].dt.total_seconds() / (24 * 60 * 60)\n",
    "        else:\n",
    "            repair_duration_df = combined_rma_df.dropna(subset=[repair_col])\n",
    "            repair_duration_days_after = repair_duration_df[repair_col]\n",
    "            \n",
    "        repair_mean_after = repair_duration_days_after.mean()\n",
    "        repair_median_after = repair_duration_days_after.median()\n",
    "        repair_max_after = repair_duration_days_after.max()\n",
    "        repair_min_after = repair_duration_days_after.min()\n",
    "\n",
    "        print(f\"Repair duration after (in days): mean={repair_mean_after:.2f}, median={repair_median_after:.2f}, min={repair_min_after:.2f}, max={repair_max_after:.2f}\")\n",
    "        print(f\"Number of outliers capped: {len(outlier_indices)}\")\n",
    "    else:\n",
    "        print(\"No outliers found in repair duration data\")\n",
    "\n",
    "# Handle outliers in flight duration data\n",
    "print(\"\\nHandling outliers in flight duration...\")\n",
    "flight_duration_df = merged_flight_data_df.dropna(subset=['FlightDuration'])\n",
    "\n",
    "# Convert to hours for easier interpretation\n",
    "flight_duration_hours = flight_duration_df['FlightDuration'].dt.total_seconds() / 3600\n",
    "\n",
    "flight_mean_before = flight_duration_hours.mean()\n",
    "flight_median_before = flight_duration_hours.median()\n",
    "flight_max_before = flight_duration_hours.max()\n",
    "flight_min_before = flight_duration_hours.min()\n",
    "\n",
    "# Apply outlier handling\n",
    "z_scores = zscore(flight_duration_hours, nan_policy='omit')\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "threshold = 3\n",
    "\n",
    "# Identify which values to cap\n",
    "outliers = abs_z_scores > threshold\n",
    "outlier_indices = flight_duration_df.index[outliers]\n",
    "\n",
    "if len(outlier_indices) > 0:\n",
    "    # Calculate the threshold values in hours\n",
    "    mean_val = flight_duration_hours.mean()\n",
    "    std_val = flight_duration_hours.std()\n",
    "    upper_limit_hours = mean_val + threshold * std_val\n",
    "    lower_limit_hours = max(0, mean_val - threshold * std_val)  # Ensure non-negative\n",
    "\n",
    "    # Convert back to timedelta for capping\n",
    "    upper_limit_td = pd.Timedelta(hours=upper_limit_hours)\n",
    "    lower_limit_td = pd.Timedelta(hours=lower_limit_hours)\n",
    "\n",
    "    # Apply the caps to the original dataframe\n",
    "    merged_flight_data_df.loc[merged_flight_data_df.index.isin(outlier_indices) & (merged_flight_data_df['FlightDuration'] > upper_limit_td), 'FlightDuration'] = upper_limit_td\n",
    "    merged_flight_data_df.loc[merged_flight_data_df.index.isin(outlier_indices) & (merged_flight_data_df['FlightDuration'] < lower_limit_td), 'FlightDuration'] = lower_limit_td\n",
    "\n",
    "flight_duration_df = merged_flight_data_df.dropna(subset=['FlightDuration'])\n",
    "flight_duration_hours_after = flight_duration_df['FlightDuration'].dt.total_seconds() / 3600\n",
    "flight_mean_after = flight_duration_hours_after.mean()\n",
    "flight_median_after = flight_duration_hours_after.median()\n",
    "flight_max_after = flight_duration_hours_after.max()\n",
    "flight_min_after = flight_duration_hours_after.min()\n",
    "\n",
    "print(f\"Flight duration after (in hours): mean={flight_mean_after:.2f}, median={flight_median_after:.2f}, min={flight_min_after:.2f}, max={flight_max_after:.2f}\")\n",
    "print(f\"Number of outliers capped: {len(outlier_indices)}\")\n",
    "\n",
    "# Save dataframes as parquet files\n",
    "output_dir = os.path.join('private', 'data', 'transformed')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Data Fixes\n",
    "combined_rma_df['customer'] = combined_rma_df['customer'].astype(str)\n",
    "\n",
    "# Check the variance of all columns in merged_flight_data_df, no columns should have 0 variance...\n",
    "print(\"\\nChecking variance of all columns in merged_flight_data_df:\")\n",
    "numeric_columns = merged_flight_data_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(\"merged_flight_data_df Numeric columns:\", numeric_columns)\n",
    "print(\"Variance of merged_flight_data_df numeric columns:\")\n",
    "print(merged_flight_data_df[numeric_columns].var())\n",
    "\n",
    "# Save the dataframes\n",
    "merged_inflight_parts_df.to_parquet(os.path.join(output_dir, 'merged_inflight_parts.parquet'))\n",
    "combined_rma_df.to_parquet(os.path.join(output_dir, 'combined_rma.parquet'))\n",
    "merged_flight_data_df.to_parquet(os.path.join(output_dir, 'merged_flight_data.parquet'))\n",
    "mtbf_df.to_parquet(os.path.join(output_dir, 'mtbf.parquet'))\n",
    "\n",
    "print(\"\\nDataframes saved as parquet files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "title": "Data Extraction Pipeline"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform the data for use in RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96057/2036140016.py:30: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dataframes[df_name] = pd.read_csv(\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:30: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dataframes[df_name] = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting object columns to datetimes or strings for partslist_df:\n",
      "\n",
      "Dataframe: partslist_df\n",
      "\n",
      "Column Types:\n",
      "  string: 10 columns\n",
      "  int64: 3 columns\n",
      "  float64: 2 columns\n",
      "  datetime64[ns]: 1 columns\n",
      "\n",
      "Datetime Columns:\n",
      "  - EntryIntoServiceDate\n",
      "\n",
      "Converting object columns to datetimes or strings for merged_rmaorders_df:\n",
      "\n",
      "Dataframe: merged_rmaorders_df\n",
      "\n",
      "Column Types:\n",
      "  string: 37 columns\n",
      "  datetime64[ns, UTC]: 14 columns\n",
      "  float64: 8 columns\n",
      "  int64: 2 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:30: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dataframes[df_name] = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting object columns to datetimes or strings for hist_repair_rma_df:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe: hist_repair_rma_df\n",
      "\n",
      "Column Types:\n",
      "  string: 11 columns\n",
      "  datetime64[ns]: 4 columns\n",
      "  float64: 1 columns\n",
      "\n",
      "Datetime Columns:\n",
      "  - ReceivedDate\n",
      "  - Receivedat3P\n",
      "  - ShipDate\n",
      "  - InsertDate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96057/2036140016.py:30: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dataframes[df_name] = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting object columns to datetimes or strings for flights_df:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:30: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dataframes[df_name] = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataframe: flights_df\n",
      "\n",
      "Column Types:\n",
      "  string: 8 columns\n",
      "  datetime64[ns]: 2 columns\n",
      "  int64: 1 columns\n",
      "\n",
      "Datetime Columns:\n",
      "  - FileCreatedTime\n",
      "  - InsertDate\n",
      "\n",
      "Converting object columns to datetimes or strings for flightresets_df:\n",
      "\n",
      "Dataframe: flightresets_df\n",
      "\n",
      "Column Types:\n",
      "  string: 9 columns\n",
      "  int64: 6 columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:30: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dataframes[df_name] = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting object columns to datetimes or strings for passenger_count_df:\n",
      "\n",
      "Dataframe: passenger_count_df\n",
      "\n",
      "Column Types:\n",
      "  int64: 7 columns\n",
      "  string: 3 columns\n",
      "  datetime64[ns]: 3 columns\n",
      "  float64: 1 columns\n",
      "\n",
      "Datetime Columns:\n",
      "  - FlightStartTime\n",
      "  - FlightEndTime\n",
      "  - InsertDate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:30: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dataframes[df_name] = pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting object columns to datetimes or strings for mtbf_df:\n",
      "\n",
      "Dataframe: mtbf_df\n",
      "\n",
      "Column Types:\n",
      "  int64: 5 columns\n",
      "  string: 5 columns\n",
      "  datetime64[ns]: 3 columns\n",
      "  float64: 2 columns\n",
      "\n",
      "Datetime Columns:\n",
      "  - Month\n",
      "  - InsertDate\n",
      "  - UpdateDate\n",
      "\n",
      "Converting object columns to datetimes or strings for productinfo_df:\n",
      "\n",
      "Dataframe: productinfo_df\n",
      "\n",
      "Column Types:\n",
      "  string: 23 columns\n",
      "  float64: 8 columns\n",
      "  datetime64[ns]: 2 columns\n",
      "\n",
      "Datetime Columns:\n",
      "  - milestonedate\n",
      "  - actualdate\n",
      "inflight_parts_df: Actual missing tails: 0\n",
      "\n",
      "hist_rma_df: Actual missing hist_ship_dates: 61627\n",
      " actual hist_ship_dates: 61627 missing (23.9%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:30: FutureWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  dataframes[df_name] = pd.read_csv(\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n",
      "/tmp/ipykernel_96057/2036140016.py:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[col] = pd.to_datetime(df[col], errors='raise')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New RMA Columns with missing values:\n",
      "  SvcCallId: 1 missing (0.0%)\n",
      "  DeliveryLocation_LocationId: 454 missing (1.4%)\n",
      "  ProjId_rma: 420 missing (1.3%)\n",
      "  CallTypeId: 1 missing (0.0%)\n",
      "  HcmWorker_PersonnelNumber: 4172 missing (13.2%)\n",
      "  ComplaintId: 1767 missing (5.6%)\n",
      "  ProjectIntegrationId: 1 missing (0.0%)\n",
      "  CauseId: 3543 missing (11.2%)\n",
      "  ServiceObjectId: 20 missing (0.1%)\n",
      "  InvoiceName: 1 missing (0.0%)\n",
      "  SolutionId: 28293 missing (89.3%)\n",
      "  RepairWithAccessories: 4409 missing (13.9%)\n",
      "  CallStatusId: 1 missing (0.0%)\n",
      "  CustAccount_rma: 17 missing (0.1%)\n",
      "  InternalMsg: 10190 missing (32.2%)\n",
      "  HSOCertificateType: 2399 missing (7.6%)\n",
      "  HSOOptionalRevision: 6638 missing (21.0%)\n",
      "  HSOVisualInspectionResult: 4036 missing (12.7%)\n",
      "  HSOPortalStatusRMA: 112 missing (0.4%)\n",
      "  HSOReturnReason: 1712 missing (5.4%)\n",
      "  HSORepairDirection: 2932 missing (9.3%)\n",
      "  Status: 20 missing (0.1%)\n",
      "  CustAccountUser: 78 missing (0.2%)\n",
      "  CustAccountOwner: 15354 missing (48.5%)\n",
      "  Description: 23 missing (0.1%)\n",
      "  ItemId: 20 missing (0.1%)\n",
      "  MachineTypeId: 20 missing (0.1%)\n",
      "  InventSerialId: 393 missing (1.2%)\n",
      "  WarrantyStartDate_Cust: 20 missing (0.1%)\n",
      "  WorkshopRepairStatusRefRecId: 20 missing (0.1%)\n",
      "  WorkshopRepairStatus: 20 missing (0.1%)\n",
      "  WorkshopRepairStatusRefTableId: 20 missing (0.1%)\n",
      "\n",
      "New Hist-RMA Columns with missing values:\n",
      "  PN: 2 missing (0.0%)\n",
      "  SN: 15 missing (0.0%)\n",
      "  StatusDescription: 2 missing (0.0%)\n",
      "  PartDescription: 2 missing (0.0%)\n",
      "  LRUName: 2 missing (0.0%)\n",
      "  ReceivedDate: 105614 missing (40.9%)\n",
      "  Receivedat3P: 193418 missing (74.9%)\n",
      "  FaultCode: 96532 missing (37.4%)\n",
      "  ShipDate: 16215 missing (6.3%)\n",
      "Converting Missed Datetime fields\n",
      "Earliest and Latest Dates\n",
      "\n",
      "partslist_df\n",
      "  EntryIntoServiceDate (before): 2012-03-21 00:00:00 - 2048-12-01 00:00:00\n",
      "  EntryIntoServiceDate (after): 2012-03-21 00:00:00 - 2048-12-01 00:00:00 (0 values replaced with NaT)\n",
      "\n",
      "merged_rmaorders_df\n",
      "\n",
      "hist_repair_rma_df\n",
      "  ReceivedDate (before): 1753-01-01 00:00:00 - 2023-04-19 00:00:00\n",
      "  ReceivedDate (after): 2012-05-07 00:00:00 - 2023-04-19 00:00:00 (98 values replaced with NaT)\n",
      "  Receivedat3P (before): 1753-01-01 00:00:00 - 2023-04-24 00:00:00\n",
      "  Receivedat3P (after): 2012-03-07 00:00:00 - 2023-04-24 00:00:00 (18061 values replaced with NaT)\n",
      "  ShipDate (before): 1753-01-01 00:00:00 - 2023-04-27 00:00:00\n",
      "  ShipDate (after): 2012-06-08 00:00:00 - 2023-04-27 00:00:00 (318 values replaced with NaT)\n",
      "\n",
      "flights_df\n",
      "  FlightStartTime (before): 2019-03-01 00:02:03 - 2033-06-29 21:41:59\n",
      "  FlightStartTime (after): 2019-03-01 00:02:03 - 2033-06-29 21:41:59 (0 values replaced with NaT)\n",
      "  FlightEndTime (before): 2019-03-01 01:41:34 - 2033-06-29 21:42:25\n",
      "  FlightEndTime (after): 2019-03-01 01:41:34 - 2033-06-29 21:42:25 (0 values replaced with NaT)\n",
      "\n",
      "flightresets_df\n",
      "  FlightStartTime (before): 2020-04-26 12:58:58 - 2025-04-13 18:54:50\n",
      "  FlightStartTime (after): 2020-04-26 12:58:58 - 2025-04-13 18:54:50 (0 values replaced with NaT)\n",
      "  FlightEndTime (before): 2020-04-27 03:12:01 - 2025-04-13 22:09:20\n",
      "  FlightEndTime (after): 2020-04-27 03:12:01 - 2025-04-13 22:09:20 (0 values replaced with NaT)\n",
      "\n",
      "passenger_count_df\n",
      "  FlightStartTime (before): 2018-08-23 01:48:14 - 2025-04-13 00:35:24\n",
      "  FlightStartTime (after): 2018-08-23 01:48:14 - 2025-04-13 00:35:24 (0 values replaced with NaT)\n",
      "  FlightEndTime (before): 2018-08-23 04:28:57 - 2025-04-13 02:44:25\n",
      "  FlightEndTime (after): 2018-08-23 04:28:57 - 2025-04-13 02:44:25 (0 values replaced with NaT)\n",
      "\n",
      "mtbf_df\n",
      "  Month (before): 2013-02-01 00:00:00 - 2023-06-01 00:00:00\n",
      "  Month (after): 2013-02-01 00:00:00 - 2023-06-01 00:00:00 (0 values replaced with NaT)\n",
      "\n",
      "productinfo_df\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_96057/2036140016.py:224: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  combined_rma_df = pd.concat([rma_for_concat, hist_for_concat], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flight_data_df columns: ['FlightID', 'Airline', 'DepartureCode', 'ArrivalCode', 'FlightStartTime', 'FlightEndTime', 'TailNumber', 'FlightNumber', 'AircraftType']\n",
      "flightresets_df columns: ['FlightResetsID', 'FlightID', 'Airline', 'DepartureCode', 'ArrivalCode', 'FlightNumber', 'TailNumber', 'FlightStartTime', 'FlightEndTime', 'FlightDuration', 'Class', 'AircraftType', 'SeatResets', 'RawResets', 'Processed']\n",
      "flightresets_df     FlightResetsID  FlightID Airline DepartureCode ArrivalCode FlightNumber  \\\n",
      "10         1490862   3234997     ITA           JFK         FCO            6   \n",
      "11         1490863   3235010     ITA           MIA         FCO          608   \n",
      "12         1490970   3236421     ITA           FCO         JFK            6   \n",
      "13         1492037   3244555     ITA           FCO         MIA          630   \n",
      "14         1492606   3245943     ITA           FCO         GRU          674   \n",
      "\n",
      "   TailNumber     FlightStartTime       FlightEndTime  FlightDuration  \\\n",
      "10     EI-HJO 2023-09-27 01:58:49 2023-09-27 09:26:26           26857   \n",
      "11     EI-HJO 2023-09-26 00:03:59 2023-09-26 09:27:39           33820   \n",
      "12     EI-HPA 2023-09-28 14:14:06 2023-09-28 23:09:36           32130   \n",
      "13     EI-HPA 2023-10-07 09:12:30 2023-10-07 20:04:09           39099   \n",
      "14     EI-HPA 2023-10-08 20:28:45 2023-10-09 07:43:47           40502   \n",
      "\n",
      "              Class AircraftType  SeatResets  RawResets  Processed  \n",
      "10  EconomyPremiere         <NA>           1          1          0  \n",
      "11          Economy         <NA>           2          2          0  \n",
      "12  EconomyPremiere         <NA>           2          4          0  \n",
      "13  EconomyPremiere         <NA>           1          1          1  \n",
      "14          Economy         <NA>           1          1          1  \n",
      "merged_flight_data_df columns: ['FlightID', 'Airline_x', 'DepartureCode_x', 'ArrivalCode_x', 'FlightStartTime_x', 'FlightEndTime_x', 'TailNumber_x', 'FlightNumber_x', 'AircraftType_x', 'FlightResetsID', 'Airline_y', 'DepartureCode_y', 'ArrivalCode_y', 'FlightNumber_y', 'TailNumber_y', 'FlightStartTime_y', 'FlightEndTime_y', 'FlightDuration', 'Class', 'AircraftType_y', 'SeatResets', 'RawResets', 'Processed']\n",
      "Unique RawResets values in flightresets_df:\n",
      "[  4   3   5   2   1  19   9   8   6  10  12  69  15  16   7  51  11  58\n",
      "  18  14  21  36  31  13  30  57  44  47  25  70  23  17  20  26 136  54\n",
      "  86  24  22  39  29  32  33  37 197  75  38  88  68  59  35  41  42 117\n",
      "  49  43  81  27  48  74  55  79  64  60  40 160  61  65  77  28  94  45\n",
      "  53  73 109  67  34  46  82  50 101  90  56  62  91 107  72  80  52  66\n",
      " 146  63 260  96  71 154 102  97  83 127 125  93 104 108  84 121 141 148\n",
      "  95  87 137  98 134 105 176  76 126 123 112  85 143 156  78  89  92 106\n",
      " 140 103 151 174 152 133 179 128 122 114 131 382 309 147 200 100 252 196\n",
      "  99 150 116 110 198 166 111 336 352 383 503 385 508 237 115 256 310 334\n",
      " 580 946 277 124 330 119 135 350 118 416 373 381 283 296 467 420 374 369\n",
      " 220 302 429 213 212 527 113 477 548 291 361 422 387 168 341 192 130 199\n",
      " 162 215 169 191 138 144 239 183 250 214 153 292 590 218 142 175 249 240\n",
      " 285 217 254 172 201 257 184 178 488 161 171 364 585 157 228 180]\n",
      "Unique RawResets values in merged_flight_data_df:\n",
      "<IntegerArray>\n",
      "[<NA>,    4,    3,    5,    2,    1,   19,   10,    9,   69,\n",
      " ...\n",
      "  184,  178,  488,  161,  171,  364,  585,  157,  228,  180]\n",
      "Length: 233, dtype: Int64\n",
      "Common Flight IDs: 156321 of 430986 and 223747\n",
      "Merged_flight_data_df columns: ['FlightID', 'Airline', 'DepartureCode', 'ArrivalCode', 'FlightStartTime', 'FlightEndTime', 'TailNumber', 'FlightNumber', 'AircraftType', 'FlightResetsID', 'FlightStartTime_y', 'FlightEndTime_y', 'FlightDuration', 'Class', 'SeatResets', 'RawResets', 'Processed', 'AIMSID', 'BusinessClass', 'EconomyClass', 'TotalPassengers', 'UpdatedPaxActivity', 'UpdatedPerPassengerRevenue']\n",
      "\n",
      "Date Range for Dataframes:\n",
      "2019-03-01 00:02:03 to 2033-06-29 21:42:25\n",
      "Merged Flight Data Columns: ['FlightID', 'Airline', 'DepartureCode', 'ArrivalCode', 'FlightStartTime', 'FlightEndTime', 'TailNumber', 'FlightNumber', 'AircraftType', 'FlightResetsID', 'FlightStartTime_y', 'FlightEndTime_y', 'FlightDuration', 'Class', 'SeatResets', 'RawResets', 'Processed', 'AIMSID', 'BusinessClass', 'EconomyClass', 'TotalPassengers', 'UpdatedPaxActivity', 'UpdatedPerPassengerRevenue']\n",
      "   FlightID Airline DepartureCode ArrivalCode FlightStartTime FlightEndTime  \\\n",
      "0     98226     LAN           VCP         BSB             NaT           NaT   \n",
      "1     98566     PAL           MNL         MPH             NaT           NaT   \n",
      "2     98568     PAL           MPH         MNL             NaT           NaT   \n",
      "3     99413     PAL           DVO         MNL             NaT           NaT   \n",
      "4    102549     LAN           GRU         MVD             NaT           NaT   \n",
      "\n",
      "  TailNumber FlightNumber     AircraftType  FlightResetsID  ... Class  \\\n",
      "0     PR-MHF      TAM3656  Airbus A320-200             NaN  ...  <NA>   \n",
      "1    RPC9911      GAP2053  Airbus A321-200             NaN  ...  <NA>   \n",
      "2    RPC9911      GAP2054  Airbus A321-200             NaN  ...  <NA>   \n",
      "3    RPC9911      GAP2808  Airbus A321-200             NaN  ...  <NA>   \n",
      "4     PT-MXE      TAM8030  Airbus A321-200             NaN  ...  <NA>   \n",
      "\n",
      "  SeatResets RawResets Processed  AIMSID  BusinessClass  EconomyClass  \\\n",
      "0        NaN      <NA>       NaN     NaN            NaN           NaN   \n",
      "1        NaN      <NA>       NaN     NaN            NaN           NaN   \n",
      "2        NaN      <NA>       NaN     NaN            NaN           NaN   \n",
      "3        NaN      <NA>       NaN     NaN            NaN           NaN   \n",
      "4        NaN      <NA>       NaN     NaN            NaN           NaN   \n",
      "\n",
      "   TotalPassengers  UpdatedPaxActivity  UpdatedPerPassengerRevenue  \n",
      "0              NaN                 NaN                         NaN  \n",
      "1              NaN                 NaN                         NaN  \n",
      "2              NaN                 NaN                         NaN  \n",
      "3              NaN                 NaN                         NaN  \n",
      "4              NaN                 NaN                         NaN  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "      FlightID Airline DepartureCode ArrivalCode     FlightStartTime  \\\n",
      "1537   2364960     LAN           CLO         CTG 2030-04-30 23:18:24   \n",
      "1538   2365448     LAN           MDE         SMR 2030-05-01 10:22:15   \n",
      "1539   2366043     LAN           CLO         PSO 2030-05-01 18:24:56   \n",
      "1540   2480091     LAN           SCL         SCL 2033-06-29 21:41:59   \n",
      "1542   2649976     LAN           FLN         CGH 2024-11-08 21:00:16   \n",
      "\n",
      "           FlightEndTime TailNumber FlightNumber     AircraftType  \\\n",
      "1537 2030-05-01 00:23:31     CC-COF      ARE4246  Airbus A320-200   \n",
      "1538 2030-05-01 11:17:17     CC-COF      ARE4220  Airbus A320-200   \n",
      "1539 2030-05-01 18:57:28     CC-COF      ARE4368  Airbus A320-200   \n",
      "1540 2033-06-29 21:42:25     CC-CYL         <NA>      Airbus A319   \n",
      "1542 2024-11-08 21:46:30     PR-MBG      TAM9430  Airbus A320-200   \n",
      "\n",
      "      FlightResetsID  ... Class SeatResets RawResets Processed  AIMSID  \\\n",
      "1537             NaN  ...  <NA>        NaN      <NA>       NaN     NaN   \n",
      "1538             NaN  ...  <NA>        NaN      <NA>       NaN     NaN   \n",
      "1539             NaN  ...  <NA>        NaN      <NA>       NaN     NaN   \n",
      "1540             NaN  ...  <NA>        NaN      <NA>       NaN     NaN   \n",
      "1542             NaN  ...  <NA>        NaN      <NA>       NaN     NaN   \n",
      "\n",
      "      BusinessClass  EconomyClass  TotalPassengers  UpdatedPaxActivity  \\\n",
      "1537            NaN           NaN              NaN                 NaN   \n",
      "1538            NaN           NaN              NaN                 NaN   \n",
      "1539            NaN           NaN              NaN                 NaN   \n",
      "1540            NaN           NaN              NaN                 NaN   \n",
      "1542            NaN           NaN              NaN                 NaN   \n",
      "\n",
      "      UpdatedPerPassengerRevenue  \n",
      "1537                         NaN  \n",
      "1538                         NaN  \n",
      "1539                         NaN  \n",
      "1540                         NaN  \n",
      "1542                         NaN  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "\n",
      "Unique Return Reasons (12072):\n",
      "return_reason\n",
      "V10                                                                                            22272\n",
      "NFF                                                                                            15465\n",
      "CNC                                                                                            15334\n",
      "Other                                                                                           8996\n",
      "|CNC|                                                                                           7400\n",
      "                                                                                               ...  \n",
      "Broken Fan, Damaged Compact Flash Card, X2 Connector Damage, X3 Connector Failure                  1\n",
      "Broken Fan, Damaged Compact Flash Card, Memory Module Failure, Re-soldered X1/X4 Connectors        1\n",
      "TOUCH SCREEN IS GLIT                                                                               1\n",
      "Damaged Compact Flash Card, Modem Connection Failure, Re-soldered X1/X4 Connectors                 1\n",
      "SPI Ticket  4777  Un                                                                               1\n",
      "Name: count, Length: 12071, dtype: Int64\n",
      "repair_duration dtype: object\n",
      "\n",
      "Handling outliers in repair duration...\n",
      "Using repair_duration column (timedelta)\n",
      "Repair duration before (in days): mean=71.79, median=14.00, min=0.00, max=45721.00\n",
      "Identified 168 outliers\n",
      "Upper limit: 4302.05 days, Lower limit: 0.00 days\n",
      "Repair duration after (in days): mean=32.10, median=14.00, min=0.00, max=4302.05\n",
      "Number of outliers capped: 168\n",
      "\n",
      "Handling outliers in flight duration...\n",
      "Flight duration after (in hours): mean=4.35, median=3.12, min=0.00, max=66.99\n",
      "Number of outliers capped: 236\n",
      "\n",
      "Checking variance of all columns in merged_flight_data_df:\n",
      "merged_flight_data_df Numeric columns: ['FlightID', 'FlightResetsID', 'SeatResets', 'RawResets', 'Processed', 'AIMSID', 'BusinessClass', 'EconomyClass', 'TotalPassengers', 'UpdatedPaxActivity', 'UpdatedPerPassengerRevenue']\n",
      "Variance of merged_flight_data_df numeric columns:\n",
      "FlightID                      73038075399.661285\n",
      "FlightResetsID                13625490592.080618\n",
      "SeatResets                             26.409694\n",
      "RawResets                             100.186361\n",
      "Processed                               0.019922\n",
      "AIMSID                         1207373046.903816\n",
      "BusinessClass                          15.004153\n",
      "EconomyClass                         1677.461224\n",
      "TotalPassengers                      1756.254137\n",
      "UpdatedPaxActivity                           0.0\n",
      "UpdatedPerPassengerRevenue                   0.0\n",
      "dtype: Float64\n",
      "\n",
      "Dataframes saved as parquet files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "# Load each CSV into a DataFrame\n",
    "raw_data_dir = os.path.join('private', 'data', 'raw')\n",
    "\n",
    "# List of file names\n",
    "file_names = [\n",
    "    'partslist.csv',\n",
    "    'merged_rmaorders.csv',\n",
    "    'hist_repair_rma.csv',\n",
    "    'flights.csv',\n",
    "    'flightresets.csv',\n",
    "    'passenger_count.csv',\n",
    "    'mtbf.csv',\n",
    "    'productinfo.csv',\n",
    "]\n",
    "\n",
    "# Dictionary to store DataFrames\n",
    "dataframes = {}\n",
    "pd.reset_option('display.float_format')\n",
    "\n",
    "# Load each CSV into a DataFrame and store in the dictionary\n",
    "for file_name in file_names:\n",
    "    df_name = file_name.split('.')[0] + '_df'\n",
    "    dataframes[df_name] = pd.read_csv(\n",
    "        os.path.join(raw_data_dir, file_name),\n",
    "        parse_dates=True,  # Try to parse date columns\n",
    "        infer_datetime_format=True,  # Use format inference for dates\n",
    "        low_memory=False  # Avoid mixed type inference warnings\n",
    "    )\n",
    "\n",
    "    # Temp store current dataframe\n",
    "    df = dataframes[df_name]\n",
    "\n",
    "    # Change objects to columns\n",
    "    object_columns = df.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    if len(object_columns) > 0:\n",
    "        print(f\"\\nConverting object columns to datetimes or strings for {df_name}:\")\n",
    "        for col in object_columns:\n",
    "            # Try converting to datetime\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col], errors='raise')\n",
    "            except:\n",
    "                # Convert to string dtype\n",
    "                df[col] = df[col].astype(\"string\")\n",
    "\n",
    "    print(f\"\\nDataframe: {df_name}\")\n",
    "    print(\"\\nColumn Types:\")\n",
    "    type_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in type_counts.items():\n",
    "        print(f\"  {dtype}: {count} columns\")\n",
    "\n",
    "    datetime_columns = df.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "    if datetime_columns:\n",
    "        print(\"\\nDatetime Columns:\")\n",
    "        for col in datetime_columns:\n",
    "            print(f\"  - {col}\")\n",
    "\n",
    "# DataFrames\n",
    "inflight_parts_df = dataframes['partslist_df']\n",
    "rma_df = dataframes['merged_rmaorders_df']\n",
    "hist_rma_df = dataframes['hist_repair_rma_df']\n",
    "flight_data_df = dataframes['flights_df']\n",
    "flightresets_df = dataframes['flightresets_df']\n",
    "flightpassengers_df = dataframes['passenger_count_df']\n",
    "mtbf_df = dataframes['mtbf_df']\n",
    "productinfo_df = dataframes['productinfo_df']\n",
    "\n",
    "# Label encode line manager to preserve privacy\n",
    "productinfo_df['linemanager'] = productinfo_df['linemanager'].astype('category').cat.codes\n",
    "\n",
    "## Fix Issues as they are discovered\n",
    "# Determine validity of missing fields \n",
    "actual_missing_tails = inflight_parts_df[\n",
    "    (inflight_parts_df['Tail'].isnull()) & \n",
    "    (inflight_parts_df['ServiceStatus'] != 'Not in Service Yet')\n",
    "]\n",
    "print(f\"inflight_parts_df: Actual missing tails: {len(actual_missing_tails)}\")\n",
    "actual_missing_hist_ship_dates = hist_rma_df[\n",
    "    (hist_rma_df['ReceivedDate'].isnull()) & \n",
    "    (hist_rma_df['Receivedat3P'].isnull())\n",
    "]\n",
    "print(f\"\\nhist_rma_df: Actual missing hist_ship_dates: {len(actual_missing_hist_ship_dates)}\")\n",
    "print(f\" actual hist_ship_dates: {len(actual_missing_hist_ship_dates)} missing ({len(actual_missing_hist_ship_dates)/len(hist_rma_df)*100:.1f}%)\")\n",
    "\n",
    "# Drop 'unused' fields\n",
    "rma_df.drop(['Message'], axis=1, inplace=True)\n",
    "rma_df.drop(['Subject'], axis=1, inplace=True)\n",
    "rma_df.drop(['Progress'], axis=1, inplace=True)\n",
    "rma_df.drop(['Solution'], axis=1, inplace=True)\n",
    "rma_df.drop(['HSOAirplaneType'], axis=1, inplace=True)\n",
    "rma_df.drop(['HSOFinalATPDocRevision'], axis=1, inplace=True)\n",
    "rma_df.drop(['HSOAirPlaneTailSerialNumber'], axis=1, inplace=True)\n",
    "rma_df.drop(['HSOAirPlaneTailNumber'], axis=1, inplace=True)\n",
    "rma_df.drop(['ContactPersonPhone'], axis=1, inplace=True)\n",
    "rma_df.drop(['ContactPersonEmail'], axis=1, inplace=True)\n",
    "rma_missing = rma_df.isnull().sum()\n",
    "if rma_missing.any():\n",
    "    print(\"\\nNew RMA Columns with missing values:\")\n",
    "    for col, count in rma_missing[rma_missing > 0].items():\n",
    "        print(f\"  {col}: {count} missing ({count/len(rma_df)*100:.1f}%)\")\n",
    "\n",
    "hist_rma_df.drop(['ServiceBulletinInfo'], axis=1, inplace=True)\n",
    "hist_rma_df.drop(['ServiceBulletinNumber'], axis=1, inplace=True)\n",
    "hist_rma_df.drop(['ServiceBulletin'], axis=1, inplace=True)\n",
    "hist_rma_df.drop(['AlertCategoryCode'], axis=1, inplace=True)\n",
    "hist_rma_missing = hist_rma_df.isnull().sum()\n",
    "if hist_rma_missing.any():\n",
    "    print(\"\\nNew Hist-RMA Columns with missing values:\")\n",
    "    for col, count in hist_rma_missing[hist_rma_missing > 0].items():\n",
    "        print(f\"  {col}: {count} missing ({count/len(hist_rma_df)*100:.1f}%)\")\n",
    "productinfo_df.drop(['productgroup'], axis=1, inplace=True)\n",
    "productinfo_df.drop(['actualdate'], axis=1, inplace=True)\n",
    "productinfo_df.drop(['notes'], axis=1, inplace=True)\n",
    "productinfo_df.drop(['milestonedate'], axis=1, inplace=True)\n",
    "productinfo_df.drop(['milestonestatus'], axis=1, inplace=True)\n",
    "productinfo_df.drop(['milestone'], axis=1, inplace=True)\n",
    "productinfo_df.drop(['bajfunctionality'], axis=1, inplace=True) # What?\n",
    "productinfo_df.drop(['conformitydescription'], axis=1, inplace=True)\n",
    "\n",
    "# Convert missed Datetime fields\n",
    "print(\"Converting Missed Datetime fields\")\n",
    "flight_data_df['FlightStartTime'] = pd.to_datetime(flight_data_df['FlightStartTime'], format='ISO8601', errors='coerce')\n",
    "flight_data_df['FlightEndTime'] = pd.to_datetime(flight_data_df['FlightEndTime'], format='ISO8601', errors='coerce')\n",
    "flightresets_df['FlightStartTime'] = pd.to_datetime(flightresets_df['FlightStartTime'], format='ISO8601',  errors='coerce')\n",
    "flightresets_df['FlightEndTime'] = pd.to_datetime(flightresets_df['FlightEndTime'], format='ISO8601',  errors='coerce')\n",
    "hist_rma_df['ReceivedDate'] = pd.to_datetime(hist_rma_df['ReceivedDate'], errors='coerce')\n",
    "hist_rma_df['ShipDate'] = pd.to_datetime(hist_rma_df['ShipDate'], errors='coerce')\n",
    "# Get rid of insert/update dates\n",
    "hist_rma_df.drop(['InsertDate'], axis=1, inplace=True)\n",
    "flight_data_df.drop(['FileCreatedTime'], axis=1, inplace=True)\n",
    "flight_data_df.drop(['InsertDate'], axis=1, inplace=True)\n",
    "flightpassengers_df.drop(['InsertDate'], axis=1, inplace=True)\n",
    "mtbf_df.drop(['InsertDate'], axis=1, inplace=True)\n",
    "mtbf_df.drop(['UpdateDate'], axis=1, inplace=True)\n",
    "\n",
    "# I want to see earliest dates and latest dates in all the dfs\n",
    "# I dont want placeholder dates, replace will null\n",
    "print(\"Earliest and Latest Dates\")\n",
    "for df_name, df in dataframes.items():\n",
    "    min_valid_date = pd.Timestamp('2012-01-01')\n",
    "    print(f\"\\n{df_name}\")\n",
    "    \n",
    "    # Get all datetime columns regardless of timezone\n",
    "    datetime_cols = [col for col in df.columns if pd.api.types.is_datetime64_dtype(df[col])]\n",
    "    \n",
    "    for col in datetime_cols:\n",
    "        print(f\"  {col} (before): {df[col].min()} - {df[col].max()}\")\n",
    "        # Convert timezone-aware columns to regular\n",
    "        if hasattr(df[col].dtype, 'tz') and df[col].dtype.tz is not None:\n",
    "            df[col] = df[col].dt.tz_localize(None)\n",
    "            print(f\"Removed timezone from {col}\")\n",
    "        # Replace dates before min_valid_date with NaT\n",
    "        invalid_count = (df[col] < min_valid_date).sum()\n",
    "        df.loc[df[col] < min_valid_date, col] = pd.NaT\n",
    "        print(f\"  {col} (after): {df[col].min()} - {df[col].max()} ({invalid_count} values replaced with NaT)\")\n",
    "\n",
    "# I need to find out how to merge data\n",
    "# inflight_parts_df gets Left joined to productinfo_df on PartNumber\n",
    "merged_inflight_parts_df = inflight_parts_df.merge(productinfo_df, how='left', left_on='PartNumber', right_on='partnumber')\n",
    "\n",
    "# Determine new column names\n",
    "rma_standardized = rma_df.copy()\n",
    "rma_standardized['source'] = 'current_rma'\n",
    "rma_standardized['rma_number'] = rma_standardized['SvcCallId']\n",
    "rma_standardized['part_number'] = rma_standardized['ItemId']\n",
    "rma_standardized['serial_number'] = rma_standardized['InventSerialId']\n",
    "rma_standardized['status'] = rma_standardized['WorkshopRepairStatus']\n",
    "rma_standardized['received_date'] = rma_standardized['HSOUnitReceivedDate']  \n",
    "rma_standardized['ship_date'] = rma_standardized['HSOActualShipDate'] \n",
    "rma_standardized['warranty_end_date'] = rma_standardized['HSOWarrantyEndDate']  \n",
    "rma_standardized['customer'] = rma_standardized['CustAccount_rma']\n",
    "rma_standardized['part_description'] = rma_standardized['Description']\n",
    "rma_standardized['fault_code'] = rma_standardized['ComplaintId']\n",
    "rma_standardized['lru_name'] = None  # No direct match in current RMA\n",
    "\n",
    "hist_standardized = hist_rma_df.copy()\n",
    "hist_standardized['source'] = 'historical_rma'\n",
    "hist_standardized['rma_number'] = hist_standardized['RMA']\n",
    "hist_standardized['part_number'] = hist_standardized['PN']\n",
    "hist_standardized['serial_number'] = hist_standardized['SN']\n",
    "hist_standardized['status'] = hist_standardized['StatusDescription']\n",
    "hist_standardized['received_date'] = hist_standardized['ReceivedDate']  \n",
    "hist_standardized['ship_date'] = hist_standardized['ShipDate']  \n",
    "hist_standardized['customer'] = hist_standardized['Customer']\n",
    "hist_standardized['part_description'] = hist_standardized['PartDescription']\n",
    "hist_standardized['fault_code'] = hist_standardized['FaultCode']\n",
    "hist_standardized['lru_name'] = hist_standardized['LRUName']\n",
    "\n",
    "hist_standardized['workshop_location'] = None  # No equivalent in historical data\n",
    "hist_standardized['flight_hours'] = None  # No equivalent in historical data\n",
    "hist_standardized['return_reason'] = hist_standardized['FaultCode'] \n",
    "hist_standardized['warranty_end_date'] = None  # No equivalent in historical data\n",
    "\n",
    "rma_standardized['workshop_location'] = rma_standardized['HSOWorkshopLocation']\n",
    "rma_standardized['flight_hours'] = rma_standardized['HSOFlightHours']\n",
    "rma_standardized['return_reason'] = rma_standardized['HSOReturnReason']\n",
    "rma_standardized['warranty_end_date'] =  rma_standardized['HSOWarrantyEndDate']\n",
    "\n",
    "final_columns = [\n",
    "    'source', 'rma_number', 'part_number', 'serial_number', 'customer',\n",
    "    'status', 'received_date', 'ship_date', 'part_description', \n",
    "    'fault_code', 'lru_name', 'workshop_location', 'flight_hours',\n",
    "    'return_reason', 'warranty_end_date'\n",
    "]\n",
    "\n",
    "rma_for_concat = rma_standardized[final_columns]\n",
    "hist_for_concat = hist_standardized[final_columns]\n",
    "\n",
    "combined_rma_df = pd.concat([rma_for_concat, hist_for_concat], ignore_index=True)\n",
    "\n",
    "print(\"flight_data_df columns:\", flight_data_df.columns.tolist())\n",
    "print(\"flightresets_df columns:\", flightresets_df.columns.tolist())\n",
    "print('flightresets_df', flightresets_df[flightresets_df['FlightStartTime'] > '2023-01-01'].head())\n",
    "\n",
    "merged_flight_data_df = flight_data_df.merge(flightresets_df, how='left', on='FlightID')\n",
    "\n",
    "# Fix float fields that should be integers\n",
    "merged_flight_data_df['RawResets'] = pd.array(\n",
    "    merged_flight_data_df['RawResets'].to_numpy(), \n",
    "    dtype=pd.Int64Dtype()\n",
    ")\n",
    "\n",
    "print(\"merged_flight_data_df columns:\", merged_flight_data_df.columns.tolist())\n",
    "\n",
    "print(\"Unique RawResets values in flightresets_df:\")\n",
    "print(flightresets_df['RawResets'].unique())\n",
    "# Print all unique vvalues of RawResets in merged_flight_data_df\n",
    "print(\"Unique RawResets values in merged_flight_data_df:\")\n",
    "print(merged_flight_data_df['RawResets'].unique())\n",
    "\n",
    "# It appears that this merge is not working, \n",
    "# check for common flight ids between the two dataframes\n",
    "common_flight_ids = set(flight_data_df['FlightID']).intersection(set(flightresets_df['FlightID']))\n",
    "print(f\"Common Flight IDs: {len(common_flight_ids)} of {len(flight_data_df)} and {len(flightresets_df)}\")\n",
    "\n",
    "merged_flight_data_df = merged_flight_data_df.merge(flightpassengers_df, how='left', on='FlightID'  )\n",
    "\n",
    "# Drop all duplicate columns from the merged_flight_data_df\n",
    "columns_to_drop = [\n",
    "    # Duplicates from flightresets_df merge\n",
    "    'Airline_y', \n",
    "    'DepartureCode_y', \n",
    "    'ArrivalCode_y', \n",
    "    'FlightNumber_y', \n",
    "    'TailNumber_y', \n",
    "    'AircraftType_y',\n",
    "    \n",
    "    # Duplicates from flightpassengers_df merge (keeping the passenger data)\n",
    "    'TailNumber',  # Keep the original 'TailNumber_x'\n",
    "    'FlightNumber',  # Keep the original 'FlightNumber_x'\n",
    "    'DepartureCode',  # Keep the original 'DepartureCode_x'\n",
    "    'ArrivalCode',  # Keep the original 'ArrivalCode_x'\n",
    "    'FlightStartTime',  # Keep the original 'FlightStartTime_x'\n",
    "    'FlightEndTime'  # Keep the original 'FlightEndTime_x'\n",
    "]\n",
    "\n",
    "# Drop these columns\n",
    "merged_flight_data_df = merged_flight_data_df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# Rename the remaining columns to remove the _x suffix\n",
    "columns_to_rename = {\n",
    "    'FlightID_x ': 'FlightID',\n",
    "    'Airline_x': 'Airline',\n",
    "    'DepartureCode_x': 'DepartureCode',\n",
    "    'ArrivalCode_x': 'ArrivalCode',\n",
    "    'FlightStartTime_x': 'FlightStartTime',\n",
    "    'FlightEndTime_x': 'FlightEndTime',\n",
    "    'TailNumber_x': 'TailNumber',\n",
    "    'FlightNumber_x': 'FlightNumber',\n",
    "    'AircraftType_x': 'AircraftType'\n",
    "}\n",
    "\n",
    "# Between flightstarttime_y and flightstarttime keep the one that isnt null\n",
    "# Between flightendtime_y and flightendtime keep the one that isnt null\n",
    "merged_flight_data_df = merged_flight_data_df.rename(columns=columns_to_rename)\n",
    "print(\"Merged_flight_data_df columns:\", merged_flight_data_df.columns.tolist())\n",
    "\n",
    "merged_flight_data_df['FlightStartTime'] = merged_flight_data_df['FlightStartTime_y'].combine_first(merged_flight_data_df['FlightStartTime'])\n",
    "merged_flight_data_df['FlightEndTime'] = merged_flight_data_df['FlightEndTime_y'].combine_first(merged_flight_data_df['FlightEndTime'])\n",
    "# Now I have 4 dataframes, merged_inflight_parts_df, combined_rma_df, merged_flight_data_df, mtbf_df\n",
    "\n",
    "# Print date range\n",
    "print(\"\\nDate Range for Dataframes:\")\n",
    "print(merged_flight_data_df['FlightStartTime'].min(), \"to\", merged_flight_data_df['FlightEndTime'].max())\n",
    "\n",
    "# Calculate flight duration\n",
    "merged_flight_data_df['FlightDuration'] = merged_flight_data_df['FlightEndTime'] - merged_flight_data_df['FlightStartTime']\n",
    "negative_duration = (merged_flight_data_df['FlightDuration'] < pd.Timedelta(0))\n",
    "merged_flight_data_df.loc[negative_duration, 'FlightDuration'] = pd.NaT\n",
    "\n",
    "print(\"Merged Flight Data Columns:\", merged_flight_data_df.columns.tolist())\n",
    "print(merged_flight_data_df.head())\n",
    "# print head starting in jan 1 2023\n",
    "print(merged_flight_data_df[merged_flight_data_df['FlightStartTime'] > '2023-01-01'].head())\n",
    "\n",
    "\n",
    "# Analyze RMA reasonings\n",
    "# Get all unique return reasons and their counts\n",
    "unique_return_reasons = combined_rma_df['return_reason'].unique()\n",
    "print(f\"\\nUnique Return Reasons ({len(unique_return_reasons)}):\")\n",
    "return_reason_counts = combined_rma_df['return_reason'].value_counts()\n",
    "print(return_reason_counts)\n",
    "\n",
    "def categorize_aerospace_return_reason(reason):\n",
    "    if pd.isna(reason):\n",
    "        return 'Unknown'\n",
    "        \n",
    "    reason_str = str(reason).upper().strip()\n",
    "    \n",
    "    # V-Code classification (common aerospace fault codes)\n",
    "    if re.match(r'^V\\d+$', reason_str) or reason_str.startswith('|V') or re.search(r'V\\d+,\\s*V\\d+', reason_str):\n",
    "        return 'V_Code'\n",
    "        \n",
    "    # E-Code classification\n",
    "    if re.match(r'^E\\d+$', reason_str) or reason_str.startswith('|E') or 'E17-MB' in reason_str:\n",
    "        return 'E_Code'\n",
    "    \n",
    "    # No Fault Found / Could Not Confirm\n",
    "    if reason_str in ['NFF', '|NFF|'] or 'NO FAULT FOUND' in reason_str:\n",
    "        return 'No_Fault_Found'\n",
    "        \n",
    "    if reason_str in ['CNC', '|CNC|'] or 'COULD NOT CONFIRM' in reason_str or 'CANNOT CONFIRM' in reason_str:\n",
    "        return 'Could_Not_Confirm'\n",
    "    \n",
    "    # Hardware issues\n",
    "    if 'PHYSICAL DAMAGE' in reason_str or 'BROKEN' in reason_str or 'CRACK' in reason_str or 'SCRATCH' in reason_str:\n",
    "        return 'Physical_Damage'\n",
    "        \n",
    "    if 'DISPLAY' in reason_str or 'SCREEN' in reason_str or 'BLANK' in reason_str or 'BLACK SCREEN' in reason_str:\n",
    "        return 'Display_Issue'\n",
    "        \n",
    "    if 'TOUCH' in reason_str or 'PHANTOM TOUCH' in reason_str:\n",
    "        return 'Touch_Screen_Issue'\n",
    "        \n",
    "    if 'POWER' in reason_str or 'NO POWER' in reason_str or 'NOT POWER' in reason_str or 'WILL NOT TURN ON' in reason_str:\n",
    "        return 'Power_Issue'\n",
    "        \n",
    "    if 'BOOT' in reason_str or 'NO START' in reason_str or 'WILL NOT BOOT' in reason_str:\n",
    "        return 'Boot_Issue'\n",
    "        \n",
    "    if 'AUDIO' in reason_str or 'JACK' in reason_str:\n",
    "        return 'Audio_Issue'\n",
    "        \n",
    "    if 'USB' in reason_str or 'RJU USB' in reason_str:\n",
    "        return 'USB_Issue'\n",
    "    \n",
    "    if 'ETHERNET' in reason_str or 'NETWORK' in reason_str or 'CONNECT' in reason_str:\n",
    "        return 'Network_Issue'\n",
    "        \n",
    "    # Software issues\n",
    "    if 'SOFTWARE' in reason_str or 'CORRUPT' in reason_str or 'SW' in reason_str or 'FIRMWARE' in reason_str:\n",
    "        return 'Software_Issue'\n",
    "        \n",
    "    # Common specific failures\n",
    "    if 'MEZZ' in reason_str or 'BOARD' in reason_str or 'PCB' in reason_str or 'MAIN BOARD' in reason_str:\n",
    "        return 'Board_Failure'\n",
    "        \n",
    "    if 'BATTERY' in reason_str:\n",
    "        return 'Battery_Issue'\n",
    "        \n",
    "    if 'FAN' in reason_str:\n",
    "        return 'Fan_Issue'\n",
    "        \n",
    "    if 'CRYPTO' in reason_str or 'SECURITY' in reason_str:\n",
    "        return 'Security_Component_Issue'\n",
    "        \n",
    "    if 'TEST' in reason_str or 'CERTIFICATION' in reason_str or 'RECERTIF' in reason_str:\n",
    "        return 'Certification_Test'\n",
    "        \n",
    "    # Catch-all categories\n",
    "    if 'INOP' in reason_str or 'FAIL' in reason_str or 'FAULT' in reason_str:\n",
    "        return 'General_Inoperative'\n",
    "        \n",
    "    if reason_str == 'OTHER' or reason_str == 'V10':\n",
    "        return 'Other'\n",
    "        \n",
    "    # Numeric codes and special codes\n",
    "    if re.match(r'^\\d+-\\d+-\\d+-\\d+$', reason_str) or re.match(r'^SPB-', reason_str):\n",
    "        return 'Special_Code'\n",
    "        \n",
    "    # Anything else\n",
    "    return 'Miscellaneous'\n",
    "\n",
    "# Apply the categorization to create a new column\n",
    "combined_rma_df['return_reason_category'] = combined_rma_df['return_reason'].apply(categorize_aerospace_return_reason)\n",
    "\n",
    "# Standardize part numbers\n",
    "merged_inflight_parts_df['PartNumber'] = merged_inflight_parts_df['PartNumber'].str.upper()\n",
    "combined_rma_df['part_number'] = combined_rma_df['part_number'].str.upper()\n",
    "mtbf_df['PartNumber'] = mtbf_df['PartNumber'].str.upper()\n",
    "\n",
    "# Calculate Repairs duration\n",
    "combined_rma_df['repair_duration'] = combined_rma_df['ship_date'] - combined_rma_df['received_date']\n",
    "# Remove negative repair durations\n",
    "negative_duration = (combined_rma_df['repair_duration'] < pd.Timedelta(0))\n",
    "combined_rma_df.loc[negative_duration, 'repair_duration'] = pd.NaT\n",
    "\n",
    "# Handle Outlier data for all of the dataframes\n",
    "# Calculate the z-score for the duration columns\n",
    "def remove_outliers_zscore(df, column, threshold=3):\n",
    "    # Remove outliers using z-score method\n",
    "    if df[column].dtype == 'timedelta64[ns]':\n",
    "        # Convert timedelta to seconds for z-score calculation\n",
    "        seconds = df[column].dt.total_seconds()\n",
    "        z_scores = zscore(seconds, nan_policy='omit')\n",
    "    else:\n",
    "        z_scores = zscore(df[column], nan_policy='omit')\n",
    "    \n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    filtered_entries = (abs_z_scores < threshold)\n",
    "    return df[filtered_entries]\n",
    "\n",
    "# Check data types and fix repair_duration if needed\n",
    "print(f\"repair_duration dtype: {combined_rma_df['repair_duration'].dtype}\")\n",
    "\n",
    "# Convert repair_duration to timedelta\n",
    "if not pd.api.types.is_timedelta64_dtype(combined_rma_df['repair_duration']):\n",
    "    try:\n",
    "        if combined_rma_df['repair_duration'].dtype == 'object' or pd.api.types.is_string_dtype(combined_rma_df['repair_duration']):\n",
    "            combined_rma_df['repair_duration'] = pd.to_timedelta(combined_rma_df['repair_duration'])\n",
    "        else:\n",
    "            if combined_rma_df['repair_duration'].median() < 1000:\n",
    "                combined_rma_df['repair_duration'] = pd.to_timedelta(combined_rma_df['repair_duration'], unit='D')\n",
    "                print(\"Converted numeric repair_duration to timedelta (assuming days)\")\n",
    "            else:\n",
    "                combined_rma_df['repair_duration'] = pd.to_timedelta(combined_rma_df['repair_duration'], unit='s')\n",
    "                print(\"Converted numeric repair_duration to timedelta (assuming seconds)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting repair_duration: {e}\")\n",
    "        combined_rma_df['repair_duration_days'] = np.nan\n",
    "        \n",
    "        # Try to calculate repair duration in days from received_date and ship_date\n",
    "        mask = ~combined_rma_df['received_date'].isna() & ~combined_rma_df['ship_date'].isna()\n",
    "        if mask.any():\n",
    "            duration = (combined_rma_df.loc[mask, 'ship_date'] - combined_rma_df.loc[mask, 'received_date']).dt.total_seconds() / (24*60*60)\n",
    "            combined_rma_df.loc[mask, 'repair_duration_days'] = duration\n",
    "\n",
    "## Handle Outliers in repair duration\n",
    "print(\"\\nHandling outliers in repair duration...\")\n",
    "# Determine which repair duration column to use\n",
    "if pd.api.types.is_timedelta64_dtype(combined_rma_df['repair_duration']):\n",
    "    repair_col = 'repair_duration'\n",
    "    repair_duration_df = combined_rma_df.dropna(subset=[repair_col])\n",
    "    repair_duration_days = repair_duration_df[repair_col].dt.total_seconds() / (24 * 60 * 60)\n",
    "    print(\"Using repair_duration column (timedelta)\")\n",
    "elif 'repair_duration_days' in combined_rma_df.columns:\n",
    "    repair_col = 'repair_duration_days'\n",
    "    repair_duration_df = combined_rma_df.dropna(subset=[repair_col])\n",
    "    repair_duration_days = repair_duration_df[repair_col]\n",
    "    print(\"Using repair_duration_days column (float)\")\n",
    "else:\n",
    "    print(\"No valid repair duration column available\")\n",
    "    repair_duration_days = None\n",
    "\n",
    "if repair_duration_days is not None:\n",
    "    # Calculate statistics before handling outliers\n",
    "    repair_mean_before = repair_duration_days.mean()\n",
    "    repair_median_before = repair_duration_days.median()\n",
    "    repair_max_before = repair_duration_days.max()\n",
    "    repair_min_before = repair_duration_days.min()\n",
    "\n",
    "    print(f\"Repair duration before (in days): mean={repair_mean_before:.2f}, median={repair_median_before:.2f}, min={repair_min_before:.2f}, max={repair_max_before:.2f}\")\n",
    "\n",
    "    # Apply outlier handling using the z-score method on days\n",
    "    z_scores = zscore(repair_duration_days, nan_policy='omit')\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    threshold = 3\n",
    "\n",
    "    # Identify outliers\n",
    "    outliers = abs_z_scores > threshold\n",
    "    outlier_indices = repair_duration_df.index[outliers]\n",
    "\n",
    "    if len(outlier_indices) > 0:\n",
    "        # Calculate the threshold in days\n",
    "        mean_val = repair_duration_days.mean()\n",
    "        std_val = repair_duration_days.std()\n",
    "        upper_limit_days = mean_val + threshold * std_val\n",
    "        lower_limit_days = max(0, mean_val - threshold * std_val)  # Ensure non-negative\n",
    "\n",
    "        print(f\"Identified {len(outlier_indices)} outliers\")\n",
    "        print(f\"Upper limit: {upper_limit_days:.2f} days, Lower limit: {lower_limit_days:.2f} days\")\n",
    "\n",
    "        if repair_col == 'repair_duration':\n",
    "            # Convert back to timedelta\n",
    "            upper_limit_td = pd.Timedelta(days=upper_limit_days)\n",
    "            lower_limit_td = pd.Timedelta(days=lower_limit_days)\n",
    "            \n",
    "            # Apply caps to original dataframe\n",
    "            combined_rma_df.loc[combined_rma_df.index.isin(outlier_indices) & (combined_rma_df[repair_col] > upper_limit_td), repair_col] = upper_limit_td\n",
    "            combined_rma_df.loc[combined_rma_df.index.isin(outlier_indices) & (combined_rma_df[repair_col] < lower_limit_td), repair_col] = lower_limit_td\n",
    "        else:\n",
    "            # Apply caps to days column\n",
    "            combined_rma_df.loc[combined_rma_df.index.isin(outlier_indices) & (combined_rma_df[repair_col] > upper_limit_days), repair_col] = upper_limit_days\n",
    "            combined_rma_df.loc[combined_rma_df.index.isin(outlier_indices) & (combined_rma_df[repair_col] < lower_limit_days), repair_col] = lower_limit_days\n",
    "\n",
    "        # Calculate statistics after handling outliers\n",
    "        if repair_col == 'repair_duration':\n",
    "            repair_duration_df = combined_rma_df.dropna(subset=[repair_col])\n",
    "            repair_duration_days_after = repair_duration_df[repair_col].dt.total_seconds() / (24 * 60 * 60)\n",
    "        else:\n",
    "            repair_duration_df = combined_rma_df.dropna(subset=[repair_col])\n",
    "            repair_duration_days_after = repair_duration_df[repair_col]\n",
    "            \n",
    "        repair_mean_after = repair_duration_days_after.mean()\n",
    "        repair_median_after = repair_duration_days_after.median()\n",
    "        repair_max_after = repair_duration_days_after.max()\n",
    "        repair_min_after = repair_duration_days_after.min()\n",
    "\n",
    "        print(f\"Repair duration after (in days): mean={repair_mean_after:.2f}, median={repair_median_after:.2f}, min={repair_min_after:.2f}, max={repair_max_after:.2f}\")\n",
    "        print(f\"Number of outliers capped: {len(outlier_indices)}\")\n",
    "    else:\n",
    "        print(\"No outliers found in repair duration data\")\n",
    "\n",
    "# Handle outliers in flight duration data\n",
    "print(\"\\nHandling outliers in flight duration...\")\n",
    "flight_duration_df = merged_flight_data_df.dropna(subset=['FlightDuration'])\n",
    "\n",
    "# Convert to hours for easier interpretation\n",
    "flight_duration_hours = flight_duration_df['FlightDuration'].dt.total_seconds() / 3600\n",
    "\n",
    "flight_mean_before = flight_duration_hours.mean()\n",
    "flight_median_before = flight_duration_hours.median()\n",
    "flight_max_before = flight_duration_hours.max()\n",
    "flight_min_before = flight_duration_hours.min()\n",
    "\n",
    "# Apply outlier handling\n",
    "z_scores = zscore(flight_duration_hours, nan_policy='omit')\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "threshold = 3\n",
    "\n",
    "# Identify which values to cap\n",
    "outliers = abs_z_scores > threshold\n",
    "outlier_indices = flight_duration_df.index[outliers]\n",
    "\n",
    "if len(outlier_indices) > 0:\n",
    "    # Calculate the threshold values in hours\n",
    "    mean_val = flight_duration_hours.mean()\n",
    "    std_val = flight_duration_hours.std()\n",
    "    upper_limit_hours = mean_val + threshold * std_val\n",
    "    lower_limit_hours = max(0, mean_val - threshold * std_val)  # Ensure non-negative\n",
    "\n",
    "    # Convert back to timedelta for capping\n",
    "    upper_limit_td = pd.Timedelta(hours=upper_limit_hours)\n",
    "    lower_limit_td = pd.Timedelta(hours=lower_limit_hours)\n",
    "\n",
    "    # Apply the caps to the original dataframe\n",
    "    merged_flight_data_df.loc[merged_flight_data_df.index.isin(outlier_indices) & (merged_flight_data_df['FlightDuration'] > upper_limit_td), 'FlightDuration'] = upper_limit_td\n",
    "    merged_flight_data_df.loc[merged_flight_data_df.index.isin(outlier_indices) & (merged_flight_data_df['FlightDuration'] < lower_limit_td), 'FlightDuration'] = lower_limit_td\n",
    "\n",
    "flight_duration_df = merged_flight_data_df.dropna(subset=['FlightDuration'])\n",
    "flight_duration_hours_after = flight_duration_df['FlightDuration'].dt.total_seconds() / 3600\n",
    "flight_mean_after = flight_duration_hours_after.mean()\n",
    "flight_median_after = flight_duration_hours_after.median()\n",
    "flight_max_after = flight_duration_hours_after.max()\n",
    "flight_min_after = flight_duration_hours_after.min()\n",
    "\n",
    "print(f\"Flight duration after (in hours): mean={flight_mean_after:.2f}, median={flight_median_after:.2f}, min={flight_min_after:.2f}, max={flight_max_after:.2f}\")\n",
    "print(f\"Number of outliers capped: {len(outlier_indices)}\")\n",
    "\n",
    "# Save dataframes as parquet files\n",
    "output_dir = os.path.join('private', 'data', 'transformed')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Data Fixes\n",
    "combined_rma_df['customer'] = combined_rma_df['customer'].astype(str)\n",
    "\n",
    "# Check the variance of all columns in merged_flight_data_df, no columns should have 0 variance...\n",
    "print(\"\\nChecking variance of all columns in merged_flight_data_df:\")\n",
    "numeric_columns = merged_flight_data_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "print(\"merged_flight_data_df Numeric columns:\", numeric_columns)\n",
    "print(\"Variance of merged_flight_data_df numeric columns:\")\n",
    "print(merged_flight_data_df[numeric_columns].var())\n",
    "\n",
    "# Save the dataframes\n",
    "merged_inflight_parts_df.to_parquet(os.path.join(output_dir, 'merged_inflight_parts.parquet'))\n",
    "combined_rma_df.to_parquet(os.path.join(output_dir, 'combined_rma.parquet'))\n",
    "merged_flight_data_df.to_parquet(os.path.join(output_dir, 'merged_flight_data.parquet'))\n",
    "mtbf_df.to_parquet(os.path.join(output_dir, 'mtbf.parquet'))\n",
    "\n",
    "print(\"\\nDataframes saved as parquet files\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aerospace Component Failure Prediction Model - Data Extraction\n",
    "#\n",
    "# NOTE: This notebook contains anonymized/obfuscated data for public demonstration.\n",
    "# Sensitive company information, customer details, and personal identifiers \n",
    "# have been removed or replaced with generic placeholders.\n",
    "#\n",
    "# Environment variables are used for database connections and API endpoints\n",
    "# to protect sensitive infrastructure details.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DATABASE_HOST = os.getenv('DATABASE_HOST')\n",
    "DATABASE_NAME = os.getenv('DATABASE_NAME')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_PORT = os.getenv('DB_PORT', '5432')\n",
    "\n",
    "# Database connection and query\n",
    "connection = psycopg2.connect(\n",
    "    host=DATABASE_HOST,\n",
    "    database=DATABASE_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    "    port=DB_PORT\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        aircraft_groups.program_id,\n",
    "        aircraft_groups.location_type,\n",
    "        service_start_date,\n",
    "        equipment_details.equipment_entry_id,\n",
    "        entry_type,\n",
    "        part_number,\n",
    "        description,\n",
    "        delivery_equipment_entry_id,\n",
    "        delivery_quantity,\n",
    "        group_type,\n",
    "        group_number,\n",
    "        group_name,\n",
    "        asset_id,\n",
    "        status,\n",
    "        customer_id,\n",
    "        install_location\n",
    "    FROM equipment_installations\n",
    "    LEFT JOIN equipment_details ON equipment_installations.id = equipment_details.equipment_entry_id\n",
    "    LEFT JOIN delivery_records ON delivery_records.equipment_entry_id = equipment_installations.id\n",
    "    LEFT JOIN delivery_allocation_detail ON delivery_allocation_detail.delivery_equipment_entry_id = delivery_records.id\n",
    "    LEFT JOIN delivery ON delivery_records.delivery_id = delivery.id\n",
    "    LEFT JOIN aircraft_groups ON delivery.shipset_id = aircraft_groups.id\n",
    "    LEFT JOIN shipset_detail ON aircraft_groups.id = shipset_detail.shipset_id\n",
    "    LEFT JOIN aircraft_registry ON aircraft_groups.id = aircraft_registry.id\n",
    "    LEFT JOIN install_location_detail ON equipment_installations.install_location_id = install_location_detail.install_location_id\n",
    "    WHERE entry_type = 'Equipment'\n",
    "        AND group_type = 'aircraft_groups'\n",
    "        AND status IS NOT NULL\n",
    "        AND status <> 'Cancelled'\n",
    "        AND equipment_details.inactive_date IS NULL\n",
    "        AND delivery_allocation_detail.inactive_date IS NULL\n",
    "        AND shipset_detail.inactive_date IS NULL\n",
    "        AND install_location_detail.inactive_date IS NULL\n",
    "    ORDER BY status\n",
    "        \"\"\"\n",
    "\n",
    "# Execute query and load directly to pandas\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Rename columns to match desired schema\n",
    "column_mapping = {\n",
    "    'program_id': 'program_id',\n",
    "    'location_type': 'location_type',\n",
    "    'service_start_date': 'EntryIntoServiceDate',\n",
    "    'equipment_entry_id': 'equipment_entry_id',\n",
    "    'entry_type': 'EntryType',\n",
    "    'part_number': 'PartNumber',\n",
    "    'description': 'Description',\n",
    "    'delivery_equipment_entry_id': 'delivery_equipment_entry_id',\n",
    "    'delivery_quantity': 'DeliveryQuantity',\n",
    "    'group_type': 'group_type',\n",
    "    'group_number': 'group_number',\n",
    "    'group_name': 'group_name',\n",
    "    'asset_id': 'Tail',\n",
    "    'status': 'status',\n",
    "    'customer_id': 'customer_id',\n",
    "    'install_location': 'InstallLocation'\n",
    "}\n",
    "df = df.rename(columns=column_mapping)\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(data_dir, 'partslist.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather RMA Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import msal\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 1. Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "AUTH_ENDPOINT = os.getenv('AUTH_ENDPOINT')\n",
    "CLIENT_ID = os.getenv('CLIENT_ID')\n",
    "CLIENT_SECRET = os.getenv('CLIENT_SECRET')\n",
    "SCOPE = os.getenv('SCOPE')\n",
    "API_BASE_URL = os.getenv('API_BASE_URL')\n",
    "\n",
    "# 2. Configure auth settings\n",
    "config = {\n",
    "    \"authority\": AUTH_ENDPOINT,\n",
    "    \"client_id\": CLIENT_ID,\n",
    "    \"client_secret\": CLIENT_SECRET,\n",
    "    \"scope\": [SCOPE]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # logger.info(\"Initializing MSAL application\")\n",
    "    app = msal.ConfidentialClientApplication(\n",
    "        config[\"client_id\"],\n",
    "        authority=config[\"authority\"],\n",
    "        client_credential=config[\"client_secret\"]\n",
    "    )\n",
    "\n",
    "    # logger.info(\"Acquiring token\")\n",
    "    result = app.acquire_token_for_client(scopes=config[\"scope\"])\n",
    "    \n",
    "    if \"access_token\" in result:\n",
    "        # logger.info(\"Token acquired successfully\")\n",
    "        access_token = result[\"access_token\"]\n",
    "    else:\n",
    "        logger.error(f\"Failed to acquire token. Error: {result.get('error')}\")\n",
    "        logger.error(f\"Error description: {result.get('error_description')}\")\n",
    "        raise Exception(\"Failed to acquire token\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    all_records = []\n",
    "    next_link = API_BASE_URL + 'service_requests'\n",
    "\n",
    "    while next_link:\n",
    "        try:\n",
    "            logger.info(f\"[{datetime.now()}] Fetching data from: {next_link}\")\n",
    "            response = requests.get(next_link, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            records = data.get(\"value\", [])\n",
    "            current_batch_size = len(records)\n",
    "            all_records.extend(records)\n",
    "            \n",
    "            logger.info(f\"[{datetime.now()}] Batch size: {current_batch_size}\")\n",
    "            logger.info(f\"[{datetime.now()}] Current batch records: {len(records)}\")\n",
    "            logger.info(f\"[{datetime.now()}] Total records so far: {len(all_records)}\")\n",
    "            \n",
    "            # Get next page link if exists\n",
    "            next_link = data.get('@odata.nextLink')\n",
    "            if next_link:\n",
    "                logger.info(f\"[{datetime.now()}] Next link found: {next_link}\")\n",
    "            else:\n",
    "                logger.info(f\"[{datetime.now()}] No more pages to fetch\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[{datetime.now()}] Error fetching data: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "        logger.info(f\"Total records retrieved: {len(all_records)}\")\n",
    "        \n",
    "        # Convert to DataFrame and save to CSV\n",
    "        rma_df = pd.DataFrame(all_records)\n",
    "\n",
    "        columns_to_keep = [\n",
    "        'ServiceRequestId', 'DeliveryLocation_LocationId', 'Message', 'QuotationAmount',\n",
    "        'ProjId', 'request_type', 'CallDueDateTime', 'QuotationAmountType',\n",
    "        'Subject', 'ComplaintId',\n",
    "        'ProjectIntegrationId', 'CauseId', 'Progress', 'SLARefDateTime',\n",
    "        'ServiceObjectId', 'InvoiceName', 'CallActionDateTime', 'SolutionId',\n",
    "        'RepairWithAccessories', 'CallStatusId', 'TaskDueDateTime',\n",
    "        'Solution', 'InternalMsg', 'CertificateType', 'FinalDocDate',\n",
    "        'WarrantyEndDate', 'OptionalRevision', 'VisualInspectionResult',\n",
    "        'RepairLocation', 'ExpectedShipDate', 'IsBroadbandUnit',\n",
    "        'portal_status', 'AircraftType', 'RemovedDate', 'FlightHours',\n",
    "        'AircraftTailSerialNumber', 'ReturnReason', 'ActualShipDate',\n",
    "        'FinalDocRevision', 'QuoteApprovedDate', 'repair_type',\n",
    "        'AircraftTailNumber', 'QuoteSentDate', 'WarrantyStartDate',\n",
    "        'UnitReceivedDate'\n",
    "    ]\n",
    "\n",
    "    rma_df = rma_df[columns_to_keep]\n",
    "\n",
    "    # Get part data for each rma order\n",
    "    all_so_records = []\n",
    "    next_link = API_BASE_URL + 'service_objects'\n",
    "\n",
    "    while next_link:\n",
    "        try:\n",
    "            logger.info(f\"[{datetime.now()}] Fetching data from: {next_link}\")\n",
    "            response = requests.get(next_link, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            records = data.get(\"value\", [])\n",
    "            current_batch_size = len(records)\n",
    "            all_so_records.extend(records)\n",
    "            \n",
    "            logger.info(f\"[{datetime.now()}] Batch size: {current_batch_size}\")\n",
    "            logger.info(f\"[{datetime.now()}] Current batch records: {len(records)}\")\n",
    "            logger.info(f\"[{datetime.now()}] Total records so far: {len(all_so_records)}\")\n",
    "            \n",
    "            # Get next page link if exists\n",
    "            next_link = data.get('@odata.nextLink')\n",
    "            if next_link:\n",
    "                logger.info(f\"[{datetime.now()}] Next link found: {next_link}\")\n",
    "            else:\n",
    "                logger.info(f\"[{datetime.now()}] No more pages to fetch\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[{datetime.now()}] Error fetching data: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    logger.info(f\"Total records retrieved: {len(all_so_records)}\")\n",
    "\n",
    "    so_df = pd.DataFrame(all_so_records)\n",
    "\n",
    "    # keeps only the columns we need - removing sensitive contact info\n",
    "    so_columns_to_keep = [\n",
    "        'ServiceObjectId',\n",
    "        'Status',\n",
    "        'Description',\n",
    "        'ItemId',\n",
    "        'MachineTypeId',\n",
    "        'ProjId',\n",
    "        'SerialId',\n",
    "        'WarrantyStartDate_Cust',\n",
    "        'RepairStatusRefRecId',\n",
    "        'RepairStatus',\n",
    "        'RepairStatusRefTableId'\n",
    "    ]\n",
    "    \n",
    "    so_df = so_df[so_columns_to_keep]\n",
    "\n",
    "    # Merge RMA orders and Service Objects\n",
    "    # Get overlapping columns\n",
    "    common_cols = set(rma_df.columns) & set(so_df.columns)\n",
    "    print(\"Common columns:\", common_cols)\n",
    "\n",
    "    # Merge with suffix handling\n",
    "    merged_df = pd.merge(\n",
    "        rma_df,\n",
    "        so_df,\n",
    "        on='ServiceObjectId',\n",
    "        how='left',\n",
    "        suffixes=('_rma', '_service')  # Clear suffixes to avoid conflicts\n",
    "    )\n",
    "\n",
    "    # Drop duplicate columns if needed\n",
    "    duplicate_cols = [col for col in merged_df.columns if col.endswith('_service')]\n",
    "    merged_df = merged_df.drop(columns=duplicate_cols)\n",
    "\n",
    "    # Print the number of columns and rows\n",
    "    print(\"\\nColumns in the dataset:\")\n",
    "    print(merged_df.columns.tolist())\n",
    "    print(f\"Rows: {len(merged_df)}\")\n",
    "    print(f\"\\nDataset saved to {output_path}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    data_dir = os.path.join('private', 'data', 'raw')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    output_path = os.path.join(data_dir, 'merged_rmaorders.csv')\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Product information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DATABASE_HOST = os.getenv('DATABASE_HOST')\n",
    "DATABASE_NAME = os.getenv('DATABASE_NAME')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_PORT = os.getenv('DB_PORT', '5432')\n",
    "\n",
    "# Database connection and query\n",
    "connection = psycopg2.connect(\n",
    "    host=DATABASE_HOST,\n",
    "    database=DATABASE_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    "    port=DB_PORT\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    -- Basic identification\n",
    "    product.part_number,\n",
    "    productinformation.productname,\n",
    "    product.producttype,\n",
    "    \n",
    "    -- Product categorization\n",
    "    productinformation.productfamily,\n",
    "    productpcddetails.productgroup,\n",
    "    productinformation.definitionlevel,\n",
    "    productinformation.lrucategoryclass,\n",
    "    productpcddetails.conformitydescription,\n",
    "    \n",
    "    -- Manufacturing information\n",
    "    productinformation.manufacturingtype,\n",
    "    productinformation.repairtype,\n",
    "    productinformation.pmastatus,\n",
    "    \n",
    "    -- Production status\n",
    "    productinformation.newdesignrecommendation,\n",
    "    productinformation.useequipmentfamily,\n",
    "    \n",
    "    -- Technical specifications\n",
    "    productinputpower.operatingmode,\n",
    "    productinputpower.maximuminputpower,\n",
    "    productlruspecifics.internalstorage,\n",
    "    productlruspecifics.resolution,\n",
    "    productlruspecifics.frontpanelusb,\n",
    "    productlruspecifics.oneethernetport,\n",
    "    productlruspecifics.functionalspec,\n",
    "    \n",
    "    -- Lifecycle information\n",
    "    productlifecyclephase.lifecyclephase,\n",
    "    productmilestone.milestone,\n",
    "    productmilestone.milestonedate,\n",
    "    productmilestone.milestonestatus,\n",
    "    productmilestone.actualdate,\n",
    "    productmilestone.notes,\n",
    "    \n",
    "    -- Manager information\n",
    "    productinformation.product_manager,\n",
    "    \n",
    "    -- Performance/reliability metrics\n",
    "    productreliability.target_reliability,\n",
    "    productpower.engineering AS power_engineering,\n",
    "    productpower.marketing AS power_marketing,\n",
    "    productpower.contractual AS power_contractual,\n",
    "    productweight.engineering AS weight_engineering,\n",
    "    productweight.marketing AS weight_marketing,\n",
    "    productweight.contractual AS weight_contractual\n",
    "FROM product\n",
    "LEFT JOIN productbase ON product.part_number = productbase.part_number\n",
    "LEFT JOIN productinformation ON product.part_number = productinformation.part_number\n",
    "LEFT JOIN productinputpower ON product.part_number = productinputpower.part_number\n",
    "LEFT JOIN productlifecyclephase ON product.part_number = productlifecyclephase.part_number\n",
    "LEFT JOIN productlruspecifics ON product.part_number = productlruspecifics.part_number\n",
    "LEFT JOIN productmilestone ON product.part_number = productmilestone.part_number\n",
    "LEFT JOIN productpcddetails ON product.part_number = productpcddetails.part_number\n",
    "LEFT JOIN productpower ON product.part_number = productpower.part_number \n",
    "LEFT JOIN productreliability ON product.part_number = productreliability.part_number \n",
    "LEFT JOIN productweight ON product.part_number = productweight.part_number\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and load directly to pandas\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(data_dir, 'productinfo.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Flight Events Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import pyodbc\n",
    "import os\n",
    "\n",
    "# Load ENV Variables\n",
    "load_dotenv()\n",
    "SQLSERVER_HOST = os.getenv('SQLSERVER_HOST')\n",
    "SQLSERVER_DB = os.getenv('SQLSERVER_DB')\n",
    "SQLSERVER_USER = os.getenv('SQLSERVER_USER')\n",
    "SQLSERVER_PASSWORD = os.getenv('SQLSERVER_PASSWORD')\n",
    "SQLSERVER_PORT = os.getenv('SQLSERVER_PORT', '1433')\n",
    "\n",
    "# Database connection and query\n",
    "connection = pyodbc.connect(\n",
    "    f'DRIVER=ODBC Driver 17 for SQL Server;SERVER={SQLSERVER_HOST},{SQLSERVER_PORT};DATABASE={SQLSERVER_DB};UID={SQLSERVER_USER};PWD={SQLSERVER_PASSWORD}'\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        FlightResetsID,\n",
    "        FlightID,\n",
    "        Airline,\n",
    "        DepartureCode,\n",
    "        ArrivalCode,\n",
    "        FlightNumber,\n",
    "        asset_id,\n",
    "        FlightStartTime,\n",
    "        FlightEndTime,\n",
    "        FlightDuration,\n",
    "        Class,\n",
    "        AircraftType,\n",
    "        SeatResets,\n",
    "        RawResets,\n",
    "        Processed\n",
    "    FROM [dbo].[flight_resets]\n",
    "    WHERE customer_type = 'Production' \n",
    "    AND Airline NOT IN ('DEMO', 'TRAINING')\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(data_dir, 'flightresets.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather MTBF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import pyodbc\n",
    "import os\n",
    "\n",
    "# Load ENV Variables\n",
    "load_dotenv()\n",
    "SQLSERVER_HOST = os.getenv('SQLSERVER_HOST')\n",
    "SQLSERVER_DB = os.getenv('SQLSERVER_DB')\n",
    "SQLSERVER_USER = os.getenv('SQLSERVER_USER')\n",
    "SQLSERVER_PASSWORD = os.getenv('SQLSERVER_PASSWORD')\n",
    "SQLSERVER_PORT = os.getenv('SQLSERVER_PORT', '1433')\n",
    "\n",
    "# Database connection and query\n",
    "connection = pyodbc.connect(\n",
    "    f'DRIVER=ODBC Driver 17 for SQL Server;SERVER={SQLSERVER_HOST},{SQLSERVER_PORT};DATABASE={SQLSERVER_DB};UID={SQLSERVER_USER};PWD={SQLSERVER_PASSWORD}'\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        MTBFID,\n",
    "        Airline,\n",
    "        PartNumber,\n",
    "        PartGroup,\n",
    "        DetailPartGroup,\n",
    "        Description,\n",
    "        Month,\n",
    "        PoweredOnHours,\n",
    "        FlightHours,\n",
    "        Failures,\n",
    "        NFF,\n",
    "        target_reliability,\n",
    "        InsertDate,\n",
    "        UpdateDate,\n",
    "        UpdateCount\n",
    "    FROM [dbo].[mtbf_data]\n",
    "    WHERE customer_type = 'Production'\n",
    "    \"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(data_dir, 'mtbf.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Flight Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import pyodbc\n",
    "import os\n",
    "\n",
    "# Load ENV Variables\n",
    "load_dotenv()\n",
    "SQLSERVER_HOST = os.getenv('SQLSERVER_HOST')\n",
    "SQLSERVER_DB = os.getenv('SQLSERVER_DB')\n",
    "SQLSERVER_USER = os.getenv('SQLSERVER_USER')\n",
    "SQLSERVER_PASSWORD = os.getenv('SQLSERVER_PASSWORD')\n",
    "SQLSERVER_PORT = os.getenv('SQLSERVER_PORT', '1433')\n",
    "\n",
    "# Database connection and query\n",
    "connection = pyodbc.connect(\n",
    "    f'DRIVER=ODBC Driver 17 for SQL Server;SERVER={SQLSERVER_HOST},{SQLSERVER_PORT};DATABASE={SQLSERVER_DB};UID={SQLSERVER_USER};PWD={SQLSERVER_PASSWORD}'\n",
    ")\n",
    "\n",
    "query = \"\"\" \n",
    "         SELECT\n",
    "         FlightID,\n",
    "         Airline,\n",
    "         DepartureCode,\n",
    "          ArrivalCode,\n",
    "         FlightStartTime,\n",
    "         FlightEndTime,\n",
    "         asset_id,\n",
    "         FlightNumber,\n",
    "         AircraftType,\n",
    "         InsertDate AS FileCreatedTime,\n",
    "         InsertDate\n",
    " FROM [dbo].[Flights]\n",
    "\"\"\"\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(data_dir, 'flights.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Passenger Count per flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import pyodbc\n",
    "import os\n",
    "\n",
    "# Load ENV Variables\n",
    "load_dotenv()\n",
    "SQLSERVER_HOST = os.getenv('SQLSERVER_HOST')\n",
    "SQLSERVER_DB = os.getenv('SQLSERVER_DB')\n",
    "SQLSERVER_USER = os.getenv('SQLSERVER_USER')\n",
    "SQLSERVER_PASSWORD = os.getenv('SQLSERVER_PASSWORD')\n",
    "SQLSERVER_PORT = os.getenv('SQLSERVER_PORT', '1433')\n",
    "\n",
    "# Database connection and query\n",
    "connection = pyodbc.connect(\n",
    "    f'DRIVER=ODBC Driver 17 for SQL Server;SERVER={SQLSERVER_HOST},{SQLSERVER_PORT};DATABASE={SQLSERVER_DB};UID={SQLSERVER_USER};PWD={SQLSERVER_PASSWORD}'\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        AIMSID,\n",
    "        FlightID,\n",
    "        asset_id,\n",
    "        FlightNumber,\n",
    "        DepartureCode,\n",
    "        ArrivalCode,\n",
    "        FlightStartTime,\n",
    "        FlightEndTime,\n",
    "        BusinessClass,\n",
    "        EconomyClass,\n",
    "        TotalPassengers,\n",
    "        InsertDate,\n",
    "        UpdatedPaxActivity,\n",
    "        UpdatedPerPassengerRevenue\n",
    "    FROM [dbo].[PassengerCounts]\n",
    "    WHERE asset_id IS NOT NULL\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(data_dir, 'passenger_count.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Historical RMA Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import pyodbc\n",
    "import os\n",
    "\n",
    "# Load ENV Variables\n",
    "load_dotenv()\n",
    "SQLSERVER_HOST = os.getenv('SQLSERVER_HOST')\n",
    "SQLSERVER_DB = os.getenv('SQLSERVER_DB')\n",
    "SQLSERVER_USER = os.getenv('SQLSERVER_USER')\n",
    "SQLSERVER_PASSWORD = os.getenv('SQLSERVER_PASSWORD')\n",
    "SQLSERVER_PORT = os.getenv('SQLSERVER_PORT', '1433')\n",
    "\n",
    "# Database connection and query\n",
    "connection = pyodbc.connect(\n",
    "    f'DRIVER=ODBC Driver 17 for SQL Server;SERVER={SQLSERVER_HOST},{SQLSERVER_PORT};DATABASE={SQLSERVER_DB};UID={SQLSERVER_USER};PWD={SQLSERVER_PASSWORD}'\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        Customer,\n",
    "        RMA,\n",
    "        PN,\n",
    "        SN,\n",
    "        StatusDescription,\n",
    "        PartDescription,\n",
    "        LRUName,\n",
    "        ReceivedDate,\n",
    "        ReceivedAtPartner,\n",
    "        FaultCode,\n",
    "        ShipDate,\n",
    "        ServiceBulletinInfo,\n",
    "        ServiceBulletinNumber,\n",
    "        ServiceBulletin,\n",
    "        AlertCategoryCode,\n",
    "        InsertDate\n",
    "    FROM EXAMPLE_DWH.dbo.repair_records\n",
    "    WHERE PN IS NOT NULL\n",
    "    ORDER BY ReceivedDate DESC\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Add \"hist_\" prefix to the filename\n",
    "output_path = os.path.join(data_dir, 'hist_repair_rma.csv')\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Parts Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DATABASE_HOST = os.getenv('DATABASE_HOST')\n",
    "DATABASE_NAME = os.getenv('DATABASE_NAME')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_PORT = os.getenv('DB_PORT', '5432')\n",
    "\n",
    "# Database connection and query\n",
    "connection = psycopg2.connect(\n",
    "    host=DATABASE_HOST,\n",
    "    database=DATABASE_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    "    port=DB_PORT\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    -- Basic identification\n",
    "    product.part_number,\n",
    "    productinformation.productname,\n",
    "    product.producttype,\n",
    "    \n",
    "    -- Product categorization\n",
    "    productinformation.productfamily,\n",
    "    productpcddetails.productgroup,\n",
    "    productinformation.definitionlevel,\n",
    "    productinformation.lrucategoryclass,\n",
    "    productpcddetails.conformitydescription,\n",
    "    \n",
    "    -- Manufacturing information\n",
    "    productinformation.manufacturingtype,\n",
    "    productinformation.repairtype,\n",
    "    \n",
    "    -- Production status\n",
    "    productinformation.newdesignrecommendation,\n",
    "    productinformation.useequipmentfamily,\n",
    "    \n",
    "    -- Technical specifications\n",
    "    productinputpower.operatingmode,\n",
    "    productinputpower.maximuminputpower,\n",
    "    productlruspecifics.internalstorage,\n",
    "    productlruspecifics.resolution,\n",
    "    productlruspecifics.frontpanelusb,\n",
    "    productlruspecifics.oneethernetport,\n",
    "    productlruspecifics.functionalspec,\n",
    "    \n",
    "    -- Lifecycle information\n",
    "    productlifecyclephase.lifecyclephase,\n",
    "    productmilestone.milestone,\n",
    "    productmilestone.milestonedate,\n",
    "    productmilestone.milestonestatus,\n",
    "    productmilestone.actualdate,\n",
    "    productmilestone.notes,\n",
    "    \n",
    "    -- Manager information\n",
    "    productinformation.product_manager,\n",
    "    \n",
    "    -- Performance/reliability metrics\n",
    "    productreliability.target_reliability,\n",
    "    productpower.engineering AS power_engineering,\n",
    "    productpower.marketing AS power_marketing,\n",
    "    productpower.contractual AS power_contractual,\n",
    "    productweight.engineering AS weight_engineering,\n",
    "    productweight.marketing AS weight_marketing,\n",
    "    productweight.contractual AS weight_contractual\n",
    "FROM product\n",
    "LEFT JOIN productbase ON product.part_number = productbase.part_number\n",
    "LEFT JOIN productinformation ON product.part_number = productinformation.part_number\n",
    "LEFT JOIN productinputpower ON product.part_number = productinputpower.part_number\n",
    "LEFT JOIN productlifecyclephase ON product.part_number = productlifecyclephase.part_number\n",
    "LEFT JOIN productlruspecifics ON product.part_number = productlruspecifics.part_number\n",
    "LEFT JOIN productmilestone ON product.part_number = productmilestone.part_number\n",
    "LEFT JOIN productpcddetails ON product.part_number = productpcddetails.part_number\n",
    "LEFT JOIN productpower ON product.part_number = productpower.part_number \n",
    "LEFT JOIN productreliability ON product.part_number = productreliability.part_number \n",
    "LEFT JOIN productweight ON product.part_number = productweight.part_number\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and load directly to pandas\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(data_dir, 'productinfo.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "title": "Data Extraction Pipeline"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Parts List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_PORT = os.getenv('DB_PORT', '5432')\n",
    "\n",
    "# Database connection and query\n",
    "connection = psycopg2.connect(\n",
    "    host=DB_HOST,\n",
    "    database=DB_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    "    port=DB_PORT\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        shipset.programcode,\n",
    "        shipset.installlocationtype,\n",
    "        eisdate,\n",
    "        equipmententrydetail.equipmententryid,\n",
    "        entrytype,\n",
    "        partnumber,\n",
    "        description,\n",
    "        deliveryequipmententryid,\n",
    "        deliveryquantity,\n",
    "        shipsettype,\n",
    "        shipsetnumber,\n",
    "        shipsetname,\n",
    "        tailnumber,\n",
    "        servicestatus,\n",
    "        ownercompanyid,\n",
    "        installlocation\n",
    "    FROM equipmententry\n",
    "    LEFT JOIN equipmententrydetail ON equipmententry.id = equipmententrydetail.equipmententryid\n",
    "    LEFT JOIN deliveryequipmententry ON deliveryequipmententry.equipmententryid = equipmententry.id\n",
    "    LEFT JOIN deliveryequipmentallocationdetail ON deliveryequipmentallocationdetail.deliveryequipmententryid = deliveryequipmententry.id\n",
    "    LEFT JOIN delivery ON deliveryequipmententry.deliveryid = delivery.id\n",
    "    LEFT JOIN shipset ON delivery.shipsetid = shipset.id\n",
    "    LEFT JOIN shipsetdetail ON shipset.id = shipsetdetail.shipsetid\n",
    "    LEFT JOIN programtailnumbereis ON shipset.id = programtailnumbereis.id\n",
    "    LEFT JOIN installlocationdetail ON equipmententry.installlocationid = installlocationdetail.installlocationid\n",
    "    WHERE entrytype = 'Equipment'\n",
    "        AND shipsettype = 'Shipset'\n",
    "        AND servicestatus IS NOT NULL\n",
    "        AND servicestatus <> 'Cancelled'\n",
    "        AND equipmententrydetail.inactivedate IS NULL\n",
    "        AND deliveryequipmentallocationdetail.inactivedate IS NULL\n",
    "        AND shipsetdetail.inactivedate IS NULL\n",
    "        AND installlocationdetail.inactivedate IS NULL\n",
    "    ORDER BY servicestatus\n",
    "        \"\"\"\n",
    "\n",
    "# Execute query and load directly to pandas\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Rename columns to match desired schema\n",
    "column_mapping = {\n",
    "    'programcode': 'ProgramCode',\n",
    "    'installlocationtype': 'InstallLocationType',\n",
    "    'eisdate': 'EntryIntoServiceDate',\n",
    "    'equipmententryid': 'EquipmentEntryId',\n",
    "    'entrytype': 'EntryType',\n",
    "    'partnumber': 'PartNumber',\n",
    "    'description': 'Description',\n",
    "    'deliveryequipmententryid': 'DeliveryEquipmentEntryId',\n",
    "    'deliveryquantity': 'DeliveryQuantity',\n",
    "    'shipsettype': 'ShipsetType',\n",
    "    'shipsetnumber': 'ShipsetNumber',\n",
    "    'shipsetname': 'ShipsetName',\n",
    "    'tailnumber': 'Tail',\n",
    "    'servicestatus': 'ServiceStatus',\n",
    "    'ownercompanyid': 'OwnerCompanyId',\n",
    "    'installlocation': 'InstallLocation'\n",
    "}\n",
    "df = df.rename(columns=column_mapping)\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(data_dir, 'partslist.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather RMA Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import msal\n",
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 1. Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "CLOUD_INSTANCE = os.getenv('CLOUD_INSTANCE')\n",
    "CLIENT_ID = os.getenv('CLIENT_ID')\n",
    "CLIENT_SECRET = os.getenv('CLIENT_SECRET')\n",
    "SCOPE = os.getenv('SCOPE')\n",
    "DYNAMICS_URL = os.getenv('DYNAMICS_URL')\n",
    "\n",
    "# 2. Configure auth settings\n",
    "config = {\n",
    "    \"authority\": CLOUD_INSTANCE,\n",
    "    \"client_id\": CLIENT_ID,\n",
    "    \"client_secret\": CLIENT_SECRET,\n",
    "    \"scope\": [SCOPE]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # logger.info(\"Initializing MSAL application\")\n",
    "    app = msal.ConfidentialClientApplication(\n",
    "        config[\"client_id\"],\n",
    "        authority=config[\"authority\"],\n",
    "        client_credential=config[\"client_secret\"]\n",
    "    )\n",
    "\n",
    "    # logger.info(\"Acquiring token\")\n",
    "    result = app.acquire_token_for_client(scopes=config[\"scope\"])\n",
    "    \n",
    "    if \"access_token\" in result:\n",
    "        # logger.info(\"Token acquired successfully\")\n",
    "        access_token = result[\"access_token\"]\n",
    "    else:\n",
    "        logger.error(f\"Failed to acquire token. Error: {result.get('error')}\")\n",
    "        logger.error(f\"Error description: {result.get('error_description')}\")\n",
    "        raise Exception(\"Failed to acquire token\")\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {access_token}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }\n",
    "\n",
    "    all_records = []\n",
    "    next_link = DYNAMICS_URL + 'SvcCallTables'\n",
    "\n",
    "    while next_link:\n",
    "        try:\n",
    "            logger.info(f\"[{datetime.now()}] Fetching data from: {next_link}\")\n",
    "            response = requests.get(next_link, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            records = data.get(\"value\", [])\n",
    "            current_batch_size = len(records)\n",
    "            all_records.extend(records)\n",
    "            \n",
    "            logger.info(f\"[{datetime.now()}] Batch size: {current_batch_size}\")\n",
    "            logger.info(f\"[{datetime.now()}] Current batch records: {len(records)}\")\n",
    "            logger.info(f\"[{datetime.now()}] Total records so far: {len(all_records)}\")\n",
    "            \n",
    "            # Get next page link if exists\n",
    "            next_link = data.get('@odata.nextLink')\n",
    "            if next_link:\n",
    "                logger.info(f\"[{datetime.now()}] Next link found: {next_link}\")\n",
    "            else:\n",
    "                logger.info(f\"[{datetime.now()}] No more pages to fetch\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[{datetime.now()}] Error fetching data: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "        logger.info(f\"Total records retrieved: {len(all_records)}\")\n",
    "        \n",
    "        # Convert to DataFrame and save to CSV\n",
    "        rma_df = pd.DataFrame(all_records)\n",
    "\n",
    "        # Remove unwanted columns\n",
    "        columns_to_keep = [\n",
    "        'SvcCallId', 'DeliveryLocation_LocationId', 'Message', 'QuotationAmount',\n",
    "        'ProjId', 'CallTypeId', 'CallDueDateTime', 'QuotationAmountType',\n",
    "        'Subject', 'HcmWorker_PersonnelNumber', 'SvcCallInitiator', 'ComplaintId',\n",
    "        'ProjectIntegrationId', 'CauseId', 'Progress', 'SLARefDateTime',\n",
    "        'ServiceObjectId', 'InvoiceName', 'CallActionDateTime', 'SolutionId',\n",
    "        'RepairWithAccessories', 'CallStatusId', 'CustAccount', 'TaskDueDateTime',\n",
    "        'Solution', 'InternalMsg', 'HSOCertificateType', 'HSOFinalATPDate',\n",
    "        'HSOWarrantyEndDate', 'HSOOptionalRevision', 'HSOVisualInspectionResult',\n",
    "        'HSOWorkshopLocation', 'HSOExpectedShipDate', 'HSOIsBroadbandUnit',\n",
    "        'HSOPortalStatusRMA', 'HSOAirplaneType', 'HSORemovedDate', 'HSOFlightHours',\n",
    "        'HSOAirPlaneTailSerialNumber', 'HSOReturnReason', 'HSOActualShipDate',\n",
    "        'HSOFinalATPDocRevision', 'HSOQuoteApprovedDate', 'HSORepairDirection',\n",
    "        'HSOAirPlaneTailNumber', 'HSOQuoteSentDate', 'HSOWarrantyStartDate',\n",
    "        'HSOUnitReceivedDate'\n",
    "    ]\n",
    "\n",
    "    rma_df = rma_df[columns_to_keep]\n",
    "\n",
    "    # Get part data for each rma order\n",
    "    all_so_records = []\n",
    "    next_link = DYNAMICS_URL + 'DYSCoreServiceObjectTableCollection'\n",
    "\n",
    "    while next_link:\n",
    "        try:\n",
    "            logger.info(f\"[{datetime.now()}] Fetching data from: {next_link}\")\n",
    "            response = requests.get(next_link, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            records = data.get(\"value\", [])\n",
    "            current_batch_size = len(records)\n",
    "            all_so_records.extend(records)\n",
    "            \n",
    "            logger.info(f\"[{datetime.now()}] Batch size: {current_batch_size}\")\n",
    "            logger.info(f\"[{datetime.now()}] Current batch records: {len(records)}\")\n",
    "            logger.info(f\"[{datetime.now()}] Total records so far: {len(all_so_records)}\")\n",
    "            \n",
    "            # Get next page link if exists\n",
    "            next_link = data.get('@odata.nextLink')\n",
    "            if next_link:\n",
    "                logger.info(f\"[{datetime.now()}] Next link found: {next_link}\")\n",
    "            else:\n",
    "                logger.info(f\"[{datetime.now()}] No more pages to fetch\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"[{datetime.now()}] Error fetching data: {str(e)}\", exc_info=True)\n",
    "            raise\n",
    "\n",
    "    logger.info(f\"Total records retrieved: {len(all_so_records)}\")\n",
    "\n",
    "    so_df = pd.DataFrame(all_so_records)\n",
    "\n",
    "    # keeps only the columns we need\n",
    "    so_columns_to_keep = [\n",
    "        'ServiceObjectId',\n",
    "        'Status',\n",
    "        'ContactPersonPhone',\n",
    "        'CustAccountUser',\n",
    "        'CustAccountOwner',\n",
    "        'Description',\n",
    "        'ItemId',\n",
    "        'MachineTypeId',\n",
    "        'ContactPersonEmail',\n",
    "        'ProjId',\n",
    "        'InventSerialId',\n",
    "        'WarrantyStartDate_Cust',\n",
    "        'WorkshopRepairStatusRefRecId',\n",
    "        'WorkshopRepairStatus',\n",
    "        'CustAccount',\n",
    "        'WorkshopRepairStatusRefTableId'\n",
    "    ]\n",
    "    \n",
    "    so_df = so_df[so_columns_to_keep]\n",
    "\n",
    "    # Merge RMA orders and Service Objects\n",
    "    # Get overlapping columns\n",
    "    common_cols = set(rma_df.columns) & set(so_df.columns)\n",
    "    print(\"Common columns:\", common_cols)\n",
    "\n",
    "    # Merge with suffix handling\n",
    "    merged_df = pd.merge(\n",
    "        rma_df,\n",
    "        so_df,\n",
    "        on='ServiceObjectId',\n",
    "        how='left',\n",
    "        suffixes=('_rma', '_service')  # Clear suffixes to avoid conflicts\n",
    "    )\n",
    "\n",
    "    # Drop duplicate columns if needed\n",
    "    duplicate_cols = [col for col in merged_df.columns if col.endswith('_service')]\n",
    "    merged_df = merged_df.drop(columns=duplicate_cols)\n",
    "\n",
    "    # Print the number of columns and rows\n",
    "    print(\"\\nColumns in the dataset:\")\n",
    "    print(merged_df.columns.tolist())\n",
    "    print(f\"Rows: {len(merged_df)}\")\n",
    "    print(f\"\\nDataset saved to {output_path}\")\n",
    "\n",
    "    # Save to CSV\n",
    "    data_dir = os.path.join('private', 'data', 'raw')\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    output_path = os.path.join(data_dir, 'merged_rmaorders.csv')\n",
    "    merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred: {str(e)}\", exc_info=True)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Product information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_PORT = os.getenv('DB_PORT', '5432')\n",
    "\n",
    "# Database connection and query\n",
    "connection = psycopg2.connect(\n",
    "    host=DB_HOST,\n",
    "    database=DB_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    "    port=DB_PORT\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    -- Basic identification\n",
    "    product.partnumber,\n",
    "    productinformation.productname,\n",
    "    product.producttype,\n",
    "    \n",
    "    -- Product categorization\n",
    "    productinformation.productfamily,\n",
    "    productpcddetails.productgroup,\n",
    "    productinformation.definitionlevel,\n",
    "    productinformation.lrucategoryclass,\n",
    "    productpcddetails.conformitydescription,\n",
    "    \n",
    "    -- Manufacturing information\n",
    "    productinformation.manufacturingtype,\n",
    "    productinformation.repairtype,\n",
    "    productinformation.pmastatus,\n",
    "    \n",
    "    -- Production status\n",
    "    productinformation.newdesignrecommendation,\n",
    "    productinformation.useequipmentfamily,\n",
    "    \n",
    "    -- Technical specifications\n",
    "    productinputpower.operatingmode,\n",
    "    productinputpower.maximuminputpower,\n",
    "    productlruspecifics.internalstorage,\n",
    "    productlruspecifics.resolution,\n",
    "    productlruspecifics.frontpanelusb,\n",
    "    productlruspecifics.oneethernetport,\n",
    "    productlruspecifics.bajfunctionality,\n",
    "    \n",
    "    -- Lifecycle information\n",
    "    productlifecyclephase.lifecyclephase,\n",
    "    productmilestone.milestone,\n",
    "    productmilestone.milestonedate,\n",
    "    productmilestone.milestonestatus,\n",
    "    productmilestone.actualdate,\n",
    "    productmilestone.notes,\n",
    "    \n",
    "    -- Manager information\n",
    "    productinformation.linemanager,\n",
    "    \n",
    "    -- Performance/reliability metrics\n",
    "    productreliability.contractualmtbf,\n",
    "    productpower.engineering AS power_engineering,\n",
    "    productpower.marketing AS power_marketing,\n",
    "    productpower.contractual AS power_contractual,\n",
    "    productweight.engineering AS weight_engineering,\n",
    "    productweight.marketing AS weight_marketing,\n",
    "    productweight.contractual AS weight_contractual\n",
    "FROM product\n",
    "LEFT JOIN productbase ON product.partnumber = productbase.partnumber\n",
    "LEFT JOIN productinformation ON product.partnumber = productinformation.partnumber\n",
    "LEFT JOIN productinputpower ON product.partnumber = productinputpower.partnumber\n",
    "LEFT JOIN productlifecyclephase ON product.partnumber = productlifecyclephase.partnumber\n",
    "LEFT JOIN productlruspecifics ON product.partnumber = productlruspecifics.partnumber\n",
    "LEFT JOIN productmilestone ON product.partnumber = productmilestone.partnumber\n",
    "LEFT JOIN productpcddetails ON product.partnumber = productpcddetails.partnumber\n",
    "LEFT JOIN productpower ON product.partnumber = productpower.partnumber \n",
    "LEFT JOIN productreliability ON product.partnumber = productreliability.partnumber \n",
    "LEFT JOIN productweight ON product.partnumber = productweight.partnumber\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and load directly to pandas\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(data_dir, 'productinfo.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Resets Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import pyodbc\n",
    "import os\n",
    "\n",
    "# Load ENV Variables\n",
    "load_dotenv()\n",
    "RA_DB_HOST = os.getenv('RA_DB_HOST')\n",
    "RA_DB_NAME = os.getenv('RA_DB_NAME')\n",
    "RA_DB_USER = os.getenv('RA_DB_USER')\n",
    "RA_DB_PASSWORD = os.getenv('RA_DB_PASSWORD')\n",
    "RA_DB_PORT = os.getenv('RA_DB_PORT', '1433')\n",
    "\n",
    "# Database connection and query\n",
    "connection = pyodbc.connect(\n",
    "    f'DRIVER=ODBC Driver 17 for SQL Server;SERVER={RA_DB_HOST},{RA_DB_PORT};DATABASE={RA_DB_NAME};UID={RA_DB_USER};PWD={RA_DB_PASSWORD}'\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        FlightResetsID,\n",
    "        FlightID,\n",
    "        Airline,\n",
    "        DepartureCode,\n",
    "        ArrivalCode,\n",
    "        FlightNumber,\n",
    "        TailNumber,\n",
    "        FlightStartTime,\n",
    "        FlightEndTime,\n",
    "        FlightDuration,\n",
    "        Class,\n",
    "        AircraftType,\n",
    "        SeatResets,\n",
    "        RawResets,\n",
    "        Processed\n",
    "    FROM [dbo].[FlightResets]\n",
    "    WHERE Airline != 'RAVE' \n",
    "    AND Airline != 'TEST' \n",
    "    AND Airline != 'PUBLIC'\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(data_dir, 'flightresets.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather MTBF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import pyodbc\n",
    "import os\n",
    "\n",
    "# Load ENV Variables\n",
    "load_dotenv()\n",
    "RA_DB_HOST = os.getenv('RA_DB_HOST')\n",
    "RA_DB_NAME = os.getenv('RA_DB_NAME')\n",
    "RA_DB_USER = os.getenv('RA_DB_USER')\n",
    "RA_DB_PASSWORD = os.getenv('RA_DB_PASSWORD')\n",
    "RA_DB_PORT = os.getenv('RA_DB_PORT', '1433')\n",
    "\n",
    "# Database connection and query\n",
    "connection = pyodbc.connect(\n",
    "    f'DRIVER=ODBC Driver 17 for SQL Server;SERVER={RA_DB_HOST},{RA_DB_PORT};DATABASE={RA_DB_NAME};UID={RA_DB_USER};PWD={RA_DB_PASSWORD}'\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        MTBFID,\n",
    "        Airline,\n",
    "        PartNumber,\n",
    "        PartGroup,\n",
    "        DetailPartGroup,\n",
    "        Description,\n",
    "        Month,\n",
    "        PoweredOnHours,\n",
    "        FlightHours,\n",
    "        Failures,\n",
    "        NFF,\n",
    "        ContractualMTBF,\n",
    "        InsertDate,\n",
    "        UpdateDate,\n",
    "        UpdateCount\n",
    "    FROM [dbo].[MTBF]\n",
    "    WHERE Airline != 'RAVE'\n",
    "    \"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(data_dir, 'mtbf.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Flight Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import pyodbc\n",
    "import os\n",
    "\n",
    "# Load ENV Variables\n",
    "load_dotenv()\n",
    "RA_DB_HOST = os.getenv('RA_DB_HOST')\n",
    "RA_DB_NAME = os.getenv('RA_DB_NAME')\n",
    "RA_DB_USER = os.getenv('RA_DB_USER')\n",
    "RA_DB_PASSWORD = os.getenv('RA_DB_PASSWORD')\n",
    "RA_DB_PORT = os.getenv('RA_DB_PORT', '1433')\n",
    "\n",
    "# Database connection and query\n",
    "connection = pyodbc.connect(\n",
    "    f'DRIVER=ODBC Driver 17 for SQL Server;SERVER={RA_DB_HOST},{RA_DB_PORT};DATABASE={RA_DB_NAME};UID={RA_DB_USER};PWD={RA_DB_PASSWORD}'\n",
    ")\n",
    "\n",
    "query = \"\"\" \n",
    "         SELECT\n",
    "         FlightID,\n",
    "         Airline,\n",
    "         DepartureCode,\n",
    "          ArrivalCode,\n",
    "         FlightStartTime,\n",
    "         FlightEndTime,\n",
    "         TailNumber,\n",
    "         FlightNumber,\n",
    "         AircraftType,\n",
    "         InsertDate AS FileCreatedTime,\n",
    "         InsertDate\n",
    " FROM [dbo].[Flight]\n",
    "\"\"\"\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(data_dir, 'flights.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Passenger Count per flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import pyodbc\n",
    "import os\n",
    "\n",
    "# Load ENV Variables\n",
    "load_dotenv()\n",
    "RA_DB_HOST = os.getenv('RA_DB_HOST')\n",
    "RA_DB_NAME = os.getenv('RA_DB_NAME')\n",
    "RA_DB_USER = os.getenv('RA_DB_USER')\n",
    "RA_DB_PASSWORD = os.getenv('RA_DB_PASSWORD')\n",
    "RA_DB_PORT = os.getenv('RA_DB_PORT', '1433')\n",
    "\n",
    "# Database connection and query\n",
    "connection = pyodbc.connect(\n",
    "    f'DRIVER=ODBC Driver 17 for SQL Server;SERVER={RA_DB_HOST},{RA_DB_PORT};DATABASE={RA_DB_NAME};UID={RA_DB_USER};PWD={RA_DB_PASSWORD}'\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        AIMSID,\n",
    "        FlightID,\n",
    "        TailNumber,\n",
    "        FlightNumber,\n",
    "        DepartureCode,\n",
    "        ArrivalCode,\n",
    "        FlightStartTime,\n",
    "        FlightEndTime,\n",
    "        BusinessClass,\n",
    "        EconomyClass,\n",
    "        TotalPassengers,\n",
    "        InsertDate,\n",
    "        UpdatedPaxActivity,\n",
    "        UpdatedPerPassengerRevenue\n",
    "    FROM [dbo].[ActualPassengerCounts]\n",
    "    WHERE TailNumber IS NOT NULL\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(data_dir, 'passenger_count.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Older RMA records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import pyodbc\n",
    "import os\n",
    "\n",
    "# Load ENV Variables\n",
    "load_dotenv()\n",
    "RA_DB_HOST = os.getenv('RA_DB_HOST')\n",
    "RA_DB_NAME = os.getenv('RA_DB_NAME')\n",
    "RA_DB_USER = os.getenv('RA_DB_USER')\n",
    "RA_DB_PASSWORD = os.getenv('RA_DB_PASSWORD')\n",
    "RA_DB_PORT = os.getenv('RA_DB_PORT', '1433')\n",
    "\n",
    "# Database connection and query\n",
    "connection = pyodbc.connect(\n",
    "    f'DRIVER=ODBC Driver 17 for SQL Server;SERVER={RA_DB_HOST},{RA_DB_PORT};DATABASE={RA_DB_NAME};UID={RA_DB_USER};PWD={RA_DB_PASSWORD}'\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        Customer,\n",
    "        RMA,\n",
    "        PN,\n",
    "        SN,\n",
    "        StatusDescription,\n",
    "        PartDescription,\n",
    "        LRUName,\n",
    "        ReceivedDate,\n",
    "        Receivedat3P,\n",
    "        FaultCode,\n",
    "        ShipDate,\n",
    "        ServiceBulletinInfo,\n",
    "        ServiceBulletinNumber,\n",
    "        ServiceBulletin,\n",
    "        AlertCategoryCode,\n",
    "        InsertDate\n",
    "    FROM RCS_DWH.dbo.RepairRMA\n",
    "    WHERE PN IS NOT NULL\n",
    "    ORDER BY ReceivedDate DESC\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Add \"arc_\" prefix to the filename\n",
    "output_path = os.path.join(data_dir, 'hist_repair_rma.csv')\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gather Parts Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DB_HOST = os.getenv('DB_HOST')\n",
    "DB_NAME = os.getenv('DB_NAME')\n",
    "DB_USER = os.getenv('DB_USER')\n",
    "DB_PASSWORD = os.getenv('DB_PASSWORD')\n",
    "DB_PORT = os.getenv('DB_PORT', '5432')\n",
    "\n",
    "# Database connection and query\n",
    "connection = psycopg2.connect(\n",
    "    host=DB_HOST,\n",
    "    database=DB_NAME,\n",
    "    user=DB_USER,\n",
    "    password=DB_PASSWORD,\n",
    "    port=DB_PORT\n",
    ")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    -- Basic identification\n",
    "    product.partnumber,\n",
    "    productinformation.productname,\n",
    "    product.producttype,\n",
    "    \n",
    "    -- Product categorization\n",
    "    productinformation.productfamily,\n",
    "    productpcddetails.productgroup,\n",
    "    productinformation.definitionlevel,\n",
    "    productinformation.lrucategoryclass,\n",
    "    productpcddetails.conformitydescription,\n",
    "    \n",
    "    -- Manufacturing information\n",
    "    productinformation.manufacturingtype,\n",
    "    productinformation.repairtype,\n",
    "    \n",
    "    -- Production status\n",
    "    productinformation.newdesignrecommendation,\n",
    "    productinformation.useequipmentfamily,\n",
    "    \n",
    "    -- Technical specifications\n",
    "    productinputpower.operatingmode,\n",
    "    productinputpower.maximuminputpower,\n",
    "    productlruspecifics.internalstorage,\n",
    "    productlruspecifics.resolution,\n",
    "    productlruspecifics.frontpanelusb,\n",
    "    productlruspecifics.oneethernetport,\n",
    "    productlruspecifics.bajfunctionality,\n",
    "    \n",
    "    -- Lifecycle information\n",
    "    productlifecyclephase.lifecyclephase,\n",
    "    productmilestone.milestone,\n",
    "    productmilestone.milestonedate,\n",
    "    productmilestone.milestonestatus,\n",
    "    productmilestone.actualdate,\n",
    "    productmilestone.notes,\n",
    "    \n",
    "    -- Manager information\n",
    "    productinformation.linemanager,\n",
    "    \n",
    "    -- Performance/reliability metrics\n",
    "    productreliability.contractualmtbf,\n",
    "    productpower.engineering AS power_engineering,\n",
    "    productpower.marketing AS power_marketing,\n",
    "    productpower.contractual AS power_contractual,\n",
    "    productweight.engineering AS weight_engineering,\n",
    "    productweight.marketing AS weight_marketing,\n",
    "    productweight.contractual AS weight_contractual\n",
    "FROM product\n",
    "LEFT JOIN productbase ON product.partnumber = productbase.partnumber\n",
    "LEFT JOIN productinformation ON product.partnumber = productinformation.partnumber\n",
    "LEFT JOIN productinputpower ON product.partnumber = productinputpower.partnumber\n",
    "LEFT JOIN productlifecyclephase ON product.partnumber = productlifecyclephase.partnumber\n",
    "LEFT JOIN productlruspecifics ON product.partnumber = productlruspecifics.partnumber\n",
    "LEFT JOIN productmilestone ON product.partnumber = productmilestone.partnumber\n",
    "LEFT JOIN productpcddetails ON product.partnumber = productpcddetails.partnumber\n",
    "LEFT JOIN productpower ON product.partnumber = productpower.partnumber \n",
    "LEFT JOIN productreliability ON product.partnumber = productreliability.partnumber \n",
    "LEFT JOIN productweight ON product.partnumber = productweight.partnumber\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and load directly to pandas\n",
    "df = pd.read_sql_query(query, connection)\n",
    "connection.close()\n",
    "\n",
    "# Create directory structure\n",
    "data_dir = os.path.join('private', 'data', 'raw')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(data_dir, 'productinfo.csv')\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Print the number of columns and rows\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"Rows: {len(df)}\")\n",
    "print(f\"\\nDataset saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

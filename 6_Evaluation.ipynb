{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4c0139c",
   "metadata": {},
   "source": [
    "# Aerospace Component Failure Prediction Model Evaluation\n",
    "\n",
    "This notebook evaluates the performance of the trained LSTM model for predicting aerospace component maintenance events. It provides comprehensive metrics, visualizations, and analysis of the model's predictive capabilities.\n",
    "\n",
    "## Key Features:\n",
    "- Model performance evaluation with MSE, RMSE, MAE metrics\n",
    "- Binary classification metrics for maintenance event prediction\n",
    "- Comprehensive visualization suite including density plots, confusion matrices, and error analysis\n",
    "- Sample prediction comparisons and training history analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89f123c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import gaussian_kde\n",
    "import gc\n",
    "\n",
    "# Output directory\n",
    "output_dir = os.path.join('private', 'data', 'training_data')\n",
    "viz_dir = os.path.join(output_dir, 'visualizations')\n",
    "os.makedirs(viz_dir, exist_ok=True)\n",
    "\n",
    "# Load the trained model with custom_objects\n",
    "model_path = os.path.join(output_dir, 'time_based_maintenance_model.h5')\n",
    "model = tf.keras.models.load_model(\n",
    "    model_path,\n",
    "    custom_objects={\n",
    "        'mse': tf.keras.losses.MeanSquaredError(),\n",
    "        'mae': tf.keras.metrics.MeanAbsoluteError()\n",
    "    }\n",
    ")\n",
    "print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "# Load training history\n",
    "hist_csv_file = os.path.join(output_dir, 'training_history.csv')\n",
    "if os.path.exists(hist_csv_file):\n",
    "    history_df = pd.read_csv(hist_csv_file)\n",
    "    \n",
    "    # Create a dictionary-based history object\n",
    "    history = {'history': {}}\n",
    "    for col in history_df.columns:\n",
    "        history['history'][col] = history_df[col].values\n",
    "    \n",
    "    print(f\"Loaded training history from {hist_csv_file}\")\n",
    "\n",
    "# Load test data\n",
    "test_data_path = os.path.join(output_dir, 'test_data.npz')\n",
    "if os.path.exists(test_data_path):\n",
    "    test_data_loaded = np.load(test_data_path, allow_pickle=True)\n",
    "    X_test = test_data_loaded['X_test']\n",
    "    y_test = test_data_loaded['y_test'] \n",
    "    y_test_original = test_data_loaded['y_test_original']\n",
    "    test_data = (X_test, y_test, y_test_original)\n",
    "    print(f\"Loaded test data from {test_data_path}\")\n",
    "\n",
    "# Define evaluation functions\n",
    "def evaluate_model(model, test_data, output_dir):\n",
    "    X_test, y_test, y_test_original = test_data\n",
    "    \n",
    "    # Generate predictions\n",
    "    batch_size = 128  # Batch size for prediction\n",
    "    num_batches = int(np.ceil(len(X_test) / batch_size))\n",
    "    y_pred_list = []\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(X_test))\n",
    "        batch_pred = model.predict(X_test[start_idx:end_idx], verbose=0)\n",
    "        y_pred_list.append(batch_pred)\n",
    "        \n",
    "        # Print Progress\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Processed {i+1}/{num_batches} batches\")\n",
    "    \n",
    "    # Combine predictions\n",
    "    y_pred_log = np.vstack(y_pred_list).flatten()\n",
    "    # Inverse log transformation\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    # Make sure predictions are non-negative\n",
    "    y_pred = np.maximum(0, y_pred)\n",
    "    \n",
    "    # Calculate metrics on original scale\n",
    "    mse = np.mean((y_test_original - y_pred)**2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_test_original - y_pred))\n",
    "    \n",
    "    # Calculate MAPE (Mean Absolute Percentage Error)\n",
    "    non_zero_idx = y_test_original > 0\n",
    "    if non_zero_idx.sum() > 0:\n",
    "        mape = np.mean(np.abs((y_test_original[non_zero_idx] - y_pred[non_zero_idx]) / \n",
    "                              y_test_original[non_zero_idx])) * 100\n",
    "        print(f\"MAPE (non-zero values): {mape:.2f}%\")\n",
    "    \n",
    "    threshold = 0.5  # Threshold for predicted maintenance event count\n",
    "    y_test_binary = (y_test_original > 0).astype(int)\n",
    "    y_pred_binary = (y_pred > threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics for binary classification\n",
    "    true_pos = np.sum((y_test_binary == 1) & (y_pred_binary == 1))\n",
    "    false_pos = np.sum((y_test_binary == 0) & (y_pred_binary == 1))\n",
    "    true_neg = np.sum((y_test_binary == 0) & (y_pred_binary == 0))\n",
    "    false_neg = np.sum((y_test_binary == 1) & (y_pred_binary == 0))\n",
    "    \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nTest Set Evaluation:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    \n",
    "    print(\"\\nBinary maintenance event prediction metrics:\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"True Positives: {true_pos}\")\n",
    "    print(f\"False Positives: {false_pos}\")\n",
    "    print(f\"True Negatives: {true_neg}\")\n",
    "    print(f\"False Negatives: {false_neg}\")\n",
    "    \n",
    "    return {\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'mape': mape if 'mape' in locals() else None,\n",
    "        'y_pred': y_pred  # Return predictions for visualization\n",
    "    }\n",
    "\n",
    "def generate_model_evaluation_plots(test_data, y_pred, output_dir):\n",
    "    _, _, y_test_original = test_data\n",
    "    \n",
    "    # Sample data for visualization\n",
    "    max_plot_points = min(5000, len(y_test_original))\n",
    "    if len(y_test_original) > max_plot_points:\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        indices = np.random.choice(len(y_test_original), max_plot_points, replace=False)\n",
    "        plot_y_test = y_test_original[indices]\n",
    "        plot_y_pred = y_pred[indices]\n",
    "    else:\n",
    "        plot_y_test = y_test_original\n",
    "        plot_y_pred = y_pred\n",
    "    \n",
    "    errors = plot_y_pred - plot_y_test\n",
    "    \n",
    "    # Error Histogram\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.hist(errors, bins=30, alpha=0.7, color='blue')\n",
    "    plt.axvline(x=0, color='r', linestyle='--')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Prediction Errors')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, 'error_distribution.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Confusion matrix is for binary classification \n",
    "    # Confusion Matrix\n",
    "    y_test_binary = (plot_y_test > 0).astype(int)\n",
    "    y_pred_binary = (plot_y_pred > 0.5).astype(int)\n",
    "    cm = np.zeros((2, 2), dtype=int)\n",
    "    cm[0, 0] = np.sum((y_test_binary == 0) & (y_pred_binary == 0))  # TN\n",
    "    cm[0, 1] = np.sum((y_test_binary == 0) & (y_pred_binary == 1))  # FP\n",
    "    cm[1, 0] = np.sum((y_test_binary == 1) & (y_pred_binary == 0))  # FN\n",
    "    cm[1, 1] = np.sum((y_test_binary == 1) & (y_pred_binary == 1))  # TP\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=['No Maintenance', 'Maintenance'],\n",
    "                yticklabels=['No Maintenance', 'Maintenance'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, 'confusion_matrix.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return plot_y_test, plot_y_pred, errors\n",
    "\n",
    "def generate_density_plots(plot_y_test, plot_y_pred, errors, output_dir):   \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    try:\n",
    "        # Create x,y grid\n",
    "        xy = np.vstack([plot_y_test, plot_y_pred])\n",
    "        z = gaussian_kde(xy)(xy)\n",
    "        \n",
    "        # Sort points by density for better visualization\n",
    "        idx = z.argsort()\n",
    "        x, y, z = plot_y_test[idx], plot_y_pred[idx], z[idx]\n",
    "        \n",
    "        scatter = plt.scatter(x, y, c=z, s=30, cmap='viridis', alpha=0.8)\n",
    "        plt.colorbar(scatter, label='Density')\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create density scatter plot, falling back to simple scatter: {e}\")\n",
    "        plt.scatter(plot_y_test, plot_y_pred, alpha=0.5, color='blue')\n",
    "    \n",
    "    max_val = max(np.max(plot_y_test), np.max(plot_y_pred))\n",
    "    plt.plot([0, max_val], [0, max_val], 'r--', linewidth=2)\n",
    "    plt.xlabel('Actual Maintenance Events')\n",
    "    plt.ylabel('Predicted Maintenance Events')\n",
    "    plt.title('Actual vs. Predicted Maintenance Events (Density)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, 'density_scatter.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Low Maintenance Event Range Scatter Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    small_mask = (plot_y_test <= 5) & (plot_y_pred <= 10)\n",
    "    plt.scatter(plot_y_test[small_mask], plot_y_pred[small_mask], alpha=0.6, color='green')\n",
    "    plt.plot([0, 5], [0, 5], 'r--', linewidth=2)\n",
    "    plt.xlabel('Actual Maintenance Events (0-5)')\n",
    "    plt.ylabel('Predicted Maintenance Events (0-10)')\n",
    "    plt.title('Low Maintenance Event Count Prediction')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, 'low_maintenance_prediction.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # Error by Actual Maintenance Events\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    y_test_clip = np.clip(plot_y_test, 0, np.percentile(plot_y_test, 95))\n",
    "    error_clip = np.clip(errors, np.percentile(errors, 5), np.percentile(errors, 95))\n",
    "    plt.scatter(y_test_clip, error_clip, color='purple', alpha=0.4, s=30)\n",
    "    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Actual Maintenance Events (clipped to 95th percentile)')\n",
    "    plt.ylabel('Prediction Error (clipped 5-95 percentile)')\n",
    "    plt.title('Error by Actual Maintenance Event Count')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, 'error_by_actual.png'))\n",
    "    plt.close()\n",
    "\n",
    "def generate_sample_predictions(model, test_data, output_dir):\n",
    "    X_test_sample, y_test_sample, y_test_original_sample = test_data\n",
    "    \n",
    "    # Take a sample for demonstration\n",
    "    sample_size = min(10, len(X_test_sample))\n",
    "    sample_indices = np.random.choice(len(X_test_sample), sample_size, replace=False)\n",
    "    \n",
    "    X_sample = X_test_sample[sample_indices]\n",
    "    y_sample_log = y_test_sample[sample_indices]\n",
    "    y_sample_original = y_test_original_sample[sample_indices]\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_pred_log = model.predict(X_sample)\n",
    "    y_pred = np.expm1(y_pred_log)  # Inverse log transform\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = y_pred.flatten() - y_sample_original\n",
    "    percent_errors = np.zeros_like(errors)\n",
    "    for i in range(len(errors)):\n",
    "        if y_sample_original[i] > 0:\n",
    "            percent_errors[i] = (errors[i] / y_sample_original[i]) * 100\n",
    "        else:\n",
    "            percent_errors[i] = np.nan\n",
    "    \n",
    "    # Print Results\n",
    "    print(\"\\nSAMPLE PREDICTIONS:\")\n",
    "    print(\"   ACTUAL       PREDICTED       ERROR       % ERROR        \")\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "        if not np.isnan(percent_errors[i]):\n",
    "            print(f\" {y_sample_original[i]:11.1f}   {y_pred[i][0]:11.1f}   {errors[i]:11.1f}   {percent_errors[i]:14.1f}%  \")\n",
    "        else:\n",
    "            print(f\" {y_sample_original[i]:11.1f}   {y_pred[i][0]:11.1f}   {errors[i]:11.1f}          N/A       \")\n",
    "    \n",
    "    \n",
    "    # Create a visualization of sample predictions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Bar chart comparing actual vs predicted\n",
    "    ind = np.arange(sample_size)\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(ind, y_sample_original, width, label='Actual', color='royalblue')\n",
    "    plt.bar(ind + width, y_pred.flatten(), width, label='Predicted', color='lightcoral')\n",
    "    \n",
    "    plt.ylabel('Maintenance Event Count')\n",
    "    plt.title('Sample Predictions Comparison')\n",
    "    plt.xticks(ind + width/2, [f'Sample {i+1}' for i in range(sample_size)])\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(y_sample_original):\n",
    "        plt.text(i, v + 0.1, f'{v:.1f}', color='royalblue', fontweight='bold', ha='center')\n",
    "        \n",
    "    for i, v in enumerate(y_pred.flatten()):\n",
    "        plt.text(i + width, v + 0.1, f'{v:.1f}', color='lightcoral', fontweight='bold', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, 'sample_predictions.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return y_pred, errors, percent_errors\n",
    "\n",
    "def create_actual_vs_predicted_zoomed(plot_y_test, plot_y_pred, output_dir):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # 0-10 range\n",
    "    low_mask = (plot_y_test <= 10) & (plot_y_pred <= 10)\n",
    "    plt.scatter(plot_y_test[low_mask], plot_y_pred[low_mask], alpha=0.5, s=30, color='#3366CC')\n",
    "    \n",
    "    # Add lines\n",
    "    plt.plot([0, 10], [0, 10], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    # Fit regression line\n",
    "    if np.sum(low_mask) > 5: \n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        model = LinearRegression()\n",
    "        X_low = plot_y_test[low_mask].reshape(-1, 1)\n",
    "        y_low = plot_y_pred[low_mask]\n",
    "        model.fit(X_low, y_low)\n",
    "        pred_line = model.predict(np.array([[0], [10]]))\n",
    "        plt.plot([0, 10], pred_line, 'g-', linewidth=2, \n",
    "                 label=f'Regression Line (slope={model.coef_[0]:.2f})')\n",
    "    plt.hexbin(plot_y_test[low_mask], plot_y_pred[low_mask], gridsize=30, cmap='Blues', alpha=0.4)\n",
    "    \n",
    "    # Annotations\n",
    "    low_r2 = np.corrcoef(plot_y_test[low_mask], plot_y_pred[low_mask])[0, 1]**2\n",
    "    low_mae = np.mean(np.abs(plot_y_test[low_mask] - plot_y_pred[low_mask]))\n",
    "    plt.annotate(f'R² (0-10 range) = {low_r2:.3f}\\nMAE = {low_mae:.2f}', \n",
    "                 xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", alpha=0.8))\n",
    "    \n",
    "    # Labels and title\n",
    "    plt.xlabel('Actual Maintenance Events (0-10 range)', fontsize=14)\n",
    "    plt.ylabel('Predicted Maintenance Events (0-10 range)', fontsize=14)\n",
    "    plt.title('Actual vs. Predicted Maintenance Events (Zoomed to 0-10 range)', fontsize=16)\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.legend(loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, 'actual_vs_predicted_zoomed.png'))\n",
    "    plt.close()\n",
    "\n",
    "def create_improved_density_plot(plot_y_test, plot_y_pred, output_dir):\n",
    "    # Create a zoomed-in view for 0-10 range\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    zoom_mask = (plot_y_test <= 10) & (plot_y_pred <= 10)\n",
    "    hexbin = plt.hexbin(\n",
    "        plot_y_test[zoom_mask], \n",
    "        plot_y_pred[zoom_mask], \n",
    "        gridsize=40, \n",
    "        cmap='YlOrRd',\n",
    "        mincnt=1,\n",
    "        bins='log',\n",
    "        alpha=0.8\n",
    "    )\n",
    "    cbar = plt.colorbar(hexbin, label='Log10(Count)')\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    plt.plot([0, 10], [0, 10], 'b--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    # Add regression line\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    model = LinearRegression()\n",
    "    X = plot_y_test[zoom_mask].reshape(-1, 1)\n",
    "    model.fit(X, plot_y_pred[zoom_mask])\n",
    "    pred_line = model.predict(np.array([[0], [10]]))\n",
    "    plt.plot([0, 10], pred_line, 'g-', linewidth=2, \n",
    "             label=f'Regression Line (slope={model.coef_[0]:.2f})')\n",
    "    \n",
    "    # Add annotations for zoomed region\n",
    "    r2_zoom = np.corrcoef(plot_y_test[zoom_mask], plot_y_pred[zoom_mask])[0, 1]**2\n",
    "    mae_zoom = np.mean(np.abs(plot_y_test[zoom_mask] - plot_y_pred[zoom_mask]))\n",
    "    \n",
    "    metrics_text = (\n",
    "        f'R² (0-10 range) = {r2_zoom:.3f}\\n'\n",
    "        f'MAE (0-10 range) = {mae_zoom:.2f}\\n'\n",
    "        f'Data points in 0-10 range: {np.sum(zoom_mask):,} ({np.sum(zoom_mask)/len(plot_y_test):.1%})'\n",
    "    )\n",
    "    \n",
    "    plt.annotate(\n",
    "        metrics_text, \n",
    "        xy=(0.05, 0.95), \n",
    "        xycoords='axes fraction',\n",
    "        bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", ec=\"gray\", alpha=0.8),\n",
    "        va='top'\n",
    "    )\n",
    "    \n",
    "    # Highlight important regions\n",
    "    plt.axvspan(0, 1, color='lightgreen', alpha=0.2, label='Zero Maintenance Event Region')\n",
    "    plt.axvspan(1, 5, color='lightyellow', alpha=0.2, label='Common Maintenance Event Range')\n",
    "    \n",
    "    # Set axis limits for zoom\n",
    "    plt.xlim(-0.5, 10.5)\n",
    "    plt.ylim(-0.5, 10.5)\n",
    "    \n",
    "    # Add grid, labels and title\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    plt.xlabel('Actual Maintenance Events (0-10 range)', fontsize=14)\n",
    "    plt.ylabel('Predicted Maintenance Events (0-10 range)', fontsize=14)\n",
    "    plt.title('Actual vs. Predicted Maintenance Events (0-10 Range)', fontsize=16)\n",
    "    \n",
    "    plt.legend(loc='upper left', framealpha=0.9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(viz_dir, 'density_scatter_zoomed.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "def create_enhanced_training_history(history, output_dir):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    plt.figure(figsize=(12, 15))\n",
    "    \n",
    "    # Loss curve\n",
    "    plt.subplot(3, 1, 1)\n",
    "    loss_values = history['history']['loss']\n",
    "    val_loss_values = history['history']['val_loss']\n",
    "    epochs = range(1, len(loss_values) + 1)\n",
    "    \n",
    "    plt.plot(epochs, loss_values, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, val_loss_values, 'r-', label='Validation Loss')\n",
    "    \n",
    "    # Find the best model\n",
    "    best_val_loss_idx = np.argmin(val_loss_values)\n",
    "    plt.plot(best_val_loss_idx + 1, val_loss_values[best_val_loss_idx], 'go', markersize=8)\n",
    "    plt.annotate(f'Best: {val_loss_values[best_val_loss_idx]:.2f}', \n",
    "                xy=(best_val_loss_idx + 1, val_loss_values[best_val_loss_idx]),\n",
    "                xytext=(best_val_loss_idx + 1 + 2, val_loss_values[best_val_loss_idx] * 1.2),\n",
    "                arrowprops=dict(facecolor='green', shrink=0.05))\n",
    "    \n",
    "    # Add learning rate \n",
    "    if 'lr' in history['history']:\n",
    "        ax2 = plt.gca().twinx()\n",
    "        ax2.plot(epochs, history['history']['lr'], 'g--', label='Learning Rate')\n",
    "        ax2.set_ylabel('Learning Rate')\n",
    "        ax2.set_yscale('log')\n",
    "        lines1, labels1 = plt.gca().get_legend_handles_labels()\n",
    "        lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "        ax2.legend(lines1 + lines2, labels1 + labels2, loc='upper right')\n",
    "    else:\n",
    "        plt.legend(loc='upper right')\n",
    "    \n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # MAE curve\n",
    "    plt.subplot(3, 1, 2)\n",
    "    mae_values = history['history']['mae']\n",
    "    val_mae_values = history['history']['val_mae']\n",
    "    \n",
    "    plt.plot(epochs, mae_values, 'b-', label='Training MAE')\n",
    "    plt.plot(epochs, val_mae_values, 'r-', label='Validation MAE')\n",
    "    \n",
    "    # Find the best MAE\n",
    "    best_val_mae_idx = np.argmin(val_mae_values)\n",
    "    plt.plot(best_val_mae_idx + 1, val_mae_values[best_val_mae_idx], 'go', markersize=8)\n",
    "    plt.annotate(f'Best: {val_mae_values[best_val_mae_idx]:.2f}', \n",
    "                xy=(best_val_mae_idx + 1, val_mae_values[best_val_mae_idx]),\n",
    "                xytext=(best_val_mae_idx + 1 + 2, val_mae_values[best_val_mae_idx] * 1.2),\n",
    "                arrowprops=dict(facecolor='green', shrink=0.05))\n",
    "    \n",
    "    plt.title('Training and Validation MAE')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Zoomed Loss curve (last 75% of training)\n",
    "    plt.subplot(3, 1, 3)\n",
    "    start_idx = len(loss_values) // 4  # Start from 25% of training\n",
    "    \n",
    "    plt.plot(epochs[start_idx:], loss_values[start_idx:], 'b-', label='Training Loss')\n",
    "    plt.plot(epochs[start_idx:], val_loss_values[start_idx:], 'r-', label='Validation Loss')\n",
    "    \n",
    "    # Calculate improvement\n",
    "    if loss_values[start_idx] > 0:\n",
    "        improvement = ((loss_values[start_idx] - loss_values[-1]) / loss_values[start_idx]) * 100\n",
    "        plt.annotate(f'Improvement: {improvement:.1f}%', \n",
    "                    xy=(epochs[-1], loss_values[-1]),\n",
    "                    xytext=(epochs[-1] - 5, loss_values[-1] * 1.3),\n",
    "                    arrowprops=dict(facecolor='blue', shrink=0.05))\n",
    "    \n",
    "    plt.title('Training and Validation Loss (Zoomed)')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'training_history.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Training history visualization saved to {output_dir}\")\n",
    "    return\n",
    "try:\n",
    "    if 'test_data' not in locals():\n",
    "        if os.path.exists(test_data_path):\n",
    "            test_data_loaded = np.load(test_data_path, allow_pickle=True)\n",
    "            X_test = test_data_loaded['X_test']\n",
    "            y_test = test_data_loaded['y_test'] \n",
    "            y_test_original = test_data_loaded['y_test_original']\n",
    "            test_data = (X_test, y_test, y_test_original)\n",
    "        else:\n",
    "            raise ValueError(\"Test data not available. Please save it during training.\")\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    metrics = evaluate_model(model, test_data, output_dir)\n",
    "    \n",
    "    # Save metrics\n",
    "    metrics_df = pd.DataFrame({k: [v] for k, v in metrics.items() if k != 'y_pred'})\n",
    "    metrics_df.to_csv(os.path.join(output_dir, 'model_metrics.csv'), index=False)\n",
    "    \n",
    "    # Create visualizations\n",
    "    print(\"\\nCreating visualizations...\")\n",
    "    y_pred = metrics.pop('y_pred')  # Get predictions from metrics\n",
    "    plot_y_test, plot_y_pred, errors = generate_model_evaluation_plots(test_data, y_pred, output_dir)\n",
    "    generate_density_plots(plot_y_test, plot_y_pred, errors, output_dir)\n",
    "    create_actual_vs_predicted_zoomed(plot_y_test, plot_y_pred, output_dir)\n",
    "    create_improved_density_plot(plot_y_test, plot_y_pred, viz_dir)  # Note: Output to viz_dir\n",
    "    \n",
    "    # Generate sample predictions\n",
    "    print(\"\\nGenerating sample predictions...\")\n",
    "    generate_sample_predictions(model, test_data, output_dir)\n",
    "\n",
    "    # Create enhanced training history visualization\n",
    "    create_enhanced_training_history(history, viz_dir)\n",
    "    \n",
    "    # Print summary\n",
    "    batch_size = 128  # Define this for the summary\n",
    "    print(\"\\n\")\n",
    "    print(\"MODEL EVALUATION SUMMARY\")\n",
    "    print(f\"GPU Acceleration: {'Enabled' if len(tf.config.list_physical_devices('GPU')) > 0 else 'Disabled'}\")\n",
    "    print(f\"Precision: {tf.keras.mixed_precision.global_policy().name}\")\n",
    "    print(f\"Batch Size for Prediction: {batch_size}\")\n",
    "    print(f\"Target Transformation: Log(x+1) → Inverse: exp(x)-1\")\n",
    "    print(f\"Visualization Directory: {viz_dir}\")\n",
    "    \n",
    "    print(\"\\nTest Metrics:\")\n",
    "    print(f\"MSE: {metrics['mse']:.4f}\")\n",
    "    print(f\"RMSE: {metrics['rmse']:.4f}\")\n",
    "    print(f\"MAE: {metrics['mae']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "\n",
    "    print(\"\\nAll visualizations have been saved.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "title": "Data Extraction Pipeline"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from scipy.stats import gaussian_kde\n",
    "import gc\n",
    "\n",
    "# visualization defaults\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Check for GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPU Available:\", len(physical_devices) > 0)\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"GPU Devices:\", physical_devices)\n",
    "    \n",
    "    # Set memory growth for better GPU memory utilization\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    try:\n",
    "        tf.config.optimizer.set_jit(True)\n",
    "        print(\"XLA JIT compilation enabled\")\n",
    "    except:\n",
    "        print(\"XLA JIT compilation not available\")\n",
    "\n",
    "# Set to float32 for stability\n",
    "tf.keras.mixed_precision.set_global_policy('float32')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0005,  # Reduced learning rate for stability\n",
    "    clipnorm=0.5,          # Use gradient clipping\n",
    "    epsilon=1e-7\n",
    ")\n",
    "\n",
    "output_dir = os.path.join('private', 'data', 'training_data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'visualizations'), exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "input_file_path = os.path.join('private', 'data', 'engineered', 'weekly_sequence_engineered.parquet')\n",
    "df = pd.read_parquet(input_file_path)\n",
    "gc.collect()  # Free memory after loading data\n",
    "\n",
    "print(\"Available columns in the DataFrame:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Issues with year\n",
    "# Process year info\n",
    "if 'Year' not in df.columns:\n",
    "    if 'WeekStart' in df.columns:\n",
    "        try:\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df['WeekStart']):\n",
    "                df['WeekStart'] = pd.to_datetime(df['WeekStart'])\n",
    "            df['Year'] = df['WeekStart'].dt.year\n",
    "            print(\"Created 'Year' column from 'WeekStart'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create 'Year' column from 'WeekStart': {e}\")\n",
    "    else:\n",
    "        print(\"'Year' column not found and no 'WeekStart' column available. Skipping year filtering.\")\n",
    "\n",
    "# Filter by Year if it exists\n",
    "if 'Year' in df.columns:\n",
    "    recent_years = [2022, 2023, 2024]\n",
    "    df = df[df['Year'].isin(recent_years)].copy()\n",
    "    print(f\"Filtered to {len(df)} rows from years {recent_years}\")\n",
    "else:\n",
    "    print(\"Cannot filter by year, continuing with full dataset\")\n",
    "\n",
    "\n",
    "# Add enhanced features - optimized and stabilized\n",
    "def add_enhanced_features(df):    \n",
    "    # Check if WeekStart is a datetime type\n",
    "    if 'WeekStart' in df.columns:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['WeekStart']):\n",
    "            df['WeekStart'] = pd.to_datetime(df['WeekStart'])\n",
    "    \n",
    "    # Create time features\n",
    "    if 'Month' not in df.columns and 'WeekStart' in df.columns:\n",
    "        try:\n",
    "            # Extract month and quarter from WeekStart\n",
    "            df['Month'] = df['WeekStart'].dt.month\n",
    "            df['Quarter'] = df['WeekStart'].dt.quarter\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create time columns from 'WeekStart': {e}\")\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    if 'Month' in df.columns:\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['Month']/12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['Month']/12)\n",
    "    else:\n",
    "        print(\"'Month' column not available, skipping month cyclical encoding\")\n",
    "    \n",
    "    if 'Quarter' in df.columns:\n",
    "        df['quarter_sin'] = np.sin(2 * np.pi * df['Quarter']/4)\n",
    "        df['quarter_cos'] = np.cos(2 * np.pi * df['Quarter']/4)\n",
    "    else:\n",
    "        print(\"'Quarter' column not available, skipping quarter cyclical encoding\")\n",
    "    \n",
    "    # Calculate maintenance rate\n",
    "    if all(col in df.columns for col in ['ReceivedCount', 'RunningFleetTotal']):\n",
    "        # Clip to prevent outliers\n",
    "        received_clipped = np.clip(df['ReceivedCount'], 0, df['ReceivedCount'].quantile(0.99))\n",
    "        fleet_clipped = np.maximum(df['RunningFleetTotal'], 1)\n",
    "        df['maintenance_rate'] = received_clipped / fleet_clipped\n",
    "    else:\n",
    "        print(\"Required columns for maintenance_rate calculation not available\")\n",
    "    \n",
    "    # Add volatility measure\n",
    "    if 'ReceivedCount' in df.columns:\n",
    "        received_clipped = df.groupby('PartNumber')['ReceivedCount'].transform(\n",
    "            lambda x: np.clip(x, 0, x.quantile(0.99) if len(x) > 0 else np.max(x))\n",
    "        )\n",
    "        \n",
    "        # Calculate rolling stats\n",
    "        for window in [4, 12]:\n",
    "            col_name = f'ReceivedCount_roll{window}w_std'\n",
    "            if col_name not in df.columns:\n",
    "                try:\n",
    "                    df[col_name] = df.groupby('PartNumber')['ReceivedCount'].transform(\n",
    "                        lambda x: x.rolling(window, min_periods=1).std()\n",
    "                    ).fillna(0)\n",
    "                    \n",
    "                    # Cap to prevent extreme values\n",
    "                    std_99th = df[col_name].quantile(0.99)\n",
    "                    df[col_name] = df[col_name].clip(0, std_99th)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not calculate rolling std for window {window}: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add enhanced features\n",
    "df = add_enhanced_features(df)\n",
    "gc.collect() \n",
    "\n",
    "print(\"Filtering parts with minimum maintenance events...\")\n",
    "min_repairs = 10\n",
    "\n",
    "# More efficient part filtering\n",
    "part_maintenance_sums = df.groupby('PartNumber')['ReceivedCount'].sum()\n",
    "parts_with_maintenance = part_maintenance_sums[part_maintenance_sums >= min_repairs].index\n",
    "\n",
    "print(f\"Parts with at least {min_repairs} maintenance events: {len(parts_with_maintenance)} out of {len(part_maintenance_sums)} total parts\")\n",
    "df = df[df['PartNumber'].isin(parts_with_maintenance)].copy()\n",
    "print(f\"Dataset now has {len(df)} rows after filtering to parts with substantial maintenance history\")\n",
    "gc.collect() \n",
    "\n",
    "# Enhanced feature selection with focus on stability\n",
    "enhanced_features = [\n",
    "    # Core identifiers\n",
    "    'PartNumber', 'Year', 'Month', 'Quarter', 'WeekStart',\n",
    "    'ReceivedCount',  # Target\n",
    "    # Original important features\n",
    "    'RunningFleetTotal',\n",
    "    'ReceivedCount_roll12w_mean',\n",
    "    'ReceivedCount_lag1w',\n",
    "    'ShippedCount',\n",
    "    'InService',\n",
    "    # New cyclical time features\n",
    "    'month_sin', 'month_cos', 'quarter_sin', 'quarter_cos',\n",
    "    # Volatility features\n",
    "    'ReceivedCount_roll4w_std', 'ReceivedCount_roll12w_std',\n",
    "    'maintenance_rate'\n",
    "]\n",
    "\n",
    "existing_features = [col for col in enhanced_features if col in df.columns]\n",
    "print(f\"Using {len(existing_features)} features: {existing_features}\")\n",
    "reduced_df = df[existing_features].copy()\n",
    "del df  # Delete original dataframe to free memory\n",
    "gc.collect()  \n",
    "\n",
    "print(\"Checking and cleaning data...\")\n",
    "numeric_cols = reduced_df.select_dtypes(include=['number']).columns\n",
    "\n",
    "print(\"Applying log(x+1) transform to target for better stability\")\n",
    "reduced_df['ReceivedCount_original'] = reduced_df['ReceivedCount'].copy()  # Save original for later\n",
    "reduced_df['ReceivedCount'] = np.log1p(reduced_df['ReceivedCount'])  # log(x+1) transformation\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(reduced_df['ReceivedCount_original'], bins=30, kde=True, color='blue')\n",
    "plt.title('Original Maintenance Event Distribution')\n",
    "plt.xlabel('Maintenance Events Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(reduced_df['ReceivedCount'], bins=30, kde=True, color='green')\n",
    "plt.title('Log-Transformed Maintenance Event Distribution')\n",
    "plt.xlabel('Log(Maintenance Events + 1)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'visualizations', 'target_transformation.png'))\n",
    "plt.close()\n",
    "\n",
    "# Find and replace outliers\n",
    "for col in numeric_cols:\n",
    "    if col in ['Year', 'Month', 'Quarter', 'PartNumber', 'time_id']:\n",
    "        continue  # Skip identifier columns\n",
    "    \n",
    "    # Detect infinities\n",
    "    inf_mask = np.isinf(reduced_df[col])\n",
    "    if inf_mask.any():\n",
    "        print(f\"{inf_mask.sum()} infinite values detected in {col} - replacing\")\n",
    "        reduced_df.loc[inf_mask, col] = np.nan\n",
    "    \n",
    "    # Detect NaNs\n",
    "    nan_mask = np.isnan(reduced_df[col])\n",
    "    if nan_mask.any():\n",
    "        print(f\"{nan_mask.sum()} NaN values detected in {col} - filling with 0\")\n",
    "        reduced_df.loc[nan_mask, col] = 0\n",
    "    \n",
    "    # Handle extreme values\n",
    "    if col != 'ReceivedCount' and col != 'ReceivedCount_original':  # Don't clip target\n",
    "        Q1 = reduced_df[col].quantile(0.25)\n",
    "        Q3 = reduced_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 3 * IQR\n",
    "        upper_bound = Q3 + 3 * IQR\n",
    "        \n",
    "        outliers = (reduced_df[col] < lower_bound) | (reduced_df[col] > upper_bound)\n",
    "        if outliers.any():\n",
    "            print(f\"{outliers.sum()} outliers detected in {col} - capping to IQR bounds\")\n",
    "            reduced_df.loc[outliers, col] = np.clip(\n",
    "                reduced_df.loc[outliers, col], lower_bound, upper_bound)\n",
    "\n",
    "# Create temporal identifier for sorting\n",
    "reduced_df['time_id'] = reduced_df['Year'] * 100 + reduced_df['Month']\n",
    "\n",
    "# Analyze time distribution\n",
    "print(\"\\nAnalyzing time distribution:\")\n",
    "time_counts = reduced_df['time_id'].value_counts().sort_index()\n",
    "print(f\"Earliest time period: {time_counts.index.min()}\")\n",
    "print(f\"Latest time period: {time_counts.index.max()}\")\n",
    "print(f\"Number of time periods: {len(time_counts)}\")\n",
    "\n",
    "# Prepare Time Based Sequences\n",
    "def prepare_time_based_sequences(df, sequence_length=8, target_column='ReceivedCount', batch_size=32,\n",
    "                                train_ratio=0.7, val_ratio=0.15):\n",
    "    \n",
    "    # Get numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    numeric_cols = [col for col in numeric_cols if col != target_column \n",
    "                   and col != 'time_id' \n",
    "                   and col != 'ReceivedCount_original']\n",
    "    \n",
    "    # Sort by time\n",
    "    df = df.sort_values(['PartNumber', 'time_id'])\n",
    "    # Get unique time periods\n",
    "    unique_times = np.sort(df['time_id'].unique())\n",
    "    # Calculate split points\n",
    "    if len(unique_times) < 10:\n",
    "        raise ValueError(\"Not enough time periods for splitting\")\n",
    "        \n",
    "    train_split = int(len(unique_times) * train_ratio)\n",
    "    val_split = int(len(unique_times) * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_times = unique_times[:train_split]\n",
    "    val_times = unique_times[train_split:val_split]\n",
    "    test_times = unique_times[val_split:]\n",
    "    \n",
    "    print(f\"\\nTime-based split:\")\n",
    "    print(f\"Training periods: {len(train_times)} periods from {train_times[0]} to {train_times[-1]}\")\n",
    "    print(f\"Validation periods: {len(val_times)} periods from {val_times[0]} to {val_times[-1]}\")\n",
    "    print(f\"Test periods: {len(test_times)} periods from {test_times[0]} to {test_times[-1]}\")\n",
    "    \n",
    "    # Create train/val/test datasets\n",
    "    train_mask = np.isin(df['time_id'], train_times)\n",
    "    val_mask = np.isin(df['time_id'], val_times)\n",
    "    test_mask = np.isin(df['time_id'], test_times)\n",
    "    \n",
    "    train_df = df[train_mask].copy()\n",
    "    val_df = df[val_mask].copy()\n",
    "    test_df = df[test_mask].copy()\n",
    "    \n",
    "    print(f\"\\nRows in each split:\")\n",
    "    print(f\"Training set: {len(train_df)} rows ({len(train_df) / len(df):.1%})\")\n",
    "    print(f\"Validation set: {len(val_df)} rows ({len(val_df) / len(df):.1%})\")\n",
    "    print(f\"Test set: {len(test_df)} rows ({len(test_df) / len(df):.1%})\")\n",
    "    \n",
    "    # Check part coverage\n",
    "    train_parts = set(train_df['PartNumber'].unique())\n",
    "    val_parts = set(val_df['PartNumber'].unique())\n",
    "    test_parts = set(test_df['PartNumber'].unique())\n",
    "    \n",
    "    print(f\"\\nPart coverage:\")\n",
    "    print(f\"Parts in training set: {len(train_parts)}\")\n",
    "    print(f\"Parts in validation set: {len(val_parts)}\")\n",
    "    print(f\"Parts in test set: {len(test_parts)}\")\n",
    "    \n",
    "    # Find parts that don't appear in all splits\n",
    "    missing_in_val = train_parts - val_parts\n",
    "    missing_in_test = train_parts - test_parts\n",
    "    \n",
    "    print(f\"Parts in training but not in validation: {len(missing_in_val)} ({len(missing_in_val) / len(train_parts):.1%})\")\n",
    "    print(f\"Parts in training but not in test: {len(missing_in_test)} ({len(missing_in_test) / len(train_parts):.1%})\")\n",
    "\n",
    "    # sequence creation with better error checking\n",
    "    def create_sequences_from_df(input_df, max_sequences_per_part=None):\n",
    "        X_sequences = []\n",
    "        y_values = []\n",
    "        y_original_values = []  # Store original (non-transformed) values\n",
    "        part_ids = []\n",
    "        time_periods = []  # Add this line to store time periods\n",
    "        \n",
    "        # Process in batches\n",
    "        unique_parts = input_df['PartNumber'].unique()\n",
    "        batch_size = 200\n",
    "        \n",
    "        # Create batches\n",
    "        for batch_idx in range(0, len(unique_parts), batch_size):\n",
    "            batch_parts = unique_parts[batch_idx:batch_idx + batch_size]\n",
    "            batch_df = input_df[input_df['PartNumber'].isin(batch_parts)]\n",
    "            \n",
    "            for part_id in batch_parts:\n",
    "                part_data = batch_df[batch_df['PartNumber'] == part_id].sort_values('time_id')\n",
    "                \n",
    "                if len(part_data) <= sequence_length:\n",
    "                    continue\n",
    "                \n",
    "                # Extract features and target\n",
    "                features = part_data[numeric_cols].values\n",
    "                targets = part_data[target_column].values\n",
    "                original_targets = part_data['ReceivedCount_original'].values\n",
    "                time_ids = part_data['time_id'].values  # Get time ids for each row\n",
    "                \n",
    "                # Additional validation for infinite or NaN values\n",
    "                if np.isnan(features).any() or np.isinf(features).any():\n",
    "                    print(f\"NaN/Inf values found in features for part {part_id}. Skipping.\")\n",
    "                    continue\n",
    "                    \n",
    "                if np.isnan(targets).any() or np.isinf(targets).any():\n",
    "                    print(f\"NaN/Inf values found in targets for part {part_id}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Check for unreasonable feature values\n",
    "                if np.abs(features).max() > 1e4:\n",
    "                    print(f\"Extreme feature values found for part {part_id}. Capping to Â±1e4.\")\n",
    "                    features = np.clip(features, -1e4, 1e4)\n",
    "                \n",
    "                # Create sequences with a limit per part\n",
    "                count = 0\n",
    "                for i in range(len(part_data) - sequence_length):\n",
    "                    X_sequences.append(features[i:i+sequence_length])\n",
    "                    y_values.append(targets[i+sequence_length])\n",
    "                    y_original_values.append(original_targets[i+sequence_length])\n",
    "                    part_ids.append(part_id)\n",
    "                    time_periods.append(time_ids[i+sequence_length])  # Store the prediction time period\n",
    "                    \n",
    "                    count += 1\n",
    "                    if max_sequences_per_part and count >= max_sequences_per_part:\n",
    "                        break\n",
    "            \n",
    "            del batch_df\n",
    "            \n",
    "            # Clean memory every 500 batches\n",
    "            if batch_idx % 500 == 0 and batch_idx > 0:\n",
    "                gc.collect()\n",
    "                print(f\"Processed {batch_idx}/{len(unique_parts)} parts\")\n",
    "        \n",
    "        if not X_sequences:\n",
    "            return None, None, None, None, None\n",
    "        \n",
    "        X = np.array(X_sequences, dtype=np.float32)\n",
    "        y = np.array(y_values, dtype=np.float32)\n",
    "        y_original = np.array(y_original_values, dtype=np.float32)\n",
    "        part_ids_array = np.array(part_ids)\n",
    "        time_periods_array = np.array(time_periods)\n",
    "        \n",
    "        # Replace any remaining NaN/Inf with zeros\n",
    "        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Clear lists\n",
    "        del X_sequences, y_values, y_original_values\n",
    "        gc.collect()\n",
    "        \n",
    "        return X, y, y_original, part_ids_array, time_periods_array\n",
    "    \n",
    "    # Create sequences for each split even distribution of parts\n",
    "    max_sequences_per_part = 100 \n",
    "    \n",
    "    print(\"Creating training sequences...\")\n",
    "    X_train, y_train, y_train_original, train_part_ids, train_time_periods = create_sequences_from_df(\n",
    "        train_df, max_sequences_per_part)\n",
    "    del train_df  # Free memory\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Creating validation sequences...\")\n",
    "    X_val, y_val, y_val_original, val_part_ids, val_time_periods = create_sequences_from_df(\n",
    "        val_df, max_sequences_per_part)\n",
    "    del val_df  # Free memory\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Creating test sequences...\")\n",
    "    X_test, y_test, y_test_original, test_part_ids, test_time_periods = create_sequences_from_df(\n",
    "        test_df, max_sequences_per_part)\n",
    "    del test_df  # Free memory\n",
    "    gc.collect()\n",
    "    \n",
    "    # Check if we have sequences\n",
    "    if X_train is None or X_val is None or X_test is None:\n",
    "        raise ValueError(\"One or more data splits resulted in no sequences\")\n",
    "    \n",
    "    print(f\"\\nSequence counts:\")\n",
    "    print(f\"Training set: {len(X_train)} sequences\")\n",
    "    print(f\"Validation set: {len(X_val)} sequences\")\n",
    "    print(f\"Test set: {len(X_test)} sequences\")\n",
    "    \n",
    "    print(f\"\\nTransformed target statistics:\")\n",
    "    print(f\"Train targets: min={y_train.min():.2f}, max={y_train.max():.2f}, mean={y_train.mean():.2f}, median={np.median(y_train):.2f}\")\n",
    "    print(f\"Validation targets: min={y_val.min():.2f}, max={y_val.max():.2f}, mean={y_val.mean():.2f}, median={np.median(y_val):.2f}\")\n",
    "    print(f\"Test targets: min={y_test.min():.2f}, max={y_test.max():.2f}, mean={y_test.mean():.2f}, median={np.median(y_test):.2f}\")\n",
    "    \n",
    "    print(f\"\\nOriginal target statistics:\")\n",
    "    print(f\"Train targets: min={y_train_original.min():.2f}, max={y_train_original.max():.2f}, mean={y_train_original.mean():.2f}, median={np.median(y_train_original):.2f}\")\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_dataset = train_dataset.cache()\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=min(10000, len(X_train)))\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "    test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return (train_dataset, val_dataset, test_dataset, X_train.shape[1:], \n",
    "            (X_train, y_train, y_train_original, train_part_ids, train_time_periods), \n",
    "            (X_val, y_val, y_val_original, val_part_ids, val_time_periods), \n",
    "            (X_test, y_test, y_test_original, test_part_ids, test_time_periods))\n",
    "\n",
    "# Normalization!\n",
    "# Normalization using RobustScaler\n",
    "exclude_from_normalization = ['Year', 'Month', 'Quarter', 'PartNumber', 'ReceivedCount', 'ReceivedCount_original', 'time_id']\n",
    "numeric_cols = reduced_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col not in exclude_from_normalization]\n",
    "\n",
    "print(f\"Normalizing {len(numeric_cols)} numeric columns with RobustScaler\")\n",
    "# Use RobustScaler instead of StandardScaler - better outlier handling\n",
    "scaler = RobustScaler()\n",
    "reduced_df[numeric_cols] = scaler.fit_transform(reduced_df[numeric_cols])\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 8\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Creating sequences from the dataset...\")\n",
    "try:\n",
    "    train_ds, val_ds, test_ds, input_shape, train_data, val_data, test_data = prepare_time_based_sequences(\n",
    "        reduced_df, sequence_length, 'ReceivedCount', batch_size, train_ratio=0.7, val_ratio=0.15)\n",
    "    del reduced_df  \n",
    "    gc.collect()\n",
    "except Exception as e:\n",
    "    print(f\"Error creating sequences: {e}\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "def build_stable_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(32,  \n",
    "             activation='tanh',\n",
    "             recurrent_activation='sigmoid',\n",
    "             recurrent_regularizer=l2(0.001),\n",
    "             kernel_regularizer=l2(0.001),\n",
    "             return_sequences=False,\n",
    "             input_shape=input_shape),  \n",
    "        \n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(16, \n",
    "              activation='relu',\n",
    "              kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',  \n",
    "        metrics=['mae'] \n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train model\n",
    "print(f\"Building model with input shape: {input_shape}\")\n",
    "model = build_stable_lstm_model(input_shape)\n",
    "model.summary()\n",
    "\n",
    "# NaN detection\n",
    "class NaNCallback(tf.keras.callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if logs is not None and (np.isnan(logs.get('loss', 0)) or np.isinf(logs.get('loss', 0))):\n",
    "            print(f\"\\nNaN/Inf loss detected at batch {batch}. Terminating training.\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None and (np.isnan(logs.get('loss', 0)) or np.isinf(logs.get('loss', 0))):\n",
    "            print(f\"\\nNaN/Inf loss detected at end of epoch {epoch}. Terminating training.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Memory cleanup\n",
    "class MemoryCleanupCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, cleanup_frequency=3):\n",
    "        super().__init__()\n",
    "        self.cleanup_frequency = cleanup_frequency\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.cleanup_frequency == 0:\n",
    "            gc.collect()\n",
    "            print(f\"Memory cleanup after epoch {epoch}\")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,  # Increased patience for better convergence\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Use ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,  # Increased patience for LR reduction\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Checkpoint to save best model\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=os.path.join(output_dir, 'time_based_maintenance_model.h5'),\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "nan_callback = NaNCallback()\n",
    "memory_cleanup = MemoryCleanupCallback(cleanup_frequency=3)\n",
    "\n",
    "print(\"Training model...\")\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=100,  # Increased epochs (from 40 to 100)\n",
    "        callbacks=[\n",
    "            model_checkpoint, \n",
    "            early_stopping, \n",
    "            reduce_lr,  # More stable LR reduction\n",
    "            nan_callback, \n",
    "            memory_cleanup\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "    gc.collect()  \n",
    "    \n",
    "    # Check if training completed without NaNs\n",
    "    if history.history['loss'] and any(np.isnan(loss) for loss in history.history['loss']):\n",
    "        print(\"NaN values detected in training loss history\")\n",
    "    \n",
    "    # Save training history\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_csv_file = os.path.join(output_dir, 'training_history.csv')\n",
    "    hist_df.to_csv(hist_csv_file, index=False)\n",
    "    print(f\"Training history saved to {hist_csv_file}\")\n",
    "    \n",
    "    # Save model\n",
    "    model.save(os.path.join(output_dir, 'time_based_maintenance_model.h5'))\n",
    "    print(f\"Model saved to {os.path.join(output_dir, 'time_based_maintenance_model.h5')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "test_data_path = os.path.join(output_dir, 'test_data.npz')\n",
    "np.savez(test_data_path,\n",
    "         X_test=test_data[0],\n",
    "         y_test=test_data[1],\n",
    "         y_test_original=test_data[2],\n",
    "         part_ids=test_data[3],  # Add the part IDs\n",
    "         time_periods=test_data[4])  # Add time period information\n",
    "print(f\"Saved test data to {test_data_path}\")\n",
    "print(f\"Saved test data to {test_data_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "title": "Data Extraction Pipeline"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

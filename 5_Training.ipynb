{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 2\n",
    "\n",
    "--- \n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 20:48:19.688020: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-16 20:48:20.135060: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744861700.285691    5165 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744861700.332475    5165 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744861700.707806    5165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744861700.707843    5165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744861700.707845    5165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744861700.707847    5165 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-16 20:48:20.748543: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n",
      "GPU Devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "XLA JIT compilation enabled\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1744861707.148057    5165 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6140 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns in the DataFrame:\n",
      "['InRepair', 'RawResets', 'FlightDuration', 'ReceivedCount', 'WeekStart', 'TotalPassengers', 'ShippedCount', 'PartNumber', 'Tail', 'PartNumber_encoded', 'Tail_encoded', 'ReceivedCount_roll4w_mean', 'ReceivedCount_roll4w_std', 'ReceivedCount_roll4w_max', 'ReceivedCount_roll8w_mean', 'ReceivedCount_roll8w_std', 'ReceivedCount_roll8w_max', 'ReceivedCount_roll12w_mean', 'ReceivedCount_roll12w_std', 'ReceivedCount_roll12w_max', 'ReceivedCount_roll26w_mean', 'ReceivedCount_roll26w_std', 'ReceivedCount_roll26w_max', 'ReceivedCount_roll52w_mean', 'ReceivedCount_roll52w_std', 'ReceivedCount_roll52w_max', 'ShippedCount_roll4w_mean', 'ShippedCount_roll4w_std', 'ShippedCount_roll4w_max', 'ShippedCount_roll8w_mean', 'ShippedCount_roll8w_std', 'ShippedCount_roll8w_max', 'ShippedCount_roll12w_mean', 'ShippedCount_roll12w_std', 'ShippedCount_roll12w_max', 'ShippedCount_roll26w_mean', 'ShippedCount_roll26w_std', 'ShippedCount_roll26w_max', 'ShippedCount_roll52w_mean', 'ShippedCount_roll52w_std', 'ShippedCount_roll52w_max', 'InRepair_roll4w_mean', 'InRepair_roll4w_std', 'InRepair_roll4w_max', 'InRepair_roll8w_mean', 'InRepair_roll8w_std', 'InRepair_roll8w_max', 'InRepair_roll12w_mean', 'InRepair_roll12w_std', 'InRepair_roll12w_max', 'InRepair_roll26w_mean', 'InRepair_roll26w_std', 'InRepair_roll26w_max', 'InRepair_roll52w_mean', 'InRepair_roll52w_std', 'InRepair_roll52w_max', 'RawResets_roll4w_mean', 'RawResets_roll4w_std', 'RawResets_roll4w_max', 'RawResets_roll8w_mean', 'RawResets_roll8w_std', 'RawResets_roll8w_max', 'RawResets_roll12w_mean', 'RawResets_roll12w_std', 'RawResets_roll12w_max', 'RawResets_roll26w_mean', 'RawResets_roll26w_std', 'RawResets_roll26w_max', 'RawResets_roll52w_mean', 'RawResets_roll52w_std', 'RawResets_roll52w_max', 'TotalPassengers_roll4w_mean', 'TotalPassengers_roll4w_std', 'TotalPassengers_roll4w_max', 'TotalPassengers_roll8w_mean', 'TotalPassengers_roll8w_std', 'TotalPassengers_roll8w_max', 'TotalPassengers_roll12w_mean', 'TotalPassengers_roll12w_std', 'TotalPassengers_roll12w_max', 'TotalPassengers_roll26w_mean', 'TotalPassengers_roll26w_std', 'TotalPassengers_roll26w_max', 'TotalPassengers_roll52w_mean', 'TotalPassengers_roll52w_std', 'TotalPassengers_roll52w_max', 'FlightDuration_roll4w_mean', 'FlightDuration_roll4w_std', 'FlightDuration_roll4w_max', 'FlightDuration_roll8w_mean', 'FlightDuration_roll8w_std', 'FlightDuration_roll8w_max', 'FlightDuration_roll12w_mean', 'FlightDuration_roll12w_std', 'FlightDuration_roll12w_max', 'FlightDuration_roll26w_mean', 'FlightDuration_roll26w_std', 'FlightDuration_roll26w_max', 'FlightDuration_roll52w_mean', 'FlightDuration_roll52w_std', 'FlightDuration_roll52w_max', 'RawResets_byPart_roll4w_mean', 'RawResets_byPart_roll8w_mean', 'RawResets_byPart_roll12w_mean', 'RawResets_byPart_roll26w_mean', 'RawResets_byPart_roll52w_mean', 'TotalPassengers_byPart_roll4w_mean', 'TotalPassengers_byPart_roll8w_mean', 'TotalPassengers_byPart_roll12w_mean', 'TotalPassengers_byPart_roll26w_mean', 'TotalPassengers_byPart_roll52w_mean', 'FlightDuration_byPart_roll4w_mean', 'FlightDuration_byPart_roll8w_mean', 'FlightDuration_byPart_roll12w_mean', 'FlightDuration_byPart_roll26w_mean', 'FlightDuration_byPart_roll52w_mean', 'ReceivedCount_lag1w', 'ShippedCount_lag1w', 'InRepair_lag1w', 'RawResets_byTail_lag1w', 'TotalPassengers_byTail_lag1w', 'FlightDuration_byTail_lag1w', 'ReceivedCount_lag2w', 'ShippedCount_lag2w', 'InRepair_lag2w', 'RawResets_byTail_lag2w', 'TotalPassengers_byTail_lag2w', 'FlightDuration_byTail_lag2w', 'ReceivedCount_lag4w', 'ShippedCount_lag4w', 'InRepair_lag4w', 'RawResets_byTail_lag4w', 'TotalPassengers_byTail_lag4w', 'FlightDuration_byTail_lag4w', 'ReceivedCount_lag8w', 'ShippedCount_lag8w', 'InRepair_lag8w', 'RawResets_byTail_lag8w', 'TotalPassengers_byTail_lag8w', 'FlightDuration_byTail_lag8w', 'CumulativeReceivedCount', 'CumulativeShippedCount', 'CumulativeInRepair', 'CumulativeRawResets_byTail', 'CumulativeTotalPassengers_byTail', 'CumulativeFlightDuration_byTail', 'ResetsPerFlightHour', 'PassengersPerReset']\n",
      "Created 'Year' column from 'WeekStart'\n",
      "Filtered to 1394790 rows from years [2022, 2023, 2024]\n",
      "Required columns for repair_rate calculation not available\n",
      "Filtering parts with 10 repairs...\n",
      "Parts with at least 10 repairs: 83 out of 316 total parts\n",
      "Dataset now has 801442 rows after filtering to parts with substantial repair history\n",
      "Using 16 features: ['PartNumber', 'Year', 'Month', 'Quarter', 'WeekStart', 'ReceivedCount', 'ReceivedCount_roll12w_mean', 'ReceivedCount_lag1w', 'ShippedCount', 'InRepair', 'month_sin', 'month_cos', 'quarter_sin', 'quarter_cos', 'ReceivedCount_roll4w_std', 'ReceivedCount_roll12w_std']\n",
      "Checking and cleaning data...\n",
      "Applying log(x+1) transform to target for better stability\n",
      "76528 outliers detected in ReceivedCount_roll12w_mean - capping to IQR bounds\n",
      "2 NaN values detected in ReceivedCount_lag1w - filling with 0\n",
      "75682 outliers detected in ReceivedCount_lag1w - capping to IQR bounds\n",
      "80763 outliers detected in ShippedCount - capping to IQR bounds\n",
      "62880 outliers detected in InRepair - capping to IQR bounds\n",
      "2 NaN values detected in ReceivedCount_roll4w_std - filling with 0\n",
      "12065 outliers detected in ReceivedCount_roll4w_std - capping to IQR bounds\n",
      "2 NaN values detected in ReceivedCount_roll12w_std - filling with 0\n",
      "37736 outliers detected in ReceivedCount_roll12w_std - capping to IQR bounds\n",
      "\n",
      "Analyzing time distribution:\n",
      "Earliest time period: 202201\n",
      "Latest time period: 202412\n",
      "Number of time periods: 36\n",
      "Normalizing 7 numeric columns with RobustScaler\n",
      "Creating sequences from the dataset...\n",
      "\n",
      "Time-based split:\n",
      "Training periods: 25 periods from 202201 to 202401\n",
      "Validation periods: 5 periods from 202402 to 202406\n",
      "Test periods: 6 periods from 202407 to 202412\n",
      "\n",
      "Rows in each split:\n",
      "Training set: 554664 rows (69.2%)\n",
      "Validation set: 108144 rows (13.5%)\n",
      "Test set: 138634 rows (17.3%)\n",
      "\n",
      "Part coverage:\n",
      "Parts in training set: 82\n",
      "Parts in validation set: 83\n",
      "Parts in test set: 83\n",
      "Parts in training but not in validation: 0 (0.0%)\n",
      "Parts in training but not in test: 0 (0.0%)\n",
      "Creating training sequences...\n",
      "Creating validation sequences...\n",
      "Creating test sequences...\n",
      "\n",
      "Sequence counts:\n",
      "Training set: 8200 sequences\n",
      "Validation set: 7686 sequences\n",
      "Test set: 7895 sequences\n",
      "\n",
      "Transformed target statistics:\n",
      "Train targets: min=0.00, max=5.75, mean=0.29, median=0.00\n",
      "Validation targets: min=0.00, max=4.97, mean=0.39, median=0.00\n",
      "Test targets: min=0.00, max=4.88, mean=0.57, median=0.00\n",
      "\n",
      "Original target statistics:\n",
      "Train targets: min=0.00, max=313.00, mean=1.62, median=0.00\n"
     ]
    }
   ],
   "source": [
    "# Part 1\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from scipy.stats import gaussian_kde\n",
    "import gc\n",
    "\n",
    "# visualization defaults\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# Check for GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"GPU Available:\", len(physical_devices) > 0)\n",
    "if len(physical_devices) > 0:\n",
    "    print(\"GPU Devices:\", physical_devices)\n",
    "    \n",
    "    # Set memory growth for better GPU memory utilization\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)\n",
    "    try:\n",
    "        tf.config.optimizer.set_jit(True)\n",
    "        print(\"XLA JIT compilation enabled\")\n",
    "    except:\n",
    "        print(\"XLA JIT compilation not available\")\n",
    "\n",
    "# Set to float32 for stability\n",
    "tf.keras.mixed_precision.set_global_policy('float32')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.0005,  # Reduced learning rate for stability\n",
    "    clipnorm=0.5,          # Use gradient clipping\n",
    "    epsilon=1e-7\n",
    ")\n",
    "\n",
    "output_dir = os.path.join('private', 'data', 'training_data')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_dir, 'visualizations'), exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "input_file_path = os.path.join('private', 'data', 'engineered', 'weekly_sequence_engineered.parquet')\n",
    "df = pd.read_parquet(input_file_path)\n",
    "gc.collect()  # Free memory after loading data\n",
    "\n",
    "print(\"Available columns in the DataFrame:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Issues with year\n",
    "# Process year info\n",
    "if 'Year' not in df.columns:\n",
    "    if 'WeekStart' in df.columns:\n",
    "        try:\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df['WeekStart']):\n",
    "                df['WeekStart'] = pd.to_datetime(df['WeekStart'])\n",
    "            df['Year'] = df['WeekStart'].dt.year\n",
    "            print(\"Created 'Year' column from 'WeekStart'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create 'Year' column from 'WeekStart': {e}\")\n",
    "    else:\n",
    "        print(\"'Year' column not found and no 'WeekStart' column available. Skipping year filtering.\")\n",
    "\n",
    "# Filter by Year if it exists\n",
    "if 'Year' in df.columns:\n",
    "    recent_years = [2022, 2023, 2024]\n",
    "    df = df[df['Year'].isin(recent_years)].copy()\n",
    "    print(f\"Filtered to {len(df)} rows from years {recent_years}\")\n",
    "else:\n",
    "    print(\"Cannot filter by year, continuing with full dataset\")\n",
    "\n",
    "\n",
    "# Add enhanced features - optimized and stabilized\n",
    "def add_enhanced_features(df):    \n",
    "    # Check if WeekStart is a datetime type\n",
    "    if 'WeekStart' in df.columns:\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['WeekStart']):\n",
    "            df['WeekStart'] = pd.to_datetime(df['WeekStart'])\n",
    "    \n",
    "    # Create time features\n",
    "    if 'Month' not in df.columns and 'WeekStart' in df.columns:\n",
    "        try:\n",
    "            # Extract month and quarter from WeekStart\n",
    "            df['Month'] = df['WeekStart'].dt.month\n",
    "            df['Quarter'] = df['WeekStart'].dt.quarter\n",
    "        except Exception as e:\n",
    "            print(f\"Could not create time columns from 'WeekStart': {e}\")\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    if 'Month' in df.columns:\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['Month']/12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['Month']/12)\n",
    "    else:\n",
    "        print(\"'Month' column not available, skipping month cyclical encoding\")\n",
    "    \n",
    "    if 'Quarter' in df.columns:\n",
    "        df['quarter_sin'] = np.sin(2 * np.pi * df['Quarter']/4)\n",
    "        df['quarter_cos'] = np.cos(2 * np.pi * df['Quarter']/4)\n",
    "    else:\n",
    "        print(\"'Quarter' column not available, skipping quarter cyclical encoding\")\n",
    "    \n",
    "    # Calculate repair rate\n",
    "    if all(col in df.columns for col in ['ReceivedCount', 'RunningFleetTotal']):\n",
    "        # Clip to prevent outliers\n",
    "        received_clipped = np.clip(df['ReceivedCount'], 0, df['ReceivedCount'].quantile(0.99))\n",
    "        fleet_clipped = np.maximum(df['RunningFleetTotal'], 1)\n",
    "        df['repair_rate'] = received_clipped / fleet_clipped\n",
    "    else:\n",
    "        print(\"Required columns for repair_rate calculation not available\")\n",
    "    \n",
    "    # Add volatility measure\n",
    "    if 'ReceivedCount' in df.columns:\n",
    "        received_clipped = df.groupby('PartNumber')['ReceivedCount'].transform(\n",
    "            lambda x: np.clip(x, 0, x.quantile(0.99) if len(x) > 0 else np.max(x))\n",
    "        )\n",
    "        \n",
    "        # Calculate rolling stats\n",
    "        for window in [4, 12]:\n",
    "            col_name = f'ReceivedCount_roll{window}w_std'\n",
    "            if col_name not in df.columns:\n",
    "                try:\n",
    "                    df[col_name] = df.groupby('PartNumber')['ReceivedCount'].transform(\n",
    "                        lambda x: x.rolling(window, min_periods=1).std()\n",
    "                    ).fillna(0)\n",
    "                    \n",
    "                    # Cap to prevent extreme values\n",
    "                    std_99th = df[col_name].quantile(0.99)\n",
    "                    df[col_name] = df[col_name].clip(0, std_99th)\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not calculate rolling std for window {window}: {e}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Add enhanced features\n",
    "df = add_enhanced_features(df)\n",
    "gc.collect() \n",
    "\n",
    "print(\"Filtering parts with 10 repairs...\")\n",
    "min_repairs = 10\n",
    "\n",
    "# More efficient part filtering\n",
    "part_repair_sums = df.groupby('PartNumber')['ReceivedCount'].sum()\n",
    "parts_with_repairs = part_repair_sums[part_repair_sums >= min_repairs].index\n",
    "\n",
    "print(f\"Parts with at least {min_repairs} repairs: {len(parts_with_repairs)} out of {len(part_repair_sums)} total parts\")\n",
    "df = df[df['PartNumber'].isin(parts_with_repairs)].copy()\n",
    "print(f\"Dataset now has {len(df)} rows after filtering to parts with substantial repair history\")\n",
    "gc.collect() \n",
    "\n",
    "# Enhanced feature selection with focus on stability\n",
    "enhanced_features = [\n",
    "    # Core identifiers\n",
    "    'PartNumber', 'Year', 'Month', 'Quarter', 'WeekStart',\n",
    "    'ReceivedCount',  # Target\n",
    "    # Original important features\n",
    "    'RunningFleetTotal',\n",
    "    'ReceivedCount_roll12w_mean',\n",
    "    'ReceivedCount_lag1w',\n",
    "    'ShippedCount',\n",
    "    'InRepair',\n",
    "    # New cyclical time features\n",
    "    'month_sin', 'month_cos', 'quarter_sin', 'quarter_cos',\n",
    "    # Volatility features\n",
    "    'ReceivedCount_roll4w_std', 'ReceivedCount_roll12w_std',\n",
    "    'repair_rate'\n",
    "]\n",
    "\n",
    "existing_features = [col for col in enhanced_features if col in df.columns]\n",
    "print(f\"Using {len(existing_features)} features: {existing_features}\")\n",
    "reduced_df = df[existing_features].copy()\n",
    "del df  # Delete original dataframe to free memory\n",
    "gc.collect()  \n",
    "\n",
    "print(\"Checking and cleaning data...\")\n",
    "numeric_cols = reduced_df.select_dtypes(include=['number']).columns\n",
    "\n",
    "print(\"Applying log(x+1) transform to target for better stability\")\n",
    "reduced_df['ReceivedCount_original'] = reduced_df['ReceivedCount'].copy()  # Save original for later\n",
    "reduced_df['ReceivedCount'] = np.log1p(reduced_df['ReceivedCount'])  # log(x+1) transformation\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(reduced_df['ReceivedCount_original'], bins=30, kde=True, color='blue')\n",
    "plt.title('Original Repair Count Distribution')\n",
    "plt.xlabel('Repair Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(reduced_df['ReceivedCount'], bins=30, kde=True, color='green')\n",
    "plt.title('Log-Transformed Repair Count Distribution')\n",
    "plt.xlabel('Log(Repair Count + 1)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(output_dir, 'visualizations', 'target_transformation.png'))\n",
    "plt.close()\n",
    "\n",
    "# Find and replace outliers\n",
    "for col in numeric_cols:\n",
    "    if col in ['Year', 'Month', 'Quarter', 'PartNumber', 'time_id']:\n",
    "        continue  # Skip identifier columns\n",
    "    \n",
    "    # Detect infinities\n",
    "    inf_mask = np.isinf(reduced_df[col])\n",
    "    if inf_mask.any():\n",
    "        print(f\"{inf_mask.sum()} infinite values detected in {col} - replacing\")\n",
    "        reduced_df.loc[inf_mask, col] = np.nan\n",
    "    \n",
    "    # Detect NaNs\n",
    "    nan_mask = np.isnan(reduced_df[col])\n",
    "    if nan_mask.any():\n",
    "        print(f\"{nan_mask.sum()} NaN values detected in {col} - filling with 0\")\n",
    "        reduced_df.loc[nan_mask, col] = 0\n",
    "    \n",
    "    # Handle extreme values\n",
    "    if col != 'ReceivedCount' and col != 'ReceivedCount_original':  # Don't clip target\n",
    "        Q1 = reduced_df[col].quantile(0.25)\n",
    "        Q3 = reduced_df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 3 * IQR\n",
    "        upper_bound = Q3 + 3 * IQR\n",
    "        \n",
    "        outliers = (reduced_df[col] < lower_bound) | (reduced_df[col] > upper_bound)\n",
    "        if outliers.any():\n",
    "            print(f\"{outliers.sum()} outliers detected in {col} - capping to IQR bounds\")\n",
    "            reduced_df.loc[outliers, col] = np.clip(\n",
    "                reduced_df.loc[outliers, col], lower_bound, upper_bound)\n",
    "\n",
    "# Create temporal identifier for sorting\n",
    "reduced_df['time_id'] = reduced_df['Year'] * 100 + reduced_df['Month']\n",
    "\n",
    "# Analyze time distribution\n",
    "print(\"\\nAnalyzing time distribution:\")\n",
    "time_counts = reduced_df['time_id'].value_counts().sort_index()\n",
    "print(f\"Earliest time period: {time_counts.index.min()}\")\n",
    "print(f\"Latest time period: {time_counts.index.max()}\")\n",
    "print(f\"Number of time periods: {len(time_counts)}\")\n",
    "\n",
    "# Prepare Time Based Sequences\n",
    "def prepare_time_based_sequences(df, sequence_length=8, target_column='ReceivedCount', batch_size=32,\n",
    "                                train_ratio=0.7, val_ratio=0.15):\n",
    "    \n",
    "    # Get numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "    numeric_cols = [col for col in numeric_cols if col != target_column \n",
    "                   and col != 'time_id' \n",
    "                   and col != 'ReceivedCount_original']\n",
    "    \n",
    "    # Sort by time\n",
    "    df = df.sort_values(['PartNumber', 'time_id'])\n",
    "    # Get unique time periods\n",
    "    unique_times = np.sort(df['time_id'].unique())\n",
    "    # Calculate split points\n",
    "    if len(unique_times) < 10:\n",
    "        raise ValueError(\"Not enough time periods for splitting\")\n",
    "        \n",
    "    train_split = int(len(unique_times) * train_ratio)\n",
    "    val_split = int(len(unique_times) * (train_ratio + val_ratio))\n",
    "    \n",
    "    train_times = unique_times[:train_split]\n",
    "    val_times = unique_times[train_split:val_split]\n",
    "    test_times = unique_times[val_split:]\n",
    "    \n",
    "    print(f\"\\nTime-based split:\")\n",
    "    print(f\"Training periods: {len(train_times)} periods from {train_times[0]} to {train_times[-1]}\")\n",
    "    print(f\"Validation periods: {len(val_times)} periods from {val_times[0]} to {val_times[-1]}\")\n",
    "    print(f\"Test periods: {len(test_times)} periods from {test_times[0]} to {test_times[-1]}\")\n",
    "    \n",
    "    # Create train/val/test datasets\n",
    "    train_mask = np.isin(df['time_id'], train_times)\n",
    "    val_mask = np.isin(df['time_id'], val_times)\n",
    "    test_mask = np.isin(df['time_id'], test_times)\n",
    "    \n",
    "    train_df = df[train_mask].copy()\n",
    "    val_df = df[val_mask].copy()\n",
    "    test_df = df[test_mask].copy()\n",
    "    \n",
    "    print(f\"\\nRows in each split:\")\n",
    "    print(f\"Training set: {len(train_df)} rows ({len(train_df) / len(df):.1%})\")\n",
    "    print(f\"Validation set: {len(val_df)} rows ({len(val_df) / len(df):.1%})\")\n",
    "    print(f\"Test set: {len(test_df)} rows ({len(test_df) / len(df):.1%})\")\n",
    "    \n",
    "    # Check part coverage\n",
    "    train_parts = set(train_df['PartNumber'].unique())\n",
    "    val_parts = set(val_df['PartNumber'].unique())\n",
    "    test_parts = set(test_df['PartNumber'].unique())\n",
    "    \n",
    "    print(f\"\\nPart coverage:\")\n",
    "    print(f\"Parts in training set: {len(train_parts)}\")\n",
    "    print(f\"Parts in validation set: {len(val_parts)}\")\n",
    "    print(f\"Parts in test set: {len(test_parts)}\")\n",
    "    \n",
    "    # Find parts that don't appear in all splits\n",
    "    missing_in_val = train_parts - val_parts\n",
    "    missing_in_test = train_parts - test_parts\n",
    "    \n",
    "    print(f\"Parts in training but not in validation: {len(missing_in_val)} ({len(missing_in_val) / len(train_parts):.1%})\")\n",
    "    print(f\"Parts in training but not in test: {len(missing_in_test)} ({len(missing_in_test) / len(train_parts):.1%})\")\n",
    "\n",
    "    # sequence creation with better error checking\n",
    "    def create_sequences_from_df(input_df, max_sequences_per_part=None):\n",
    "        X_sequences = []\n",
    "        y_values = []\n",
    "        y_original_values = []  # Store original (non-transformed) values\n",
    "        part_ids = []\n",
    "        \n",
    "        # Process in batches\n",
    "        unique_parts = input_df['PartNumber'].unique()\n",
    "        batch_size = 200\n",
    "        \n",
    "        # Create batches\n",
    "        for batch_idx in range(0, len(unique_parts), batch_size):\n",
    "            batch_parts = unique_parts[batch_idx:batch_idx + batch_size]\n",
    "            batch_df = input_df[input_df['PartNumber'].isin(batch_parts)]\n",
    "            \n",
    "            for part_id in batch_parts:\n",
    "                part_data = batch_df[batch_df['PartNumber'] == part_id].sort_values('time_id')\n",
    "                \n",
    "                if len(part_data) <= sequence_length:\n",
    "                    continue\n",
    "                \n",
    "                # Extract features and target\n",
    "                features = part_data[numeric_cols].values\n",
    "                targets = part_data[target_column].values\n",
    "                original_targets = part_data['ReceivedCount_original'].values\n",
    "                \n",
    "                # Additional validation for infinite or NaN values\n",
    "                if np.isnan(features).any() or np.isinf(features).any():\n",
    "                    print(f\"NaN/Inf values found in features for part {part_id}. Skipping.\")\n",
    "                    continue\n",
    "                    \n",
    "                if np.isnan(targets).any() or np.isinf(targets).any():\n",
    "                    print(f\"NaN/Inf values found in targets for part {part_id}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                # Check for unreasonable feature values\n",
    "                if np.abs(features).max() > 1e4:\n",
    "                    print(f\"Extreme feature values found for part {part_id}. Capping to ±1e4.\")\n",
    "                    features = np.clip(features, -1e4, 1e4)\n",
    "                \n",
    "                # Create sequences with a limit per part\n",
    "                count = 0\n",
    "                for i in range(len(part_data) - sequence_length):\n",
    "                    X_sequences.append(features[i:i+sequence_length])\n",
    "                    y_values.append(targets[i+sequence_length])\n",
    "                    y_original_values.append(original_targets[i+sequence_length])\n",
    "                    part_ids.append(part_id)\n",
    "                    \n",
    "                    count += 1\n",
    "                    if max_sequences_per_part and count >= max_sequences_per_part:\n",
    "                        break\n",
    "            \n",
    "            del batch_df\n",
    "            \n",
    "            # Clean memory every 500 batches\n",
    "            if batch_idx % 500 == 0 and batch_idx > 0:\n",
    "                gc.collect()\n",
    "                print(f\"Processed {batch_idx}/{len(unique_parts)} parts\")\n",
    "        \n",
    "        if not X_sequences:\n",
    "            return None, None, None, None\n",
    "        \n",
    "        X = np.array(X_sequences, dtype=np.float32)\n",
    "        y = np.array(y_values, dtype=np.float32)\n",
    "        y_original = np.array(y_original_values, dtype=np.float32)\n",
    "        \n",
    "        # Replace any remaining NaN/Inf with zeros\n",
    "        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Clear lists\n",
    "        del X_sequences, y_values, y_original_values\n",
    "        gc.collect()\n",
    "        \n",
    "        return X, y, y_original, part_ids\n",
    "    \n",
    "# Create sequences for each split even distribution of parts\n",
    "    max_sequences_per_part = 100 \n",
    "    \n",
    "    print(\"Creating training sequences...\")\n",
    "    X_train, y_train, y_train_original, train_part_ids = create_sequences_from_df(\n",
    "        train_df, max_sequences_per_part)\n",
    "    del train_df  # Free memory\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Creating validation sequences...\")\n",
    "    X_val, y_val, y_val_original, val_part_ids = create_sequences_from_df(\n",
    "        val_df, max_sequences_per_part)\n",
    "    del val_df  # Free memory\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"Creating test sequences...\")\n",
    "    X_test, y_test, y_test_original, test_part_ids = create_sequences_from_df(\n",
    "        test_df, max_sequences_per_part)\n",
    "    del test_df  # Free memory\n",
    "    gc.collect()\n",
    "    \n",
    "    # Check if we have sequences\n",
    "    if X_train is None or X_val is None or X_test is None:\n",
    "        raise ValueError(\"One or more data splits resulted in no sequences\")\n",
    "    \n",
    "    print(f\"\\nSequence counts:\")\n",
    "    print(f\"Training set: {len(X_train)} sequences\")\n",
    "    print(f\"Validation set: {len(X_val)} sequences\")\n",
    "    print(f\"Test set: {len(X_test)} sequences\")\n",
    "    \n",
    "    print(f\"\\nTransformed target statistics:\")\n",
    "    print(f\"Train targets: min={y_train.min():.2f}, max={y_train.max():.2f}, mean={y_train.mean():.2f}, median={np.median(y_train):.2f}\")\n",
    "    print(f\"Validation targets: min={y_val.min():.2f}, max={y_val.max():.2f}, mean={y_val.mean():.2f}, median={np.median(y_val):.2f}\")\n",
    "    print(f\"Test targets: min={y_test.min():.2f}, max={y_test.max():.2f}, mean={y_test.mean():.2f}, median={np.median(y_test):.2f}\")\n",
    "    \n",
    "    print(f\"\\nOriginal target statistics:\")\n",
    "    print(f\"Train targets: min={y_train_original.min():.2f}, max={y_train_original.max():.2f}, mean={y_train_original.mean():.2f}, median={np.median(y_train_original):.2f}\")\n",
    "    \n",
    "    # Create TensorFlow datasets\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    train_dataset = train_dataset.cache()\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=min(10000, len(X_train)))\n",
    "    train_dataset = train_dataset.batch(batch_size)\n",
    "    train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    test_dataset = test_dataset.batch(batch_size)\n",
    "    test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return (train_dataset, val_dataset, test_dataset, X_train.shape[1:], \n",
    "            (X_train, y_train, y_train_original), \n",
    "            (X_val, y_val, y_val_original), \n",
    "            (X_test, y_test, y_test_original))\n",
    "\n",
    "# Normalization!\n",
    "# Normalization using RobustScaler\n",
    "exclude_from_normalization = ['Year', 'Month', 'Quarter', 'PartNumber', 'ReceivedCount', 'ReceivedCount_original', 'time_id']\n",
    "numeric_cols = reduced_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "numeric_cols = [col for col in numeric_cols if col not in exclude_from_normalization]\n",
    "\n",
    "print(f\"Normalizing {len(numeric_cols)} numeric columns with RobustScaler\")\n",
    "# Use RobustScaler instead of StandardScaler - better outlier handling\n",
    "scaler = RobustScaler()\n",
    "reduced_df[numeric_cols] = scaler.fit_transform(reduced_df[numeric_cols])\n",
    "\n",
    "# Create sequences\n",
    "sequence_length = 8\n",
    "batch_size = 32\n",
    "\n",
    "print(\"Creating sequences from the dataset...\")\n",
    "try:\n",
    "    train_ds, val_ds, test_ds, input_shape, train_data, val_data, test_data = prepare_time_based_sequences(\n",
    "        reduced_df, sequence_length, 'ReceivedCount', batch_size, train_ratio=0.7, val_ratio=0.15)\n",
    "    del reduced_df  \n",
    "    gc.collect()\n",
    "except Exception as e:\n",
    "    print(f\"Error creating sequences: {e}\")\n",
    "    raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model with input shape: (8, 13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chris/Documents/Github/CSUF-Masters-Project-Public/.venv/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,888</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │            <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m5,888\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │            \u001b[38;5;34m64\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,625</span> (25.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,625\u001b[0m (25.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,529</span> (25.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,529\u001b[0m (25.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">96</span> (384.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m96\u001b[0m (384.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744861751.635426    5317 service.cc:152] XLA service 0x7b96d0019570 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1744861751.635447    5317 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Ti Laptop GPU, Compute Capability 8.6\n",
      "I0000 00:00:1744861751.660833    5317 cuda_dnn.cc:529] Loaded cuDNN version 90800\n",
      "I0000 00:00:1744861751.774866    5317 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-04-16 20:49:13.036628: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.4676 - mae: 0.3418\n",
      "Epoch 1: val_loss improved from inf to 1.08266, saving model to private/data/training_data/time_based_repair_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleanup after epoch 0\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.4674 - mae: 0.3418 - val_loss: 1.0827 - val_mae: 0.9452 - learning_rate: 5.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3366 - mae: 0.3103\n",
      "Epoch 2: val_loss improved from 1.08266 to 1.00241, saving model to private/data/training_data/time_based_repair_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - loss: 0.3366 - mae: 0.3103 - val_loss: 1.0024 - val_mae: 0.9056 - learning_rate: 5.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3391 - mae: 0.3148\n",
      "Epoch 3: val_loss did not improve from 1.00241\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3390 - mae: 0.3148 - val_loss: 13.7360 - val_mae: 1.0530 - learning_rate: 5.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m241/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4037 - mae: 0.3183\n",
      "Epoch 4: val_loss improved from 1.00241 to 0.85741, saving model to private/data/training_data/time_based_repair_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleanup after epoch 3\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.3985 - mae: 0.3170 - val_loss: 0.8574 - val_mae: 0.8772 - learning_rate: 5.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m252/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2551 - mae: 0.2791\n",
      "Epoch 5: val_loss did not improve from 0.85741\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2551 - mae: 0.2790 - val_loss: 2.1870 - val_mae: 1.4183 - learning_rate: 5.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m239/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2652 - mae: 0.2679\n",
      "Epoch 6: val_loss did not improve from 0.85741\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2646 - mae: 0.2680 - val_loss: 23.9814 - val_mae: 4.8159 - learning_rate: 5.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m239/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2345 - mae: 0.2635\n",
      "Epoch 7: val_loss did not improve from 0.85741\n",
      "Memory cleanup after epoch 6\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2351 - mae: 0.2639 - val_loss: 10.2914 - val_mae: 3.1580 - learning_rate: 5.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m232/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2393 - mae: 0.2692\n",
      "Epoch 8: val_loss did not improve from 0.85741\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2390 - mae: 0.2690 - val_loss: 6526.9214 - val_mae: 80.7862 - learning_rate: 5.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2345 - mae: 0.2711\n",
      "Epoch 9: val_loss did not improve from 0.85741\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2345 - mae: 0.2711 - val_loss: 39.6363 - val_mae: 6.2631 - learning_rate: 5.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m256/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2183 - mae: 0.2638\n",
      "Epoch 10: val_loss improved from 0.85741 to 0.15231, saving model to private/data/training_data/time_based_repair_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory cleanup after epoch 9\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2182 - mae: 0.2638 - val_loss: 0.1523 - val_mae: 0.2091 - learning_rate: 2.5000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m242/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1913 - mae: 0.2389\n",
      "Epoch 11: val_loss did not improve from 0.15231\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1922 - mae: 0.2393 - val_loss: 1.2878 - val_mae: 1.0959 - learning_rate: 2.5000e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m251/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2084 - mae: 0.2508\n",
      "Epoch 12: val_loss did not improve from 0.15231\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2081 - mae: 0.2505 - val_loss: 131.8315 - val_mae: 11.4722 - learning_rate: 2.5000e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m248/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2001 - mae: 0.2502\n",
      "Epoch 13: val_loss did not improve from 0.15231\n",
      "Memory cleanup after epoch 12\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2007 - mae: 0.2504 - val_loss: 92.7099 - val_mae: 9.5985 - learning_rate: 2.5000e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m242/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2027 - mae: 0.2534\n",
      "Epoch 14: val_loss did not improve from 0.15231\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2036 - mae: 0.2535 - val_loss: 0.4746 - val_mae: 0.6107 - learning_rate: 2.5000e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m246/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2077 - mae: 0.2528\n",
      "Epoch 15: val_loss did not improve from 0.15231\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.2079 - mae: 0.2528 - val_loss: 2.1124 - val_mae: 1.3935 - learning_rate: 2.5000e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m249/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2086 - mae: 0.2484\n",
      "Epoch 16: val_loss did not improve from 0.15231\n",
      "Memory cleanup after epoch 15\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2084 - mae: 0.2483 - val_loss: 0.2646 - val_mae: 0.4627 - learning_rate: 1.2500e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m239/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1959 - mae: 0.2473\n",
      "Epoch 17: val_loss did not improve from 0.15231\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1961 - mae: 0.2472 - val_loss: 0.1808 - val_mae: 0.2093 - learning_rate: 1.2500e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m249/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2057 - mae: 0.2366\n",
      "Epoch 18: val_loss improved from 0.15231 to 0.11527, saving model to private/data/training_data/time_based_repair_model.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.2054 - mae: 0.2366 - val_loss: 0.1153 - val_mae: 0.1703 - learning_rate: 1.2500e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m248/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1867 - mae: 0.2279\n",
      "Epoch 19: val_loss did not improve from 0.11527\n",
      "Memory cleanup after epoch 18\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1868 - mae: 0.2281 - val_loss: 0.1693 - val_mae: 0.2185 - learning_rate: 1.2500e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m247/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1830 - mae: 0.2254\n",
      "Epoch 20: val_loss did not improve from 0.11527\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1832 - mae: 0.2256 - val_loss: 0.1849 - val_mae: 0.2574 - learning_rate: 1.2500e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m254/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1863 - mae: 0.2321\n",
      "Epoch 21: val_loss did not improve from 0.11527\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1863 - mae: 0.2321 - val_loss: 4.1349 - val_mae: 1.9911 - learning_rate: 1.2500e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m240/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1896 - mae: 0.2353\n",
      "Epoch 22: val_loss did not improve from 0.11527\n",
      "Memory cleanup after epoch 21\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1890 - mae: 0.2347 - val_loss: 0.1192 - val_mae: 0.1775 - learning_rate: 1.2500e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m241/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1770 - mae: 0.2245\n",
      "Epoch 23: val_loss did not improve from 0.11527\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1769 - mae: 0.2245 - val_loss: 0.1543 - val_mae: 0.2358 - learning_rate: 1.2500e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1830 - mae: 0.2211\n",
      "Epoch 24: val_loss did not improve from 0.11527\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1830 - mae: 0.2211 - val_loss: 0.2483 - val_mae: 0.4332 - learning_rate: 6.2500e-05\n",
      "Epoch 25/100\n",
      "\u001b[1m243/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1635 - mae: 0.2166\n",
      "Epoch 25: val_loss did not improve from 0.11527\n",
      "Memory cleanup after epoch 24\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1635 - mae: 0.2164 - val_loss: 0.3051 - val_mae: 0.4691 - learning_rate: 6.2500e-05\n",
      "Epoch 26/100\n",
      "\u001b[1m251/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1593 - mae: 0.2077\n",
      "Epoch 26: val_loss did not improve from 0.11527\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1594 - mae: 0.2077 - val_loss: 1.7398 - val_mae: 1.2900 - learning_rate: 6.2500e-05\n",
      "Epoch 27/100\n",
      "\u001b[1m251/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1602 - mae: 0.2111\n",
      "Epoch 27: val_loss did not improve from 0.11527\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1603 - mae: 0.2111 - val_loss: 8.3892 - val_mae: 2.8721 - learning_rate: 6.2500e-05\n",
      "Epoch 28/100\n",
      "\u001b[1m247/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1469 - mae: 0.2067\n",
      "Epoch 28: val_loss did not improve from 0.11527\n",
      "\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "Memory cleanup after epoch 27\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1472 - mae: 0.2067 - val_loss: 0.2968 - val_mae: 0.4867 - learning_rate: 6.2500e-05\n",
      "Epoch 29/100\n",
      "\u001b[1m253/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1482 - mae: 0.2006\n",
      "Epoch 29: val_loss did not improve from 0.11527\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1483 - mae: 0.2007 - val_loss: 0.2075 - val_mae: 0.3792 - learning_rate: 3.1250e-05\n",
      "Epoch 30/100\n",
      "\u001b[1m256/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1563 - mae: 0.2027\n",
      "Epoch 30: val_loss did not improve from 0.11527\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1563 - mae: 0.2027 - val_loss: 0.1268 - val_mae: 0.2194 - learning_rate: 3.1250e-05\n",
      "Epoch 31/100\n",
      "\u001b[1m246/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1558 - mae: 0.2123\n",
      "Epoch 31: val_loss did not improve from 0.11527\n",
      "Memory cleanup after epoch 30\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1556 - mae: 0.2121 - val_loss: 0.2070 - val_mae: 0.3976 - learning_rate: 3.1250e-05\n",
      "Epoch 32/100\n",
      "\u001b[1m242/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1494 - mae: 0.1976\n",
      "Epoch 32: val_loss did not improve from 0.11527\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.1498 - mae: 0.1980 - val_loss: 0.1159 - val_mae: 0.2496 - learning_rate: 3.1250e-05\n",
      "Epoch 33/100\n",
      "\u001b[1m237/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1454 - mae: 0.2040\n",
      "Epoch 33: val_loss did not improve from 0.11527\n",
      "\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "\u001b[1m257/257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.1458 - mae: 0.2041 - val_loss: 0.3546 - val_mae: 0.5459 - learning_rate: 3.1250e-05\n",
      "Epoch 33: early stopping\n",
      "Restoring model weights from the end of the best epoch: 18.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training history saved to private/data/training_data/training_history.csv\n",
      "Model saved to private/data/training_data/time_based_repair_model.h5\n",
      "Saved test data to private/data/training_data/test_data.npz\n"
     ]
    }
   ],
   "source": [
    "# Model Definition\n",
    "def build_stable_lstm_model(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(32,  \n",
    "             activation='tanh',\n",
    "             recurrent_activation='sigmoid',\n",
    "             recurrent_regularizer=l2(0.001),\n",
    "             kernel_regularizer=l2(0.001),\n",
    "             return_sequences=False,\n",
    "             input_shape=input_shape),  \n",
    "        \n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(16, \n",
    "              activation='relu',\n",
    "              kernel_regularizer=l2(0.001)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mse',  \n",
    "        metrics=['mae'] \n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and train model\n",
    "print(f\"Building model with input shape: {input_shape}\")\n",
    "model = build_stable_lstm_model(input_shape)\n",
    "model.summary()\n",
    "\n",
    "# NaN detection\n",
    "class NaNCallback(tf.keras.callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        if logs is not None and (np.isnan(logs.get('loss', 0)) or np.isinf(logs.get('loss', 0))):\n",
    "            print(f\"\\nNaN/Inf loss detected at batch {batch}. Terminating training.\")\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None and (np.isnan(logs.get('loss', 0)) or np.isinf(logs.get('loss', 0))):\n",
    "            print(f\"\\nNaN/Inf loss detected at end of epoch {epoch}. Terminating training.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Memory cleanup\n",
    "class MemoryCleanupCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, cleanup_frequency=3):\n",
    "        super().__init__()\n",
    "        self.cleanup_frequency = cleanup_frequency\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % self.cleanup_frequency == 0:\n",
    "            gc.collect()\n",
    "            print(f\"Memory cleanup after epoch {epoch}\")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,  # Increased patience for better convergence\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Use ReduceLROnPlateau\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,  # Increased patience for LR reduction\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Checkpoint to save best model\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=os.path.join(output_dir, 'time_based_repair_model.h5'),\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "nan_callback = NaNCallback()\n",
    "memory_cleanup = MemoryCleanupCallback(cleanup_frequency=3)\n",
    "\n",
    "print(\"Training model...\")\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=100,  # Increased epochs (from 40 to 100)\n",
    "        callbacks=[\n",
    "            model_checkpoint, \n",
    "            early_stopping, \n",
    "            reduce_lr,  # More stable LR reduction\n",
    "            nan_callback, \n",
    "            memory_cleanup\n",
    "        ],\n",
    "        verbose=1\n",
    "    )\n",
    "    gc.collect()  \n",
    "    \n",
    "    # Check if training completed without NaNs\n",
    "    if history.history['loss'] and any(np.isnan(loss) for loss in history.history['loss']):\n",
    "        print(\"NaN values detected in training loss history\")\n",
    "    \n",
    "    # Save training history\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_csv_file = os.path.join(output_dir, 'training_history.csv')\n",
    "    hist_df.to_csv(hist_csv_file, index=False)\n",
    "    print(f\"Training history saved to {hist_csv_file}\")\n",
    "    \n",
    "    # Save model\n",
    "    model.save(os.path.join(output_dir, 'time_based_repair_model.h5'))\n",
    "    print(f\"Model saved to {os.path.join(output_dir, 'time_based_repair_model.h5')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "test_data_path = os.path.join(output_dir, 'test_data.npz')\n",
    "np.savez(test_data_path, \n",
    "         X_test=test_data[0], \n",
    "         y_test=test_data[1], \n",
    "         y_test_original=test_data[2])\n",
    "print(f\"Saved test data to {test_data_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
